#| warning: false
#| message: false
#| error: false
# probability of getting output as 2 - benign cancer
knn.predict_proba(X_testPy)
quit
#| label: funcion_calculo_error2
#| echo: true
#| eval: true
#| warning: false
#| message: false
#| error: false
calc_class_err = function(actual, predicted) {
mean(actual != predicted)
}
#| label: aplicar_calculo_error
#| echo: true
#| eval: true
#| warning: false
#| message: false
#| error: false
calc_class_err(actual    = y_test,
predicted = knn(train = X_train,
test  = X_test,
cl    = y_train,
k     = 5))
#| label: extraer_valor_tabla
#| echo: true
#| eval: true
#| warning: false
#| message: false
#| error: false
max(which(err_k == min(err_k)))
#| label: matriz_confusion_base
#| echo: true
#| eval: true
#| warning: false
#| message: false
#| error: false
predicciones <- knn(train = X_train, test = X_test, cl = y_train, k = 5)
table(predicciones, y_test)
#| label: aplicar_calculo_error2
#| echo: true
#| eval: true
#| warning: false
#| message: false
#| error: false
caret::confusionMatrix(predict(entrenamiento, newdata = X_testC), y_testC)
#| label: confusion_matrix_ggplot2
#| echo: true
#| eval: true
#| warning: false
#| message: false
#| error: false
library(ggplot2)
library(reshape2)
# Crear matriz de confusión como tabla
conf_tbl <- table(Predicted = predict(entrenamiento, newdata = X_testC), Actual = y_testC)
# Convertir a data.frame para ggplot
conf_df <- as.data.frame(conf_tbl)
colnames(conf_df) <- c("Predicted", "Actual", "Freq")
# Visualización con ggplot2
ggplot(conf_df, aes(x = Actual, y = Predicted, fill = Freq)) +
geom_tile(color = "white") +
geom_text(aes(label = Freq), size = 5) +
scale_fill_gradient(low = "#f7fcf0", high = "#084081") +
labs(title = "Matriz de confusión", x = "Valor real", y = "Predicción") +
theme_minimal()
reticulate::repl_python()
#| label: acurracy_python
#| echo: true
#| eval: true
#| warning: false
#| message: false
#| error: false
from sklearn.metrics import accuracy_score
print('Model accuracy score: {0:0.4f}'. format(accuracy_score(y_testPy, y_predPy)))
#| label: python_sobreajuste
#| echo: true
#| eval: true
#| warning: false
#| message: false
#| error: false
print('Training set score: {:.4f}'.format(knn.score(X_trainPy, y_trainPy)))
print('Test set score: {:.4f}'.format(knn.score(X_testPy, y_testPy)))
#| label: heatmap_matriz_confusion_python
#| echo: true
#| eval: true
#| warning: false
#| message: false
#| error: false
import seaborn as sns # for data visualization
from sklearn.metrics import confusion_matrix
# visualize confusion matrix with seaborn heatmap
plt.figure(figsize=(6,4))
confMatrix = confusion_matrix(y_testPy, y_predPy)
cm_matrix = pd.DataFrame(data=confMatrix, columns=['Actual Positive:1', 'Actual Negative:0'], index=['Predict Positive:1', 'Predict Negative:0'])
sns.heatmap(cm_matrix, annot=True, fmt='d', cmap='YlGnBu')
#| label: classificacion_python
#| echo: true
#| eval: true
#| warning: false
#| message: false
#| error: false
from sklearn.metrics import classification_report
print(classification_report(y_testPy, y_predPy))
#| label: grafico_clasificacion_python
#| echo: true
#| eval: true
#| warning: false
#| message: false
#| error: false
import numpy as np
import matplotlib.pyplot as plt
from matplotlib.colors import ListedColormap
import matplotlib.patches as mpatches
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.neighbors import KNeighborsClassifier
# ==== Parámetros ====
n_neighbors = 5
weights = 'distance'
h = 0.02
# ==== 1) Estandarizar con medias/SD de train + PCA en train ====
pca = PCA(n_components = 2, random_state=0)
Z_train = pca.fit_transform(X_trainPy)   # coords PCA train
Z_test  = pca.transform(X_testPy)        # coords PCA test
# ==== 2) Entrenar KNN en el espacio PCA (train) ====
clf = KNeighborsClassifier(n_neighbors=n_neighbors, weights=weights)
clf.fit(Z_train, y_train)
# ==== 3) Mallado en el plano PCA (usando rango de TRAIN para coherencia) ====
x_min, x_max = Z_train[:, 0].min() - 1, Z_train[:, 0].max() + 1
y_min, y_max = Z_train[:, 1].min() - 1, Z_train[:, 1].max() + 1
xx, yy = np.meshgrid(np.arange(x_min, x_max, h),
np.arange(y_min, y_max, h))
Z_grid_pred = clf.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)
# ==== 4) Paletas (hasta 5 clases) ====
cmap_light = ListedColormap(['#FFAAAA', '#b3ffff'])
cmap_bold  = ListedColormap(['#FF0000', '#00ffff'])
# ==== 5) Plot fondo + puntos ====
plt.figure(figsize=(7, 5))
plt.pcolormesh(xx, yy, Z_grid_pred, cmap=cmap_light, shading='auto')
# Opción A: pintar TRAIN
plt.scatter(Z_train[:, 0], Z_train[:, 1], c=y_trainPy, cmap=cmap_bold, edgecolor='k', s=25, alpha=0.8, label='Train')
# Opción B: y/o pintar TEST (etiquetas reales)
plt.scatter(Z_test[:, 0],  Z_test[:, 1],  c=y_testPy,  cmap=cmap_bold, edgecolor='k', s=35, marker='o', label='Test')
plt.xlim(xx.min(), xx.max())
plt.ylim(yy.min(), yy.max())
plt.xlabel('PC1')
plt.ylabel('PC2')
plt.title(f"Frontera KNN en plano PCA (k={n_neighbors}, weights='{weights}')")
plt.legend()
# Leyenda de clases (dinámica)
classes = np.unique(np.concatenate([y_trainPy, y_testPy]))
patches = []
palette = ['#FF0000', '#00ffff']
for i, cls in enumerate(classes):
color = palette[i % len(palette)]
patches.append(mpatches.Patch(color=color, label=str(cls)))
legend1 = plt.legend(handles=patches, title="Clases", loc='upper right', bbox_to_anchor=(1.32, 1.0))
plt.gca().add_artist(legend1)  # mantener ambas leyendas
plt.tight_layout()
plt.show()
#| label: grafico_clasificacion_python
#| echo: true
#| eval: true
#| warning: false
#| message: false
#| error: false
import numpy as np
import matplotlib.pyplot as plt
from matplotlib.colors import ListedColormap
import matplotlib.patches as mpatches
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.neighbors import KNeighborsClassifier
# ==== Parámetros ====
n_neighbors = 5
weights = 'distance'
h = 0.02
# ==== 1) Estandarizar con medias/SD de train + PCA en train ====
pca = PCA(n_components = 2, random_state=0)
Z_train = pca.fit_transform(X_trainPy)   # coords PCA train
Z_test  = pca.transform(X_testPy)        # coords PCA test
# ==== 2) Entrenar KNN en el espacio PCA (train) ====
clf = KNeighborsClassifier(n_neighbors=n_neighbors, weights=weights)
clf.fit(Z_train, y_trainPy)
# ==== 3) Mallado en el plano PCA (usando rango de TRAIN para coherencia) ====
x_min, x_max = Z_train[:, 0].min() - 1, Z_train[:, 0].max() + 1
y_min, y_max = Z_train[:, 1].min() - 1, Z_train[:, 1].max() + 1
xx, yy = np.meshgrid(np.arange(x_min, x_max, h),
np.arange(y_min, y_max, h))
Z_grid_pred = clf.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)
# ==== 4) Paletas (hasta 5 clases) ====
cmap_light = ListedColormap(['#FFAAAA', '#b3ffff'])
cmap_bold  = ListedColormap(['#FF0000', '#00ffff'])
# ==== 5) Plot fondo + puntos ====
plt.figure(figsize=(7, 5))
plt.pcolormesh(xx, yy, Z_grid_pred, cmap=cmap_light, shading='auto')
# Opción A: pintar TRAIN
plt.scatter(Z_train[:, 0], Z_train[:, 1], c=y_trainPy, cmap=cmap_bold, edgecolor='k', s=25, alpha=0.8, label='Train')
# Opción B: y/o pintar TEST (etiquetas reales)
plt.scatter(Z_test[:, 0],  Z_test[:, 1],  c=y_testPy,  cmap=cmap_bold, edgecolor='k', s=35, marker='o', label='Test')
plt.xlim(xx.min(), xx.max())
plt.ylim(yy.min(), yy.max())
plt.xlabel('PC1')
plt.ylabel('PC2')
plt.title(f"Frontera KNN en plano PCA (k={n_neighbors}, weights='{weights}')")
plt.legend()
# Leyenda de clases (dinámica)
classes = np.unique(np.concatenate([y_trainPy, y_testPy]))
patches = []
palette = ['#FF0000', '#00ffff']
for i, cls in enumerate(classes):
color = palette[i % len(palette)]
patches.append(mpatches.Patch(color=color, label=str(cls)))
legend1 = plt.legend(handles=patches, title="Clases", loc='upper right', bbox_to_anchor=(1.32, 1.0))
plt.gca().add_artist(legend1)  # mantener ambas leyendas
plt.tight_layout()
plt.show()
quit
#| label: grafico_pca_modelo
#| echo: true
#| eval: true
#| warning: false
#| message: false
#| error: false
# Paquetes
library(class)      # knn
library(ggplot2)    # plotting
library(dplyr)      # %>%, mutate
library(tidyr)
# --- Datos de entrada esperados ---
# X_train, X_test: data.frames/ matrices numéricas
# y_train, y_test: vector/factor de etiquetas (clases)
k <- 5
# 1) PCA AJUSTADO EN TRAIN (con centrado y escalado). Proyectamos train y test.
pca <- prcomp(X_train, center = TRUE, scale. = TRUE)
Z_train <- predict(pca, newdata = X_train)[, 1:2]
Z_test  <- predict(pca, newdata = X_test)[, 1:2]
colnames(Z_train) <- c("PC1","PC2")
colnames(Z_test)  <- c("PC1","PC2")
# 2) Rango para el grid (usamos TRAIN para coherencia)
h <- 0.02
x_min <- min(Z_train[,1]) - 1; x_max <- max(Z_train[,1]) + 1
y_min <- min(Z_train[,2]) - 1; y_max <- max(Z_train[,2]) + 1
grid <- expand.grid(
PC1 = seq(x_min, x_max, by = h),
PC2 = seq(y_min, y_max, by = h)
)
# 3) kNN en el plano PCA (class::knn no "entrena"; predice dado train/test)
#    Usamos las coordenadas PCA de train como "train", y el grid como "test".
#    Asegura que y_train sea factor.
y_train <- factor(y_train)
y_test  <- factor(y_test, levels = levels(y_train))
grid_pred <- knn(
train = Z_train,
test  = grid,
cl    = y_train,
k     = k
)
grid$pred <- factor(grid_pred, levels = levels(y_train))
# 4) Data frames para puntos
df_train <- data.frame(Z_train, clase = y_train, split = "Train")
df_test  <- data.frame(Z_test,  clase = y_test,  split = "Test")
# 5) Paleta (ajusta si hay >2 clases)
pal_cls  <- c("#FF0000", "#00ffff")           # puntos (clases)
pal_fill <- c("#FFAAAA", "#b3ffff")           # fondo (clases)
names(pal_cls)  <- levels(y_train)
names(pal_fill) <- levels(y_train)
# 6) Plot: fondo (grid) + puntos (train/test)
ggplot() +
geom_raster(data = grid, aes(x = PC1, y = PC2, fill = pred), alpha = 1) +
scale_fill_manual(values = pal_fill, name = "Clase (fondo)") +
geom_point(data = df_train, aes(PC1, PC2, color = clase), size = 1.8, stroke = .2) +
geom_point(data = df_test,  aes(PC1, PC2, color = clase), size = 2.2, stroke = .2, shape = 21) +
scale_color_manual(values = pal_cls, name = "Clase (puntos)") +
labs(
title = sprintf("Frontera kNN en plano PCA (k=%d, weights='distance')", k),
x = "PC1", y = "PC2"
) +
coord_equal(expand = FALSE, xlim = c(x_min, x_max), ylim = c(y_min, y_max)) +
theme_minimal() +
theme(legend.position = "right")
#| label: grafico_pca_caret
#| echo: true
#| eval: true
#| warning: false
#| message: false
#| error: false
library(caret)
library(ggplot2)
library(dplyr)
k <- 5
# 1) Preprocesado: center, scale, pca (2 comps) AJUSTADO EN TRAIN
preproc <- preProcess(X_trainC, method = c("center", "scale", "pca"), pcaComp = 2)
Z_train <- predict(preproc, X_trainC)  # columnas: PC1, PC2
Z_test  <- predict(preproc, X_testC)
# 2) Entrenar kNN en el espacio PCA (con caret)
y_train <- factor(y_trainC)
y_trainC
#| label: grafico_pca_caret
#| echo: true
#| eval: true
#| warning: false
#| message: false
#| error: false
library(caret)
library(ggplot2)
library(dplyr)
k <- 5
# 1) Preprocesado: center, scale, pca (2 comps) AJUSTADO EN TRAIN
preproc <- preProcess(X_trainC, method = c("center", "scale", "pca"), pcaComp = 2)
Z_train <- predict(preproc, X_trainC)  # columnas: PC1, PC2
Z_test  <- predict(preproc, X_testC)
# 2) Entrenar kNN en el espacio PCA (con caret)
y_trainC <- factor(y_trainC)
#| label: grafico_pca_caret
#| echo: true
#| eval: true
#| warning: false
#| message: false
#| error: false
library(caret)
library(ggplot2)
library(dplyr)
k <- 5
# 1) Preprocesado: center, scale, pca (2 comps) AJUSTADO EN TRAIN
preproc <- preProcess(X_trainC, method = c("center", "scale", "pca"), pcaComp = 2)
Z_train <- predict(preproc, X_trainC)  # columnas: PC1, PC2
Z_test  <- predict(preproc, X_testC)
# 2) Entrenar kNN en el espacio PCA (con caret)
y_trainC <- factor(X_trainC$Response)
y_testC  <- factor(y_testC, levels = levels(X_trainC$Response))
modelo_knn <- train(
x = Z_train,
y = y_trainC,
method = "knn",
tuneGrid = data.frame(k = k)
)
Z_train
y_trainC
modelo_knn <- train(
x = Z_train,
y = y_trainC,
method = "knn",
tuneGrid = data.frame(k = k)
)
entrenamiento
#| label: grafico_pca_caret
#| echo: true
#| eval: true
#| warning: false
#| message: false
#| error: false
library(caret)
library(ggplot2)
library(dplyr)
k <- 5
# 1) Preprocesado: center, scale, pca (2 comps) AJUSTADO EN TRAIN
preproc <- preProcess(X_trainC, method = c("center", "scale", "pca"), pcaComp = 2)
Z_train <- predict(preproc, X_trainC)  # columnas: PC1, PC2
Z_test  <- predict(preproc, X_testC)
# 2) Entrenar kNN en el espacio PCA (con caret)
y_trainC <- factor(X_trainC$Response)
y_testC  <- factor(y_testC, levels = levels(X_trainC$Response))
modelo_knn <- entrenamiento
# 3) Grid en el plano PCA (rango del TRAIN)
h <- 0.02
x_min <- min(Z_train$PC1) - 1; x_max <- max(Z_train$PC1) + 1
y_min <- min(Z_train$PC2) - 1; y_max <- max(Z_train$PC2) + 1
grid <- expand.grid(
PC1 = seq(x_min, x_max, by = h),
PC2 = seq(y_min, y_max, by = h)
)
# 4) Predicción del fondo en el grid
grid$pred <- predict(modelo_knn, newdata = grid)
# 4) Predicción del fondo en el grid
grid$pred <- predict(entrenamiento, newdata = grid)
entrenamiento
grid
grid <- expand.grid(
PC1 = seq(x_min, x_max, by = h),
PC2 = seq(y_min, y_max, by = h)
)
grid
colnames(grid)
modelo_knn <- train(
x = Z_train,
y = y_trainC,
method = "knn",
tuneGrid = data.frame(k = k)
)
y_trainC
Z_train <- predict(preproc, X_trainC)  # columnas: PC1, PC2
Z_train
k <- 5
# 1) Preprocesado: center, scale, pca (2 comps) AJUSTADO EN TRAIN
preproc <- preProcess(X_trainC, method = c("center", "scale", "pca"), pcaComp = 2)
preproc
Z_train <- predict(preproc, X_trainC)  # columnas: PC1, PC2
Z_train
Z_test  <- predict(preproc, X_testC)
# 2) Entrenar kNN en el espacio PCA (con caret)
y_trainC <- factor(X_trainC$Response)
y_testC  <- factor(y_testC, levels = levels(X_trainC$Response))
modelo_knn <- train(
x = Z_train,
y = y_trainC,
method = "knn",
tuneGrid = data.frame(k = k)
)
library(caret)
library(ggplot2)
library(dplyr)
# --- Si X_trainC incluye la columna 'Response', separamos:
#     (si ya la tienes separada, omite estas dos líneas y usa tus objetos)
y_trainC <- factor(X_trainC$Response)
X_train_num <- X_trainC[, setdiff(names(X_trainC), "Response"), drop = FALSE]
# Asegura que los predictores son numéricos
X_train_num[] <- lapply(X_train_num, function(col) as.numeric(as.character(col)))
# 1) Preprocesado SOLO sobre predictores (center, scale, pca=2)
preproc <- preProcess(X_train_num, method = c("center", "scale", "pca"), pcaComp = 2)
Z_train <- predict(preproc, X_train_num)  # tendrá columnas PC1 y PC2
# --- Prepara también el test de forma consistente ---
# Si X_testC tiene 'Response', sepárala:
if ("Response" %in% names(X_testC)) {
y_testC  <- factor(X_testC$Response, levels = levels(y_trainC))
X_test_num <- X_testC[, setdiff(names(X_testC), "Response"), drop = FALSE]
} else {
# si ya tienes y_testC aparte:
X_test_num <- X_testC
y_testC    <- factor(y_testC, levels = levels(y_trainC))
}
X_test_num[] <- lapply(X_test_num, function(col) as.numeric(as.character(col)))
Z_test <- predict(preproc, X_test_num)
# 2) Control de entrenamiento (CV estratificada)
ctrl <- trainControl(method = "cv", number = 5)
# 3) Entrenar kNN en el espacio PCA
k <- 5
modelo_knn <- train(
x = Z_train,
y = y_trainC,
method = "knn",
tuneGrid = data.frame(k = k),
trControl = ctrl,
metric = "Accuracy"
)
# 3) Grid en el plano PCA (rango del TRAIN)
h <- 0.02
x_min <- min(Z_train$PC1) - 1; x_max <- max(Z_train$PC1) + 1
y_min <- min(Z_train$PC2) - 1; y_max <- max(Z_train$PC2) + 1
grid <- expand.grid(
PC1 = seq(x_min, x_max, by = h),
PC2 = seq(y_min, y_max, by = h)
)
# 4) Predicción del fondo en el grid
grid$pred <- predict(modelo_knn, newdata = grid)
# 4) Predicción del fondo en el grid
grid$pred <- predict(modelo_knn, newdata = grid)
# 5) Data frames para puntos
df_train <- data.frame(Z_train, clase = y_trainC, split = "Train")
df_test  <- data.frame(Z_test,  clase = y_testC,  split = "Test")
df_test  <- data.frame(Z_test,  clase = y_testC,  split = "Test")
# 6) Paletas
pal_cls  <- c("#FF0000", "#00ffff")
pal_fill <- c("#FFAAAA", "#b3ffff")
names(pal_cls)  <- levels(y_trainC)
names(pal_fill) <- levels(y_trainC)
# 7) Plot
ggplot() +
geom_raster(data = grid, aes(x = PC1, y = PC2, fill = pred), alpha = 1) +
scale_fill_manual(values = pal_fill, name = "Clase (fondo)") +
geom_point(data = df_train, aes(PC1, PC2, color = clase), size = 1.8, stroke = .2) +
geom_point(data = df_test,  aes(PC1, PC2, color = clase), size = 2.2, stroke = .2, shape = 21) +
scale_color_manual(values = pal_cls, name = "Clase (puntos)") +
labs(
title = sprintf("Frontera kNN en plano PCA (k=%d)", k),
x = "PC1", y = "PC2"
) +
coord_equal(expand = FALSE, xlim = c(x_min, x_max), ylim = c(y_min, y_max)) +
theme_minimal() +
theme(legend.position = "right")
reticulate::repl_python()
#| label: prediccion_python
#| echo: true
#| eval: true
#| warning: false
#| message: false
#| error: false
y_predPy = knn.predict(X_testPy)
y_predPy[:5]
#| label: prediccion_python
#| echo: true
#| eval: true
#| warning: false
#| message: false
#| error: false
y_predPy = knn.predict(X_testPy)
y_predPy[:10]
#| label: prediccion_python
#| echo: true
#| eval: true
#| warning: false
#| message: false
#| error: false
y_predPy = knn.predict(X_testPy)
y_predPy[:20]
quit
default_idx = sample(nrow(datos), nrow(datos)*0.7)
default_idx
head(datos)

{"title":"Anàlisis de Components Principals (ACP)","markdown":{"yaml":{"title":"Anàlisis de Components Principals (ACP)","author":"Dante Conti, Sergi Ramirez, (c) IDEAI","date":"`r Sys.Date()`","date-modified":"`r Sys.Date()`","toc":true,"number-sections":true,"format":{"html":{"theme":{"light":"cerulean","dark":"darkly"}}},"editor":"visual"},"headingText":"¿Qué es el ACP?","containsRefs":false,"markdown":"\n\n\nEl **Análisis de Componentes Principales (ACP)** es una técnica de **reducción de dimensionalidad** que transforma un conjunto de variables posiblemente correlacionadas en un conjunto más pequeño de variables no correlacionadas llamadas **componentes principales (CPs)**.\\\nObjetivos típicos:\n\n-   Capturar la mayor **varianza** con el menor número de componentes.\\\n-   **Descorrelacionar** variables y simplificar la estructura.\\\n-   Facilitar **visualización**, **preprocesamiento** para modelos y **compresión**.\n\n> Intuición: el ACP encuentra direcciones (vectores) en el espacio de los datos que maximizan la varianza. Esas direcciones son los **autovectores** de la matriz de covarianzas (o correlaciones), y la varianza capturada por cada componente viene dada por su **autovalor**.\n\n# Supuestos y buenas prácticas\n\n-   **Estandarización**: si las variables tienen escalas diferentes, estandariza (media 0, var 1).\\\n-   **Linealidad**: PCA capta relaciones lineales.\\\n-   **Outliers**: pueden dominar la varianza; considera limpiarlos o usar variantes robustas.\\\n-   **Datos numéricos**: para variables categóricas puras, usa técnicas específicas (MCA/FAMD).\n\n# Flujo de trabajo (resumen)\n\n1.  Estandarizar (opcional pero recomendado).\\\n2.  Ajustar ACP sobre matriz estandarizada.\\\n3.  Evaluar **varianza explicada** y elegir k componentes.\\\n4.  Inspeccionar **cargas** (loadings) y **scores**.\\\n5.  Visualizar: **scree plot**, **biplot**.\\\n6.  Proyectar nuevos datos y (si aplica) reconstruir.\n\n# Dataset de ejemplo\n\nUsaremos el clásico **Iris** (4 variables numéricas, 3 especies).\\\nEn R: `datasets::iris`.\\\nEn Python: `sklearn.datasets.load_iris()`.\n\n# Datos y estandarización\n\n::: panel-tabset\n## R\n\n```{r}\n# Paquetes\nlibrary(dplyr)\nlibrary(ggplot2)\n\n# Cargar iris\ndata(iris)\ndf <- iris %>% select(-Species)  # solo numéricas\n\n# Estandarizar (media 0, sd 1)\ndf_scaled <- scale(df)\n\n# Vista rápida\nas_tibble(head(df)) %>% print(n=6)\napply(df_scaled, 2, sd)   # ~1\n```\n\n## Python\n\n```{python}\nimport numpy as np\nimport pandas as pd\nfrom sklearn.datasets import load_iris\nfrom sklearn.preprocessing import StandardScaler\n\n# Cargar iris\niris = load_iris()\nX = pd.DataFrame(iris.data, columns=iris.feature_names)\ny = pd.Series(iris.target, name=\"species\")  # 0,1,2\n\n# Estandarizar\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n\n# Vista rápida\nX.head()\nnp.std(X_scaled, axis=0)  # ~1\n```\n:::\n\n# Ajuste del ACP\n\n::: panel-tabset\n## R\n\n```{r}\n# prcomp usa SVD; center=TRUE, scale.=TRUE también estandariza internamente\npca <- prcomp(df, center = TRUE, scale. = TRUE)\n\n# Varianza explicada\nvar_exp <- pca$sdev^2\nvar_exp_ratio <- var_exp / sum(var_exp)\ncum_exp_ratio <- cumsum(var_exp_ratio)\n\nvar_exp_ratio\ncum_exp_ratio\n```\n\n## Python\n\n```{python}\nfrom sklearn.decomposition import PCA\n\npca = PCA()  # por defecto n_components = min(n_samples, n_features)\npca.fit(X_scaled)\n\nvar_exp = pca.explained_variance_\nvar_exp_ratio = pca.explained_variance_ratio_\ncum_exp_ratio = np.cumsum(var_exp_ratio)\n\nvar_exp_ratio, cum_exp_ratio\n```\n\n\n```{python}\nimport prince\n\ndf = pd.DataFrame(X_scaled, columns=[\"col1\", \"col2\", \"col3\", \"col4\"])\n\npcaP = prince.PCA(\n    n_components=3,\n    n_iter=3,\n    rescale_with_mean=True,\n    rescale_with_std=True,\n    copy=True,\n    check_input=True,\n    engine='sklearn',\n    random_state=42\n)\n\npcaP = pcaP.fit(\n    df,\n    sample_weight=None,\n    column_weight=None,\n    supplementary_columns=None\n)\n\n```\n\n:::\n\n# Scree plot y varianza acumulada\n\n::: panel-tabset\n## R\n\n```{r}\nscree_df <- data.frame(\n  PC = paste0(\"PC\", seq_along(var_exp_ratio)),\n  VarExp = var_exp_ratio,\n  CumExp = cum_exp_ratio\n)\n\n# Scree plot\nggplot(scree_df, aes(x = seq_along(VarExp), y = VarExp)) +\n  geom_line() + geom_point() +\n  labs(x = \"Componente\", y = \"Proporción varianza explicada\",\n       title = \"Scree plot (R)\") +\n  theme_minimal()\n\n# Varianza acumulada\nggplot(scree_df, aes(x = seq_along(CumExp), y = CumExp)) +\n  geom_line() + geom_point() +\n  geom_hline(yintercept = 0.95, linetype = \"dashed\") +\n  labs(x = \"Componente\", y = \"Varianza acumulada\",\n       title = \"Varianza acumulada (R)\") +\n  theme_minimal()\n```\n\n## Python\n\n```{python}\nimport matplotlib.pyplot as plt\n\n# Scree plot\nplt.figure()\nplt.plot(range(1, len(var_exp_ratio)+1), var_exp_ratio, marker=\"o\")\nplt.xlabel(\"Componente\")\nplt.ylabel(\"Proporción varianza explicada\")\nplt.title(\"Scree plot (Python)\")\nplt.show()\n\n# Varianza acumulada\nplt.figure()\nplt.plot(range(1, len(cum_exp_ratio)+1), cum_exp_ratio, marker=\"o\")\nplt.axhline(0.95, linestyle=\"--\")\nplt.xlabel(\"Componente\")\nplt.ylabel(\"Varianza acumulada\")\nplt.title(\"Varianza acumulada (Python)\")\nplt.show()\n```\n:::\n\n# Cargas (loadings) y scores\n\nLas **cargas** indican cuánto contribuye cada variable a cada componente (correlaciones variable–componente).\n\nLos **scores** son las coordenadas de las observaciones en el espacio de componentes.\n\n::: panel-tabset\n## R\n\n```{r}\n# Cargas\nloadings <- pca$rotation\nround(loadings, 3)\n\n# Scores\nscores <- pca$x\nhead(scores)\n\n# Biplot rápido\nbiplot(pca, cex = 0.7)  # base R\n```\n\n## Python\n\n```{python}\n# Cargas = vectores propios (componentes)\nloadings = pca.components_.T  # shape: n_features x n_components\npd.DataFrame(loadings, index=iris.feature_names,\n             columns=[f\"PC{i+1}\" for i in range(loadings.shape[1])]).round(3)\n\n# Scores (transformación de los datos)\nscores = pca.transform(X_scaled)\npd.DataFrame(scores, columns=[f\"PC{i+1}\" for i in range(scores.shape[1])]).head()\n\n# Biplot sencillo (PC1 vs PC2)\npc1, pc2 = 0, 1\nplt.figure()\nplt.scatter(scores[:, pc1], scores[:, pc2], alpha=0.7)\n# Vectores de variables\nfor i, feature in enumerate(iris.feature_names):\n    plt.arrow(0, 0, loadings[i, pc1]*3, loadings[i, pc2]*3, head_width=0.05)\n    plt.text(loadings[i, pc1]*3.2, loadings[i, pc2]*3.2, feature)\nplt.xlabel(\"PC1\")\nplt.ylabel(\"PC2\")\nplt.title(\"Biplot (Python)\")\nplt.axhline(0, linewidth=0.5); plt.axvline(0, linewidth=0.5)\nplt.show()\n```\n:::\n\n## Elección de número de componentes\n\nCriterios habituales:\n\n1.  **Codo** del scree plot.\n2.  Umbral de **varianza acumulada** (p. ej., 90–95%).\n3.  **Kaiser** (autovalores \\> 1) cuando se usa matriz de correlaciones.\n4.  **Validación** en tareas posteriores (clasificación/regresión).\n\n::: panel-tabset\n## R\n\n```{r}\n# Elegir k para alcanzar >= 95% varianza\n(k <- which(cum_exp_ratio >= 0.95)[1])\n```\n\n## Python\n\n```{python}\nk = int(np.argmax(cum_exp_ratio >= 0.95)) + 1\nk\n```\n:::\n\n# Proyección y reconstrucción\n\n::: panel-tabset\n## R\n\n```{r}\n# Proyectar datos a k componentes\nk <- which(cum_exp_ratio >= 0.95)[1]\nscores_k <- scores[, 1:k, drop = FALSE]\n\n# Reconstrucción aproximada (desde scores_k -> espacio original estandarizado)\n# X_std_hat = scores_k %*% t(loadings[,1:k])\nXstd_hat <- scores_k %*% t(loadings[, 1:k])\n\n# \"Desestandarizar\" para volver a escala original\nmeans <- attr(df_scaled, \"scaled:center\")\nsds   <- attr(df_scaled, \"scaled:scale\")\nXrec <- sweep(Xstd_hat, 2, sds, `*`)\nXrec <- sweep(Xrec, 2, means, `+`)\n\nas_tibble(head(Xrec))\n```\n\n## Python\n\n```{python}\nk = int(np.argmax(cum_exp_ratio >= 0.95)) + 1\nscores_k = scores[:, :k]\nload_k = loadings[:, :k]\n\n# Reconstrucción en espacio estandarizado\nXstd_hat = np.dot(scores_k, load_k.T)\n\n# Desestandarizar\nXrec = scaler.inverse_transform(Xstd_hat)\npd.DataFrame(Xrec, columns=iris.feature_names).head()\n```\n:::\n\n# Pipeline para uso en modelos\n\nEn ML, es común encadenar **escalado + PCA** y luego un modelo.\n\n::: panel-tabset\n## R\n\n```{r}\n# Ejemplo con caret o tidymodels (aquí base con prcomp para simplicidad)\n# Generar componentes y usar los primeros k como features\nk <- 2\npc_df <- as.data.frame(scores[, 1:k])\npc_df$Species <- iris$Species\nhead(pc_df)\n```\n\n## Python\n\n```{python}\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import cross_val_score\n\nk = 2\npipe = Pipeline([\n    (\"scaler\", StandardScaler()),\n    (\"pca\", PCA(n_components=k)),\n    (\"clf\", LogisticRegression(max_iter=1000))\n])\n\nscores_cv = cross_val_score(pipe, X, y, cv=5)\nscores_cv.mean(), scores_cv.std()\n```\n:::\n\n## Interpretación de cargas y biplot\n\n-   Vectores largos: variables con alta contribución al componente.\n-   Ángulos pequeños entre vectores: variables **correlacionadas.**\n-   Puntos (observaciones) próximos: perfiles similares en variables originales.\n-   Signo de la carga: dirección de la relación con el componente.\n\n# Trucos y errores comunes\n\n-   **No estandarizar** cuando hay escalas muy distintas distorsiona el resultado\n-   **Outliers**: revisar o usar alternativas robustas (p. ej., `robustbase` en R, `sklearn` + métodos robustos).\n-   **Muchas variables**: considerar **PCA incremental** si hay memoria limitada.\n-   **Interpretabilidad**: PCA rota ejes de forma no supervisada, no siempre mapea a factores *interpretables*.\n\n# Referencias rápidas (concepto)\n\n-   Componentes = autovectores de covarianzas/correlaciones; varianza = autovalores.\n-   `scores = X_estandarizado %*% loadings`\n-   Reconstrucción aproximada: `X̂_est = scores_k %*% t(loadings_k)`; desestandarizar para volver a escala original.\n","srcMarkdownNoYaml":"\n\n# ¿Qué es el ACP?\n\nEl **Análisis de Componentes Principales (ACP)** es una técnica de **reducción de dimensionalidad** que transforma un conjunto de variables posiblemente correlacionadas en un conjunto más pequeño de variables no correlacionadas llamadas **componentes principales (CPs)**.\\\nObjetivos típicos:\n\n-   Capturar la mayor **varianza** con el menor número de componentes.\\\n-   **Descorrelacionar** variables y simplificar la estructura.\\\n-   Facilitar **visualización**, **preprocesamiento** para modelos y **compresión**.\n\n> Intuición: el ACP encuentra direcciones (vectores) en el espacio de los datos que maximizan la varianza. Esas direcciones son los **autovectores** de la matriz de covarianzas (o correlaciones), y la varianza capturada por cada componente viene dada por su **autovalor**.\n\n# Supuestos y buenas prácticas\n\n-   **Estandarización**: si las variables tienen escalas diferentes, estandariza (media 0, var 1).\\\n-   **Linealidad**: PCA capta relaciones lineales.\\\n-   **Outliers**: pueden dominar la varianza; considera limpiarlos o usar variantes robustas.\\\n-   **Datos numéricos**: para variables categóricas puras, usa técnicas específicas (MCA/FAMD).\n\n# Flujo de trabajo (resumen)\n\n1.  Estandarizar (opcional pero recomendado).\\\n2.  Ajustar ACP sobre matriz estandarizada.\\\n3.  Evaluar **varianza explicada** y elegir k componentes.\\\n4.  Inspeccionar **cargas** (loadings) y **scores**.\\\n5.  Visualizar: **scree plot**, **biplot**.\\\n6.  Proyectar nuevos datos y (si aplica) reconstruir.\n\n# Dataset de ejemplo\n\nUsaremos el clásico **Iris** (4 variables numéricas, 3 especies).\\\nEn R: `datasets::iris`.\\\nEn Python: `sklearn.datasets.load_iris()`.\n\n# Datos y estandarización\n\n::: panel-tabset\n## R\n\n```{r}\n# Paquetes\nlibrary(dplyr)\nlibrary(ggplot2)\n\n# Cargar iris\ndata(iris)\ndf <- iris %>% select(-Species)  # solo numéricas\n\n# Estandarizar (media 0, sd 1)\ndf_scaled <- scale(df)\n\n# Vista rápida\nas_tibble(head(df)) %>% print(n=6)\napply(df_scaled, 2, sd)   # ~1\n```\n\n## Python\n\n```{python}\nimport numpy as np\nimport pandas as pd\nfrom sklearn.datasets import load_iris\nfrom sklearn.preprocessing import StandardScaler\n\n# Cargar iris\niris = load_iris()\nX = pd.DataFrame(iris.data, columns=iris.feature_names)\ny = pd.Series(iris.target, name=\"species\")  # 0,1,2\n\n# Estandarizar\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n\n# Vista rápida\nX.head()\nnp.std(X_scaled, axis=0)  # ~1\n```\n:::\n\n# Ajuste del ACP\n\n::: panel-tabset\n## R\n\n```{r}\n# prcomp usa SVD; center=TRUE, scale.=TRUE también estandariza internamente\npca <- prcomp(df, center = TRUE, scale. = TRUE)\n\n# Varianza explicada\nvar_exp <- pca$sdev^2\nvar_exp_ratio <- var_exp / sum(var_exp)\ncum_exp_ratio <- cumsum(var_exp_ratio)\n\nvar_exp_ratio\ncum_exp_ratio\n```\n\n## Python\n\n```{python}\nfrom sklearn.decomposition import PCA\n\npca = PCA()  # por defecto n_components = min(n_samples, n_features)\npca.fit(X_scaled)\n\nvar_exp = pca.explained_variance_\nvar_exp_ratio = pca.explained_variance_ratio_\ncum_exp_ratio = np.cumsum(var_exp_ratio)\n\nvar_exp_ratio, cum_exp_ratio\n```\n\n\n```{python}\nimport prince\n\ndf = pd.DataFrame(X_scaled, columns=[\"col1\", \"col2\", \"col3\", \"col4\"])\n\npcaP = prince.PCA(\n    n_components=3,\n    n_iter=3,\n    rescale_with_mean=True,\n    rescale_with_std=True,\n    copy=True,\n    check_input=True,\n    engine='sklearn',\n    random_state=42\n)\n\npcaP = pcaP.fit(\n    df,\n    sample_weight=None,\n    column_weight=None,\n    supplementary_columns=None\n)\n\n```\n\n:::\n\n# Scree plot y varianza acumulada\n\n::: panel-tabset\n## R\n\n```{r}\nscree_df <- data.frame(\n  PC = paste0(\"PC\", seq_along(var_exp_ratio)),\n  VarExp = var_exp_ratio,\n  CumExp = cum_exp_ratio\n)\n\n# Scree plot\nggplot(scree_df, aes(x = seq_along(VarExp), y = VarExp)) +\n  geom_line() + geom_point() +\n  labs(x = \"Componente\", y = \"Proporción varianza explicada\",\n       title = \"Scree plot (R)\") +\n  theme_minimal()\n\n# Varianza acumulada\nggplot(scree_df, aes(x = seq_along(CumExp), y = CumExp)) +\n  geom_line() + geom_point() +\n  geom_hline(yintercept = 0.95, linetype = \"dashed\") +\n  labs(x = \"Componente\", y = \"Varianza acumulada\",\n       title = \"Varianza acumulada (R)\") +\n  theme_minimal()\n```\n\n## Python\n\n```{python}\nimport matplotlib.pyplot as plt\n\n# Scree plot\nplt.figure()\nplt.plot(range(1, len(var_exp_ratio)+1), var_exp_ratio, marker=\"o\")\nplt.xlabel(\"Componente\")\nplt.ylabel(\"Proporción varianza explicada\")\nplt.title(\"Scree plot (Python)\")\nplt.show()\n\n# Varianza acumulada\nplt.figure()\nplt.plot(range(1, len(cum_exp_ratio)+1), cum_exp_ratio, marker=\"o\")\nplt.axhline(0.95, linestyle=\"--\")\nplt.xlabel(\"Componente\")\nplt.ylabel(\"Varianza acumulada\")\nplt.title(\"Varianza acumulada (Python)\")\nplt.show()\n```\n:::\n\n# Cargas (loadings) y scores\n\nLas **cargas** indican cuánto contribuye cada variable a cada componente (correlaciones variable–componente).\n\nLos **scores** son las coordenadas de las observaciones en el espacio de componentes.\n\n::: panel-tabset\n## R\n\n```{r}\n# Cargas\nloadings <- pca$rotation\nround(loadings, 3)\n\n# Scores\nscores <- pca$x\nhead(scores)\n\n# Biplot rápido\nbiplot(pca, cex = 0.7)  # base R\n```\n\n## Python\n\n```{python}\n# Cargas = vectores propios (componentes)\nloadings = pca.components_.T  # shape: n_features x n_components\npd.DataFrame(loadings, index=iris.feature_names,\n             columns=[f\"PC{i+1}\" for i in range(loadings.shape[1])]).round(3)\n\n# Scores (transformación de los datos)\nscores = pca.transform(X_scaled)\npd.DataFrame(scores, columns=[f\"PC{i+1}\" for i in range(scores.shape[1])]).head()\n\n# Biplot sencillo (PC1 vs PC2)\npc1, pc2 = 0, 1\nplt.figure()\nplt.scatter(scores[:, pc1], scores[:, pc2], alpha=0.7)\n# Vectores de variables\nfor i, feature in enumerate(iris.feature_names):\n    plt.arrow(0, 0, loadings[i, pc1]*3, loadings[i, pc2]*3, head_width=0.05)\n    plt.text(loadings[i, pc1]*3.2, loadings[i, pc2]*3.2, feature)\nplt.xlabel(\"PC1\")\nplt.ylabel(\"PC2\")\nplt.title(\"Biplot (Python)\")\nplt.axhline(0, linewidth=0.5); plt.axvline(0, linewidth=0.5)\nplt.show()\n```\n:::\n\n## Elección de número de componentes\n\nCriterios habituales:\n\n1.  **Codo** del scree plot.\n2.  Umbral de **varianza acumulada** (p. ej., 90–95%).\n3.  **Kaiser** (autovalores \\> 1) cuando se usa matriz de correlaciones.\n4.  **Validación** en tareas posteriores (clasificación/regresión).\n\n::: panel-tabset\n## R\n\n```{r}\n# Elegir k para alcanzar >= 95% varianza\n(k <- which(cum_exp_ratio >= 0.95)[1])\n```\n\n## Python\n\n```{python}\nk = int(np.argmax(cum_exp_ratio >= 0.95)) + 1\nk\n```\n:::\n\n# Proyección y reconstrucción\n\n::: panel-tabset\n## R\n\n```{r}\n# Proyectar datos a k componentes\nk <- which(cum_exp_ratio >= 0.95)[1]\nscores_k <- scores[, 1:k, drop = FALSE]\n\n# Reconstrucción aproximada (desde scores_k -> espacio original estandarizado)\n# X_std_hat = scores_k %*% t(loadings[,1:k])\nXstd_hat <- scores_k %*% t(loadings[, 1:k])\n\n# \"Desestandarizar\" para volver a escala original\nmeans <- attr(df_scaled, \"scaled:center\")\nsds   <- attr(df_scaled, \"scaled:scale\")\nXrec <- sweep(Xstd_hat, 2, sds, `*`)\nXrec <- sweep(Xrec, 2, means, `+`)\n\nas_tibble(head(Xrec))\n```\n\n## Python\n\n```{python}\nk = int(np.argmax(cum_exp_ratio >= 0.95)) + 1\nscores_k = scores[:, :k]\nload_k = loadings[:, :k]\n\n# Reconstrucción en espacio estandarizado\nXstd_hat = np.dot(scores_k, load_k.T)\n\n# Desestandarizar\nXrec = scaler.inverse_transform(Xstd_hat)\npd.DataFrame(Xrec, columns=iris.feature_names).head()\n```\n:::\n\n# Pipeline para uso en modelos\n\nEn ML, es común encadenar **escalado + PCA** y luego un modelo.\n\n::: panel-tabset\n## R\n\n```{r}\n# Ejemplo con caret o tidymodels (aquí base con prcomp para simplicidad)\n# Generar componentes y usar los primeros k como features\nk <- 2\npc_df <- as.data.frame(scores[, 1:k])\npc_df$Species <- iris$Species\nhead(pc_df)\n```\n\n## Python\n\n```{python}\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import cross_val_score\n\nk = 2\npipe = Pipeline([\n    (\"scaler\", StandardScaler()),\n    (\"pca\", PCA(n_components=k)),\n    (\"clf\", LogisticRegression(max_iter=1000))\n])\n\nscores_cv = cross_val_score(pipe, X, y, cv=5)\nscores_cv.mean(), scores_cv.std()\n```\n:::\n\n## Interpretación de cargas y biplot\n\n-   Vectores largos: variables con alta contribución al componente.\n-   Ángulos pequeños entre vectores: variables **correlacionadas.**\n-   Puntos (observaciones) próximos: perfiles similares en variables originales.\n-   Signo de la carga: dirección de la relación con el componente.\n\n# Trucos y errores comunes\n\n-   **No estandarizar** cuando hay escalas muy distintas distorsiona el resultado\n-   **Outliers**: revisar o usar alternativas robustas (p. ej., `robustbase` en R, `sklearn` + métodos robustos).\n-   **Muchas variables**: considerar **PCA incremental** si hay memoria limitada.\n-   **Interpretabilidad**: PCA rota ejes de forma no supervisada, no siempre mapea a factores *interpretables*.\n\n# Referencias rápidas (concepto)\n\n-   Componentes = autovectores de covarianzas/correlaciones; varianza = autovalores.\n-   `scores = X_estandarizado %*% loadings`\n-   Reconstrucción aproximada: `X̂_est = scores_k %*% t(loadings_k)`; desestandarizar para volver a escala original.\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"knitr"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../../styles.css"],"toc":true,"number-sections":true,"output-file":"ACP.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.6.37","theme":{"light":"cerulean","dark":"darkly"},"title":"Anàlisis de Components Principals (ACP)","author":"Dante Conti, Sergi Ramirez, (c) IDEAI","date":"`r Sys.Date()`","date-modified":"`r Sys.Date()`","editor":"visual"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}
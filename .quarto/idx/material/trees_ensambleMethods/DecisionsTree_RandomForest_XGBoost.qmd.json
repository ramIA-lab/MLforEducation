{"title":"Árboles de Decisión, Random Forest y XGBoost","markdown":{"yaml":{"title":"Árboles de Decisión, Random Forest y XGBoost","author":"Dante Conti, Sergi Ramirez, (c) IDEAI","date":"`r Sys.Date()`","date-modified":"`r Sys.Date()`","toc":true,"number-sections":true,"format":{"html":{"theme":"cerulean"}},"editor":"visual"},"headingText":"Descripción del problema","containsRefs":false,"markdown":"\n\n\nEn este ejemplo se entrena un árbol de regresión para predecir el precio unitario de la vivienda en Madrid. Para ello se utilizan los datos de viviendas a la venta en Madrid publicados en Idealista durante el año 2018. Estos datos están incluidos en el paquete `idealista18`. Las variables que contienen nuestra base de datos son las siguientes:\n\n-   ***\"ASSETID\" :*** Identificador único del activo\n\n-   ***\"PERIOD\" :*** Fecha AAAAMM, indica el trimestre en el que se extrajo el anuncio, utilizamos AAAA03 para el 1.er trimestre, AAAA06 para el 2.º, AAAA09 para el 3.er y AAAA12 para el 4.º\n\n-   ***\"PRICE\" :*** Precio de venta del anuncio en idealista expresado en euros\n\n-   ***\"UNITPRICE\" :*** Precio en euros por metro cuadrado\n\n-   ***\"CONSTRUCTEDAREA\" :*** Superficie construida de la casa en metros cuadrados\n\n-   ***\"ROOMNUMBER\" :*** Número de habitaciones\n\n-   ***\"BATHNUMBER\" :*** Número de baños\n\n-   ***\"HASTERRACE\" :*** Variable ficticia para terraza (toma 1 si hay una terraza, 0 en caso contrario)\n\n-   ***\"HASLIFT\" :*** Variable ficticia para ascensor (toma 1 si hay ascensor en el edificio, 0 en caso contrario)\n\n-   ***\"HASAIRCONDITIONING\" :*** Variable ficticia para Aire Acondicionado (toma 1 si hay una Aire Acondicionado, 0 en caso contrario)\n\n-   ***\"AMENITYID\" :*** Indica las comodidades incluidas (1 - sin muebles, sin comodidades de cocina, 2 - comodidades de cocina, sin muebles, 3 - comodidades de cocina, muebles)\n\n-   ***\"HASPARKINGSPACE\" :*** Variable ficticia para estacionamiento (toma 1 si el estacionamiento está incluido en el anuncio, 0 en caso contrario)\n\n-   ***\"ISPARKINGSPACEINCLUDEDINPRICE\" :*** Variable ficticia para estacionamiento (toma 1 si el estacionamiento está incluido en el anuncio, 0 en caso contrario)\n\n-   ***\"PARKINGSPACEPRICE\" :*** Precio de plaza de parking en euros\n\n-   ***\"HASNORTHORIENTATION\" :*** Variable ficticia para orientación (toma 1 si la orientación es Norte en el anuncio, 0 en caso contrario) - *Nota importante: las características de orientación no son características ortogonales, una casa orientada al norte también puede estar orientada al este*\n\n-   ***\"HASSOUTHORIENTATION\" :*** Variable ficticia para orientación (toma 1 si la orientación es Sur en el anuncio, 0 en caso contrario) - *Nota importante: las características de orientación no son características ortogonales, una casa orientada al norte también puede estar orientada al este*\n\n-   ***\"HASEASTORIENTATION\" :*** Variable ficticia para orientación (toma 1 si la orientación es Este en el anuncio, 0 en caso contrario) - *Nota importante: las características de orientación no son características ortogonales, una casa orientada al norte también puede estar orientada al este*\n\n-   ***\"HASWESTORIENTATION\" :*** Variable ficticia para orientación (toma 1 si la orientación es Oeste en el anuncio, 0 en caso contrario) - *Nota importante: las características de orientación no son características ortogonales, una casa orientada al norte también puede estar orientada al este*\n\n-   ***\"HASBOXROOM\" :*** Variable ficticia para boxroom (toma 1 si boxroom está incluido en el anuncio, 0 en caso contrario)\n\n-   ***\"HASWARDROBE\" :*** Variable ficticia para vestuario (toma 1 si el vestuario está incluido en el anuncio, 0 en caso contrario)\n\n-   ***\"HASSWIMMINGPOOL\" :*** Variable ficticia para piscina (toma 1 si la piscina está incluida en el anuncio, 0 en caso contrario)\n\n-   ***\"HASDOORMAN\" :*** Variable ficticia para portero (toma 1 si hay un portero en el edificio, 0 en caso contrario)\n\n-   ***\"HASGARDEN\" :*** Variable ficticia para jardín (toma 1 si hay un jardín en el edificio, 0 en caso contrario)\n\n-   ***\"ISDUPLEX\" :*** Variable ficticia para dúplex (toma 1 si es un dúplex, 0 en caso contrario)\n\n-   ***\"ISSTUDIO\" :*** Variable ficticia para piso de soltero (estudio en español) (toma 1 si es un piso para una sola persona, 0 en caso contrario)\n\n-   ***\"ISINTOPFLOOR\" :*** Variable ficticia que indica si el apartamento está ubicado en el piso superior (toma 1 en el piso superior, 0 en caso contrario)\n\n-   ***\"CONSTRUCTIONYEAR\" :*** Año de construcción (fuente: anunciante)\n\n-   ***\"FLOORCLEAN\" :*** Indica el número de piso del apartamento comenzando desde el valor 0 para la planta baja (fuente: anunciante)\n\n-   ***\"FLATLOCATIONID\" :*** Indica el tipo de vistas que tiene el piso (1 - exterior, 2 - interior)\n\n-   ***\"CADCONSTRUCTIONYEAR\" :*** Año de construcción según fuente catastral (fuente: catastro), tenga en cuenta que esta cifra puede diferir de la proporcionada por el anunciante\n\n-   ***\"CADMAXBUILDINGFLOOR\" :*** Superficie máxima del edificio (fuente: catastro)\n\n-   ***\"CADDWELLINGCOUNT\" :*** Recuento de viviendas en el edificio (fuente: catastro)\n\n-   ***\"CADASTRALQUALITYID\" :*** Calidad catastral (fuente: catastro)\n\n-   ***\"BUILTTYPEID_1\" :*** Valor ficticio para estado del piso: 1 obra nueva 0 en caso contrario (fuente: anunciante)\n\n-   ***\"BUILTTYPEID_2\" :*** Valor ficticio para condición plana: 1 segundero a restaurar 0 en caso contrario (fuente: anunciante)\n\n-   ***\"BUILTTYPEID_3\" :*** Valor ficticio para estado plano: 1 de segunda mano en buen estado 0 en caso contrario (fuente: anunciante)\n\n-   ***\"DISTANCE_TO_CITY_CENTER\" :*** Distancia al centro de la ciudad en km\n\n-   ***\"DISTANCE_TO_METRO\" :*** Distancia istancia a una parada de metro en km.\n\n-   ***\"DISTANCE_TO_DIAGONAL\" :*** Distancia a la Avenida Diagonal en km; Diagonal es una calle principal que corta la ciudad en diagonal a la cuadrícula de calles.\n\n-   ***\"LONGITUDE\" :*** Longitud del activo\n\n-   ***\"LATITUDE\" :*** Latitud del activo\n\n-   ***\"geometry\" :*** Geometría de características simples en latitud y longitud.\n\n**Fuente**: [Idealista](https://www.idealista.com/)\n\n```{r}\n#| label: cargar-datos\n#| echo: true\n#| eval: false\n#| warning: false\n#| message: false\n#| error: false\n\nlibrary(\"idealista18\")\nBCN <- get(data(\"Barcelona_Sale\"))\n```\n\n```{r}\n#| label: geo-BCN\n#| echo: true\n#| eval: false\n#| warning: false\n#| message: false\n#| error: false\n\n# Filtramos la epoca a Navidad\nBCN <- BCN[which(BCN$PERIOD == \"201812\"), ]\n\npisos_sf_BCN <- st_as_sf(BCN, coords = c(\"LONGITUDE\", \"LATITUDE\"), crs = 4326)\n\n# Leer shapefile de secciones censales\nsecciones <- st_read(\"C:/Users/sergi/Downloads/Shapefile/seccionado_2024/SECC_CE_20240101.shp\")\n\n# Transformar pisos al sistema de referencia de las secciones censales\npisos_sf_BCN <- st_transform(pisos_sf_BCN, crs = st_crs(secciones))\n\n# Hacer el match entre pisos y secciones censales\npisos_con_seccion <- st_join(pisos_sf_BCN, secciones, join = st_within)\n\n# Convertir a dataframe para exportar\nBCN <- as.data.frame(pisos_con_seccion)\n\nrm(Barcelona_Sale, Barcelona_Polygons, Barcelona_POIS, pisos_con_seccion, pisos_sf_BCN, secciones); gc()\n```\n\n```{r}\n#| label: cargar-renta-media\n#| echo: true\n#| eval: false\n#| warning: false\n#| message: false\n#| error: false\n\nrentaMedia <- read.csv(\"https://raw.githubusercontent.com/miguel-angel-monjas/spain-datasets/refs/heads/master/data/Renta%20media%20en%20Espa%C3%B1a.csv\")\n# NOs quedamos con los datos que nos interesa de Barcelona\nrentaMedia <- rentaMedia[which(rentaMedia$Provincia == \"Barcelona\" & rentaMedia$Tipo.de.elemento == \"sección\"), ]\nrentaMedia$Código.de.territorio <- paste0(\"0\", rentaMedia$Código.de.territorio)\n```\n\n```{r}\n#| label: unir-informacion\n#| echo: true\n#| eval: false\n#| warning: false\n#| message: false\n#| error: false\n\ncols <- c(\"Renta.media.por.persona\", \"Renta.media.por.hogar\")\n\nm <- match(BCN$CUSEC, rentaMedia$Código.de.territorio)\nBCN[, cols] <- rentaMedia[m, cols]\n```\n\n```{r}\n#| label: cargar-datos-final\n#| echo: false\n#| eval: true\n#| warning: false\n#| message: false\n#| error: false\n\npath <- 'https://raw.githubusercontent.com/ramIA-lab/MLforEducation/refs/heads/main/material/trees_ensambleMethods/idealista18_BCN_conRenta.csv'\nBCN <- read.csv2(path)\n```\n\n```{r}\n#| label: cargar-paquetes-r\n#| echo: false\n#| warning: false\n#| message: false\n#| error: false\n\nlibrary(dplyr)\nlibrary(tidyr)\n```\n\n# Preprocessing de los datos\n\n```{r}\n#| label: preprocessing-R\n#| echo: true\n#| warning: false\n#| message: false\n#| error: false\n\nBCN <- BCN %>%\n  select(-X, -PRICE, -LONGITUDE, -LATITUDE, -geometry, -CONSTRUCTIONYEAR, \n         -ASSETID, -PERIOD, -CUSEC, -CSEC, -CMUN, -CPRO, -CCA, -CUDIS, -CLAU2, \n         -NPRO, -NCA, -CNUT0, -CNUT1, -CNUT2, -CNUT3, -NMUN, -Shape_Leng,\n         -Shape_Area, -geometry, -CUMUN, -CADASTRALQUALITYID) %>%\n  mutate(\n    across(\n      .cols = starts_with(c(\"HAS\", \"IS\")),\n      .fns = ~ case_when(. == 0 ~ \"No\", . == 1 ~ \"Si\"),\n      .names = \"{.col}\"), \n    AMENITYID = case_when(\n      AMENITYID == 1 ~ \"SinMuebleSinCocina\", AMENITYID == 2 ~ \"CocinaSinMuebles\", \n      AMENITYID == 3 ~ \"CocinaMuebles\"), \n    FLATLOCATIONID = case_when(\n      FLATLOCATIONID == 1 ~ \"exterior\", FLATLOCATIONID == 2 ~ \"interior\", \n      .default = \"noInfo\"),\n    BUILTTYPEID_1 = case_when(\n      BUILTTYPEID_1 == 0 ~ \"noObraNueva\", BUILTTYPEID_1 == 1 ~ \"obraNueva\"),\n    BUILTTYPEID_2 = case_when(\n      BUILTTYPEID_2 == 0 ~ \"noRestaurar\", BUILTTYPEID_2 == 1 ~ \"Restaurar\"),\n    BUILTTYPEID_3 = case_when(\n      BUILTTYPEID_3 == 0 ~ \"noSegundaMano\", BUILTTYPEID_3 == 1 ~ \"SegundaMano\"),\n    FLOORCLEAN = replace_na(FLOORCLEAN, 0),\n    CDIS = case_when(\n      CDIS == 1 ~ \"Ciutat-Vella\", CDIS == 2 ~ \"Eixample\", CDIS == 3 ~ \"Sants-Montjuic\", \n      CDIS == 4 ~ \"Les Corts\", CDIS == 5 ~ \"Sarrià-Sant Gervasi\", \n      CDIS == 6 ~ \"Gràcia\", CDIS == 7 ~ \"Horta-Guinardó\", CDIS == 8 ~ \"Nou Barris\",\n      CDIS == 9 ~ \"Sant Andreu\", CDIS == 10 ~ \"Sant Martí\"),\n    RENTA = case_when(\n     Renta.media.por.hogar < 30000 ~ \"Baja\",\n     Renta.media.por.hogar >= 30000 & Renta.media.por.hogar <= 50000 ~ \"Media\",\n     Renta.media.por.hogar > 50000 ~ \"Alta\"\n    )\n  ) %>%\n  select(-Renta.media.por.hogar, -Renta.media.por.persona)\n```\n\n## Análisi descriptivo de los datos\n\n```{r}\n#| label: descriptiva-R\n#| echo: true\n#| warning: false\n#| message: false\n#| error: false\n\n## Descriptiva de los datos\nlibrary(DataExplorer)\nlibrary(lubridate)\nlibrary(dplyr)\n\n## Data Manipulation\nlibrary(reshape2)\n\n## Plotting\nlibrary(ggplot2)\n\n## Descripción completa\nDataExplorer::introduce(BCN)\n\n## Descripción de la bbdd\nplot_intro(BCN)\n\n## Descripción de los missings\nplot_missing(BCN)\n\n## Descripción de las varaibles categoricas\nplot_bar(BCN)\n\n## Descripción variables numéricas\nplot_histogram(BCN)\nplot_density(BCN)\nplot_qq(BCN)\nplot_correlation(BCN)\n```\n\n# Data Manipulation\n\n::: panel-tabset\n## R\n\n```{r}\n#| label: gestion-datos-R\n#| echo: true\n#| warning: false\n#| message: false\n#| error: false\n\nlibrary(caret)\n\nset.seed(1994)\n\nindex <- caret::createDataPartition(BCN$UNITPRICE, p = 0.8, list = FALSE)\nrtrain <- BCN %>% slice(index) %>% na.omit()\nrtest <- BCN %>% slice(-index) %>% na.omit()\n```\n\n## Python\n\n```{python}\n#| label: conversor-r_py\n#| echo: false\n#| warning: false\n#| message: false\n#| error: false\n\npyBCN = r.BCN\n```\n\n```{python}\n#| label: preprocessing-py\n#| echo: true\n#| warning: false\n#| message: false\n#| error: false\n\nfrom sklearn.preprocessing import LabelEncoder\n\n# Crear un LabelEncoder\nle = LabelEncoder()\n\n# Lista de variables a transformar\ncolumns_to_encode = ['HASTERRACE', 'HASLIFT', 'HASAIRCONDITIONING', 'AMENITYID',\n    'HASPARKINGSPACE', 'ISPARKINGSPACEINCLUDEDINPRICE', 'HASNORTHORIENTATION',\n    'HASSOUTHORIENTATION', 'HASEASTORIENTATION', 'HASWESTORIENTATION',\n    'HASBOXROOM', 'HASWARDROBE', 'HASSWIMMINGPOOL', 'HASDOORMAN', 'HASGARDEN',\n    'ISDUPLEX', 'ISSTUDIO', 'ISINTOPFLOOR', 'FLOORCLEAN', 'FLATLOCATIONID',\n    'CADMAXBUILDINGFLOOR', 'CADDWELLINGCOUNT','BUILTTYPEID_1', 'BUILTTYPEID_2', \n    'BUILTTYPEID_3', 'CDIS', 'RENTA']\n\n# Aplicar LabelEncoder a cada columna de la lista\nfor col in columns_to_encode:\n    if col in pyBCN.columns:  # Verificar que la columna existe en el DataFrame\n        pyBCN[col] = le.fit_transform(pyBCN[col].astype(str))  # Convertir a string si no es categórica\n\n```\n\n```{python}\n#| label: gestion-datos-Python\n#| echo: true\n#| warning: false\n#| message: false\n#| error: false\n\nfrom sklearn.model_selection import train_test_split\n\n# dividimos la base de datos \nX = pyBCN.drop(columns=[\"RENTA\"])\n\npyX_train, pyX_test, pyy_train, pyy_test = train_test_split(\n    X, pyBCN['RENTA'], test_size = 0.2, random_state = 1994)\n```\n\n```{python}\n#| label: normalizacion-python\n#| echo: true\n#| warning: false\n#| message: false\n#| error: false\n\nfrom sklearn.preprocessing import StandardScaler\n\n# Scale dataset\nsc = StandardScaler()\npyX_train = sc.fit_transform(pyX_train)\npyX_test = sc.fit_transform(pyX_test)\n```\n:::\n\n# Árboles de decisión\n\n## Creación del árbol\n\n::: panel-tabset\n## R\n\n```{r}\n#| label: arbol-training-r\n#| echo: true\n#| warning: false\n#| message: false\n#| error: false\n\nlibrary(rpart)\nlibrary(rpart.plot)\n\nset.seed(1994)\n\narbol <- rpart(RENTA ~ ., data = rtrain)\nsummary(arbol)\n```\n\n```{r}\n#| label: plot-arbol-r\n#| echo: true\n#| warning: false\n#| message: false\n#| error: false\n\nrpart.plot(arbol)\n```\n\n## Python\n\n```{python}\n#| label: arbol-training-python\n#| echo: true\n#| warning: false\n#| message: false\n#| error: false\n\n# Decision Tree Classification\nfrom sklearn.tree import DecisionTreeClassifier\n\nclassifier = DecisionTreeClassifier(criterion = 'entropy', random_state = 1994)\nclf = classifier.fit(pyX_train, pyy_train)\n```\n\n```{python}\n#| label: plot-arbol-python\n#| echo: true\n#| warning: false\n#| message: false\n#| error: false\n\nfrom sklearn import tree\n\ntree.plot_tree(clf)\n```\n:::\n\n## Creamos las predicciones\n\n::: panel-tabset\n## R\n\nAplicamos el modelo a nuestros valores de test.\n\n```{r}\n#| label: decisionsTree-predict-r\n#| echo: true\n#| warning: false\n#| message: false\n#| error: false\n\npredict(arbol, rtest[1:10, ])\n```\n\n```{r}\n#| label: decisionsTree-confMatrix-r\n#| echo: true\n#| warning: false\n#| message: false\n#| error: false\n\npredicciones <- predict(arbol, rtrain, type = \"class\")\ncaret::confusionMatrix(predicciones, as.factor(rtrain$RENTA))\n\npredicciones <- predict(arbol, rtest, type = \"class\")\ncaret::confusionMatrix(predicciones, as.factor(rtest$RENTA))\n```\n\n```{r}\n#| label: plot-confusionMatrix-decisionTree-r\n#| echo: true\n#| warning: false\n#| message: false\n#| error: false\n\nCM <- caret::confusionMatrix(predicciones, as.factor(rtest$RENTA)); CM <- data.frame(CM$table)\n\ngrafico <- ggplot(CM, aes(Prediction,Reference, fill= Freq)) +\n        geom_tile() + geom_text(aes(label=Freq)) +\n        scale_fill_gradient(low=\"white\", high=\"#009194\") +\n        labs(x = \"Reference\",y = \"Prediction\")\n\nplot(grafico)\n```\n\n## Python\n\n```{python}\n#| label: decisionsTree-predict-python\n#| echo: true\n#| warning: false\n#| message: false\n#| error: false\n\nimport pandas as pd\n\n# Prediction\ny_pred = classifier.predict(pyX_test)\n\nresults = pd.DataFrame({\n    'Real': pyy_test,  # Valores reales\n    'Predicho': y_pred  # Valores predichos\n})\n\n# Muestra los primeros 5 registros\nprint(results.head())  \n```\n\n```{python}\n#| label: decisionsTree-confMatrix-python\n#| echo: true\n#| warning: false\n#| message: false\n#| error: false\n#| \nfrom sklearn.metrics import classification_report\n\nprint(f'Classification Report: \\n{classification_report(pyy_test, y_pred)}')\n```\n\n```{python}\n#| label: plot-confMatrix-python\n#| echo: true\n#| warning: false\n#| message: false\n#| error: false\n\nfrom sklearn.metrics import confusion_matrix\nimport seaborn as sns\n\n# Confusion matrix\ncf_matrix = confusion_matrix(pyy_test, y_pred)\nsns.heatmap(cf_matrix, annot=True, fmt='d', cmap='Blues', cbar=False)\n```\n:::\n\n## Modelo de classificación con cross-evaluación\n\n::: panel-tabset\n## R\n\n```{r}\n#| label: decisionsTree-crossvalidation-r\n#| echo: true\n#| warning: false\n#| message: false\n#| error: false\n#| \n## Generamos los parámetros de control\ntrControl <- trainControl(method = \"cv\", number = 10, classProbs = TRUE,\n  summaryFunction = multiClassSummary)\n## En este caso, se realiza una cros-validación de 10 etapas\n\n# se fija una semilla aleatoria\nset.seed(1994)\n\n# se entrena el modelo\nmodel <- train(RENTA ~ .,  # . equivale a incluir todas las variables\n               data = rtrain,\n               method = \"rpart\",\n               metric = \"Accuracy\",\n               trControl = trControl)\n\n# Obtenemos los valores del árbol óptimo\nmodel$finalModel\n\n# Generamos el gráfico del árbol\nrpart.plot(model$finalModel)\n```\n\n```{r}\n#| label: plot-resultados-crossvalidation-r\n#| echo: true\n#| warning: false\n#| message: false\n#| error: false\n\nlibrary(reshape2)\n\n# A continuación generamos un gráfico que nos permite ver la variabilidad de los estadísticos\n# calculados\nggplot(melt(model$resample[,c(2:5, 7:9, 12:13)]), aes(x = variable, y = value, fill=variable)) +\n  geom_boxplot(show.legend=FALSE) +\n  xlab(NULL) + ylab(NULL)\n```\n\n## Python\n\n```{python}\n#| label: decisionsTree-crossvalidation-python\n#| echo: true\n#| warning: false\n#| message: false\n#| error: false\n\nfrom sklearn.tree import DecisionTreeClassifier\n\n# Modelo de árbol de decisión\nmodel = DecisionTreeClassifier(random_state=1994)\n\nfrom sklearn.model_selection import cross_val_score\n\n# Realizar validación cruzada con 5 folds\nscores = cross_val_score(model, pyX_train, pyy_train, cv=10, scoring = 'accuracy')  # Métrica: accuracy\n\n# Mostrar resultados\nprint(f\"Accuracy por fold: {scores}\")\nprint(f\"Accuracy promedio: {scores.mean():.4f}\")\n```\n:::\n\n## Realizando hiperparámetro tunning\n\n::: panel-tabset\n## R\n\n```{r}\n#| label: parametros-decisionTree-r\n#| echo: true\n#| warning: false\n#| message: false\n#| error: false\n\n# Detectamos cuales son los parámetros del modelo que podemos realizar hiperparámeter tunning\nmodelLookup(\"rpart\")\n```\n\n```{r}\n#| label: parametros-decisionTree-grid-r\n#| echo: true\n#| warning: false\n#| message: false\n#| error: false\n\n# Se especifica un rango de valores típicos para el hiperparámetro\ntuneGrid <- expand.grid(cp = seq(0.01,0.05,0.01))\n```\n\n```{r}\n#| label: hyperparam-decisionTree-r\n#| echo: true\n#| warning: false\n#| message: false\n#| error: false\n\n# se entrena el modelo\nset.seed(1994)\n\nmodel <- train(RENTA ~ .,\n               data = rtrain,\n               method = \"rpart\",\n               metric = \"Accuracy\",\n               trControl = trControl,\n               tuneGrid = tuneGrid)\n\n# Obtenemos la información del mejor modelo\nmodel$bestTune\n\n# Gráfico del árbol obtenido\nrpart.plot(model$finalModel)\n```\n\n## Python\n\n```{python}\n#| label: hyperparam-decisionTree-python\n#| echo: true\n#| warning: false\n#| message: false\n#| error: false\n\nfrom sklearn.model_selection import GridSearchCV\n\n# Definir rejilla de hiperparámetros\nparam_grid = {\n    'max_depth': [None, 5, 10],\n    'min_samples_split': [2, 5, 10],\n    'min_samples_leaf': [1, 2, 4]\n}\n\n# Declaramos el modelo\nmodel = DecisionTreeClassifier(random_state=1994)\n\n# Configurar GridSearch con validación cruzada\ngrid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=10, scoring='accuracy', n_jobs=-1)\n\n# Ajustar modelo\ngrid_search.fit(pyX_train, pyy_train)\n\n# Mostrar mejores parámetros\nprint(f\"Mejores parámetros: {grid_search.best_params_}\")\nprint(f\"Mejor accuracy: {grid_search.best_score_:.4f}\")\n\n\nfrom sklearn import tree\ntree.plot_tree(grid_search.best_estimator_)\n```\n:::\n\n## Como realizar poda de nuestro árbol\n\n::: panel-tabset\n## R\n\n```{r}\n#| label: poda-decisionTree-r\n#| echo: true\n#| warning: false\n#| message: false\n#| error: false\n\n# Con el objetivo de aumentar la generalidad del árbol y facilitar su interpretación, \n# se procede a reducir su tamaño podándolo. Para ello se establece el criterio de \n# que un nodo terminal tiene que tener, como mínimo, 50 observaciones.\nset.seed(1994)\nprunedtree <- rpart(RENTA ~ ., data = rtrain,\n                    cp= 0.01, control = rpart.control(minbucket = 50))\n\nrpart.plot(prunedtree)\n```\n\n## Python\n\nEn Python, la poda de un árbol de decisión se puede realizar ajustando los hiperparámetros del árbol durante su creación. Estos hiperparámetros controlan el crecimiento del árbol y, por lo tanto, actúan como técnicas de poda preventiva o postpoda.\n\n`scikit-learn` no implementa poda dinámica directa (como ocurre en algunos otros frameworks), pero puedes limitar el tamaño del árbol y evitar sobreajuste mediante los siguientes métodos.\n\n### Poda Preventiva (Pre-pruning)\n\n**Poda preventiva** consiste en detener el crecimiento del árbol antes de que se haga demasiado grande. Esto se logra ajustando hiperparámetros como:\n\n-   `max_depth`: Profundidad máxima del árbol\n-   `min_samples_split`: Número mínimo de muestras necesarias para dividir un nodo.\n-   `min_samples_leaf`: Número mínimo de muestras necesarias en una hoja.\n-   `max_leaf_nodes`: Número máximo de nodos hoja en el árbol.\n\n```{python}\n#| label: podaPreventiva-decisionTree-python\n#| echo: true\n#| warning: false\n#| message: false\n#| error: false\n\n# Crear un árbol con poda preventiva\nmodel = DecisionTreeClassifier(\n    max_depth=3,              # Limitar la profundidad\n    min_samples_split=10,     # Mínimo 10 muestras para dividir un nodo\n    min_samples_leaf=5,       # Mínimo 5 muestras por hoja\n    random_state=42\n)\n\n# Entrenar el modelo\nmodel.fit(pyX_train, pyy_train)\n\n# Evaluar\nprint(f\"Accuracy en entrenamiento: {model.score(pyX_train, pyy_train):.4f}\")\nprint(f\"Accuracy en prueba: {model.score(pyX_test, pyy_test):.4f}\")\n\n# Graficamos el árbol podado\ntree.plot_tree(model)\n```\n\n### Poda Posterior (Post-Pruning) con `ccp_alpha`\n\nSe puedes realizar poda posterior usando cost **complexity pruning**. Esto implica ajustar el parámetro `ccp_alpha` (el parámetro de complejidad de coste).\n\nEl árbol generará múltiples subárboles podados para diferentes valores de `ccp_alpha`, y tú puedes elegir el más adecuado evaluando su desempeño.\n\n```{python}\n#| label: PostPruning-decisionTree-python\n#| echo: true\n#| warning: false\n#| message: false\n#| error: false\n\nimport matplotlib.pyplot as plt\n\n# Crear un árbol sin poda\nmodel = DecisionTreeClassifier(random_state=1994)\nmodel.fit(pyX_train, pyy_train)\n\n# Obtener valores de ccp_alpha\npath = model.cost_complexity_pruning_path(pyX_train, pyy_train)\nccp_alphas = path.ccp_alphas\nimpurities = path.impurities\n\n# Entrenar árboles para cada valor de ccp_alpha\nmodels = []\nfor ccp_alpha in ccp_alphas:\n    clf = DecisionTreeClassifier(random_state=42, ccp_alpha=ccp_alpha)\n    clf.fit(pyX_train, pyy_train)\n    models.append(clf)\n\n# Evaluar desempeño\ntrain_scores = [clf.score(pyX_train, pyy_train) for clf in models]\ntest_scores = [clf.score(pyX_test, pyy_test) for clf in models]\n\n# Graficar resultados\nplt.figure(figsize=(8, 6))\nplt.plot(ccp_alphas, train_scores, marker='o', label=\"Train Accuracy\", drawstyle=\"steps-post\")\nplt.plot(ccp_alphas, test_scores, marker='o', label=\"Test Accuracy\", drawstyle=\"steps-post\")\nplt.xlabel(\"ccp_alpha\")\nplt.ylabel(\"Accuracy\")\nplt.title(\"Accuracy vs ccp_alpha\")\nplt.legend()\nplt.grid()\nplt.show()\n```\n:::\n\n# Random Forest\n\n## Aplicación del modelo\n\n::: panel-tabset\n## R\n\n```{r}\n#| label: randomForest-r\n#| echo: true\n#| warning: false\n#| message: false\n#| error: false\n#| \n# Random Forest \nlibrary(randomForest)\n## devtools::install_github('araastat/reprtree') # Se instala 1 vez para poder printar graficos\nlibrary(reprtree)\n\nset.seed(1994)\narbol_rf <- randomForest(as.factor(RENTA) ~ .,  data = rtrain, ntree = 25)\n```\n\n```{r}\n#| label: observar-arbol-r\n#| echo: true\n#| warning: false\n#| message: false\n#| error: false\n\n# se observa el árbol número 20\ntree20 <- getTree(arbol_rf, 20, labelVar = TRUE)\nhead(tree20)\n\n## Sin embargo, el método por el que se representa gráficamente no es muy claro y\n## puede llevar a confusión o dificultar la interpretación del árbol. \n## Si se desea estudiar el árbol, hasta un cierto nivel, se puede incluir el argumento depth.\n## El árbol, ahora con una profundidad de 5 ramas.\nplot.getTree(arbol_rf, k = 20, depth = 5)\n```\n\n```{r}\n#| label: importance-matrix-r\n#| echo: true\n#| warning: false\n#| message: false\n#| error: false\n\nlibrary(vip)\nvip(arbol_rf)\n```\n\n## Python\n\n```{python}\n#| label: randomForest-python\n#| echo: true\n#| warning: false\n#| message: false\n#| error: false\n\nfrom sklearn.ensemble import RandomForestClassifier\n\nclf = RandomForestClassifier()\nclf.fit(pyX_train, pyy_train)\n```\n\n```{python}\n#| label: importance-matrix-python\n#| echo: true\n#| warning: false\n#| message: false\n#| error: false\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nresults = pd.DataFrame(clf.feature_importances_, index=pyBCN.columns[:-1]).sort_values(by=0, ascending=False)\n\n# Crear gráfico de barras horizontales\nplt.figure(figsize=(10, 8))\nplt.barh(results.index, results[0], color='skyblue')\n\n# Añadir etiquetas y título\nplt.xlabel('Importancia')\nplt.ylabel('Características')\nplt.title('Importancia de las Características')\nplt.grid(axis='x', linestyle='--', alpha=0.7)\nplt.show()\n```\n:::\n\n## Hiperparameter tunning de Random Forest\n\n::: panel-tabset\n## R\n\n```{r}\n#| label: parametros-RF-r\n#| echo: true\n#| warning: false\n#| message: false\n#| error: false\n\n# Identificamos los parámetros que podemos tunnerar\nmodelLookup(\"rf\")\n```\n\n```{r}\n#| label: gridtunning-RF-r\n#| echo: true\n#| warning: false\n#| message: false\n#| error: false\n\n# Se especifica un rango de valores posibles de mtry\ntuneGrid <- expand.grid(mtry = c(1, 2, 5, 10))\ntuneGrid\n```\n\n```{r}\n#| label: tunning-RF-r\n#| echo: true\n#| warning: false\n#| message: false\n#| error: false\n\n# se fija la semilla aleatoria\nset.seed(1994)\n\n# se entrena el modelo\nmodel <- train(RENTA ~ ., data = rtrain, \n               ntree = 20,\n               method = \"rf\", metric = \"Accuracy\",\n               tuneGrid = tuneGrid,\n               trControl = trainControl(classProbs = TRUE))\n\n# Visualizamos los hiperparámetros obtenidos \nmodel$results\n```\n\n## Python\n\n### Ajuste de hiperparámetros con `GridSearchCV`\n\nEl `GridSearchCV` realiza una búsqueda exhaustiva sobre un conjunto de parámetros especificados. Probará todas las combinaciones posibles de hiperparámetros.\n\n```{python}\n#| label: GridSearchCV-RF-python\n#| echo: true\n#| eval: false\n#| warning: false\n#| message: false\n#| error: false\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n\n# Definir los parámetros para la búsqueda\nparam_dist = {\n    'n_estimators': [150, 200],        # Número de árboles en el bosque\n    'max_depth': [None, 10, 20],            # Profundidad máxima del árbol\n    'min_samples_split': [2, 5, 10],            # Número mínimo de muestras para dividir un nodo\n    'min_samples_leaf': [1, 2, 4],               # Número mínimo de muestras en una hoja\n    'max_features': ['auto'],      # Número de características a considerar para dividir un nodo\n    'bootstrap': [True]                      # Si usar bootstrap para los árboles\n}\n\n# Crear el modelo RandomForest\nrf = RandomForestClassifier(random_state = 1994)\n\n# Usar GridSearchCV para encontrar el mejor conjunto de parámetros\ngrid_search = GridSearchCV(estimator = rf, param_grid = param_grid, cv = 10, n_jobs = -1, verbose = 0)\n\n# Ajustar el modelo con los datos de entrenamiento\ngrid_search.fit(pyX_train, pyy_train)\n\n# Mostrar los mejores parámetros encontrados\nprint(\"Mejores parámetros encontrados:\", grid_search.best_params_)\n\ntree.plot_tree(grid_search.best_estimator_)\n```\n\n### Ajuste de Hiperparámetros con `RandomizedSearchCV`\n\n`RandomizedSearchCV` es una técnica más eficiente que `GridSearchCV`, ya que no prueba todas las combinaciones posibles, sino un número limitado de combinaciones aleatorias dentro de un rango definido. Esto es útil si el espacio de búsqueda es grande y quieres evitar un tiempo de cómputo muy largo.\n\n```{python}\n#| label: RandomizedSearchCV-RF-python\n#| echo: true\n#| eval: false\n#| warning: false\n#| message: false\n#| error: false\n\n# Definir los parámetros para la búsqueda aleatoria\nparam_dist = {\n    'n_estimators': [150, 200],        # Número de árboles en el bosque\n    'max_depth': [None, 10, 20],            # Profundidad máxima del árbol\n    'min_samples_split': [2, 5, 10],            # Número mínimo de muestras para dividir un nodo\n    'min_samples_leaf': [1, 2, 4],               # Número mínimo de muestras en una hoja\n    'max_features': ['auto'],      # Número de características a considerar para dividir un nodo\n    'bootstrap': [True]                      # Si usar bootstrap para los árboles\n}\n\n# Usar RandomizedSearchCV para búsqueda aleatoria\nrandom_search = RandomizedSearchCV(estimator=rf, param_distributions=param_dist, \n                                   n_iter=50, cv=10, n_jobs=-1, random_state=1994)\n\n# Ajustar el modelo con los datos de entrenamiento\nrandom_search.fit(X_train, y_train)\n\n# Mostrar los mejores parámetros encontrados\nprint(\"Mejores parámetros encontrados:\", random_search.best_params_)\n\ntree.plot_tree(random_search.best_estimator_)\n```\n:::\n\n## Predicciones del algoritmo\n\n::: panel-tabset\n## R\n\n```{r}\n#| label: predict-RF-r\n#| echo: true\n#| warning: false\n#| message: false\n#| error: false\n\nprediccion <- predict(arbol_rf, rtrain, type = \"class\")\ncaret::confusionMatrix(prediccion, as.factor(rtrain$RENTA))\n\n# Realizamos las predicciones de este ultimo arbol para la predicción de test\n## Si no decimos nada en type (type = prob), nos devolvera la probabilidad de \n## pertenecer a cada clase. \nprediccion <- predict(arbol_rf, rtest, type = \"class\")\n## Para ver la performance, realizaremos la matriz de confusión \ncaret::confusionMatrix(prediccion, as.factor(rtest$RENTA))\n```\n\n```{r}\n#| label: plot-confusionMatrix-RF-r\n#| echo: true\n#| warning: false\n#| message: false\n#| error: false\n\nCM <- caret::confusionMatrix(prediccion, as.factor(rtest$RENTA)); CM <- data.frame(CM$table)\n\ngrafico <- ggplot(CM, aes(Prediction,Reference, fill= Freq)) +\n        geom_tile() + geom_text(aes(label=Freq)) +\n        scale_fill_gradient(low=\"white\", high=\"#009194\") +\n        labs(x = \"Reference\",y = \"Prediction\")\n\nplot(grafico)\n```\n\n## Python\n\n```{python}\n#| label: predict-RF-python\n#| echo: true\n#| warning: false\n#| message: false\n#| error: false\n\npreds = clf.predict(pyX_test)\nprint(f'Classification Report: \\n{classification_report(pyy_test, preds)}')\n```\n\n```{python}\n#| label: plot-confusionMatrix-RF-python\n#| echo: true\n#| warning: false\n#| message: false\n#| error: false\n\n# Confusion matrix\ncf_matrix = confusion_matrix(pyy_test, preds)\nsns.heatmap(cf_matrix, annot=True, fmt='d', cmap='Blues', cbar=False)\n```\n:::\n\n# eXtrem Gradient Boosting (XGBoost)\n\n## Aplicamos el algoritmo con cross-validation e hiperparameter tunning\n\n::: panel-tabset\n\n## R\n\nPara aplicar los modelos de XGBoost es necesario pasar los datos categoricos en dummies. Una variable *dummy* (también conocida como cualitativa o binaria) es aquella que toma el valor 1 o 0 para indicar la presencia o ausencia de una cierta característica o condición. \n\n```{r}\n#| label: creacionDummies-Xgboost-r\n#| echo: false\n#| eval: false\n#| warning: false\n#| message: false\n#| error: false\n\nlibrary(xgboost)\nlibrary(Matrix)\nlibrary(fastDummies)\n\n\n# Creamos las dummies de cada base de datos \ndtrain <- sparse.model.matrix(RENTA ~ ., data = rtrain)[,-1]\ndtest <- sparse.model.matrix(RENTA ~ ., data = rtest)[,-1]\n\nlibrary(dplyr)\nlibrary(fastDummies)\n\n# Definir el nombre de la variable respuesta\nrespuesta_var <- \"RENTA\"\n\n# Identificar las columnas categóricas (factor o character)\ncategorical_vars <- rtrain %>%\n  select(-all_of(respuesta_var)) %>%\n  select(where(~ is.factor(.) || is.character(.))) %>%\n  names()\n\n# Generar dummies y procesar el dataframe\ndtrain <- rtrain %>%\n  select(-all_of(categorical_vars)) %>% # Eliminar las columnas categóricas originales\n  bind_cols(dummy_cols(df, select_columns = categorical_vars, remove_selected_columns = TRUE)) %>%\n  bind_cols(select(df, all_of(respuesta_var))) # Volver a añadir la variable respuesta\n\n# https://rpubs.com/mharris/multiclass_xgboost\n# https://cran.r-project.org/web/packages/xgboost/vignettes/discoverYourData.html\n```\n\n```{r}\n#| label: parametros-Xgboost-r\n#| echo: false\n#| eval: false\n#| warning: false\n#| message: false\n#| error: false\n\n# Miramos cuales son los parámetros de este algoritmo \nhead(modelLookup(\"xgbTree\"),4)\n\n# se determina la semilla aleatoria\nset.seed(1994)\n\n# Todas las variables excepto eta són por defecto\ntuneGrid <- expand.grid(nrounds = c(5),                   # Menor cantidad de rondas \n                        max_depth = c(6),                 # Profundidad más baja para los árboles\n                        eta = c(0.01, 0.1, 0.3, 0.5),     # Tasa de aprendizaje más alta\n                        gamma = c(1),                     # Regularización\n                        colsample_bytree = c(1),          # Submuestreo de características\n                        min_child_weight = c(1),          # Regularización adicional\n                        subsample = c(1))                 # Submuestreo de datos\n                                         \n```\n\n```{r}\n#| label: algoritmo-Xgboost-r\n#| echo: false\n#| eval: false\n#| warning: false\n#| message: false\n#| error: false\n\n## se determina la semilla aleatoria\nset.seed(1994)\n\n## se entrena el modelo\nmodel <- train(as.factor(RENTA) ~. , \n               data = rtrain, \n               method = \"xgbTree\", \n               objective = \"multi:softprob\",\n               metric = \"Accuracy\",\n               trControl = trainControl(classProbs = TRUE, method = \"cv\", number = 10, \n                                        verboseIter = FALSE), \n               tuneGrid = tuneGrid)\n\n# se muestra la salida del modelo\nmodel$bestTune\n```\n\n```{r}\n#| label: importance-Xgboost-r\n#| echo: false\n#| eval: false\n#| warning: false\n#| message: false\n#| error: false\n\nlibrary(vip)\nvip(model$finalModel)\n```\n\n## Python\n\n```{python}\n#| label: tratamientoDatos-Xgboost-python\n#| echo: false\n#| eval: false\n#| warning: false\n#| message: false\n#| error: false\n\nimport xgboost as xgb\n\n# Convertir los datos a DMatrix\ndtrain = xgb.DMatrix(pyX_train, label = pyy_train)\ndtest = xgb.DMatrix(pyX_test, label = pyy_test)\n```\n\n```{python}\n#| label: parametros-Xgboost-python\n#| echo: false\n#| eval: false\n#| warning: false\n#| message: false\n#| error: false\n\nparams = {\n    'objective': 'multi:softmax',  # Problema de clasificación multiclase\n    'num_class': 3,                # Número de clases (para el conjunto de datos Iris)\n    'max_depth': 3,                # Profundidad máxima de los árboles\n    'eta': 0.1,                    # Tasa de aprendizaje\n    'eval_metric': 'merror'        # Métrica de evaluación: error en clasificación\n}\n```\n\n```{python}\n#| label: algoritmo-Xgboost-python\n#| echo: false\n#| eval: false\n#| warning: false\n#| message: false\n#| error: false\n\n# Entrenar el modelo\nnum_round = 50  # Número de iteraciones (rounds) de boosting\nmodel = xgb.train(params, dtrain, num_round)\n```\n\n```{python}\n#| label: importance-Xgboost-python\n#| echo: false\n#| eval: false\n#| warning: false\n#| message: false\n#| error: false\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nresults = pd.DataFrame(model.feature_importances_, index=pyBCN.columns[:-1]).sort_values(by=0, ascending=False)\n\n# Crear gráfico de barras horizontales\nplt.figure(figsize=(10, 8))\nplt.barh(results.index, results[0], color='skyblue')\n\n# Añadir etiquetas y título\nplt.xlabel('Importancia')\nplt.ylabel('Características')\nplt.title('Importancia de las Características')\nplt.grid(axis='x', linestyle='--', alpha=0.7)\nplt.show()\n```\n\nSi quisieramos hacerlo en cross validación hariamos lo siguiente:\n\n```{python}\n#| label: cv-Xgboost-python\n#| echo: false\n#| eval: false\n#| warning: false\n#| message: false\n#| error: false\n\nfrom sklearn.model_selection import GridSearchCV\nfrom xgboost import XGBClassifier\n\n# Definir el modelo\nxgb_model = XGBClassifier(objective='multi:softmax', num_class=3)\n\n# Definir los hiperparámetros a ajustar\nparam_grid = {\n    'max_depth': [3, 5, 7],\n    'eta': [0.01, 0.1, 0.3],\n    'n_estimators': [50, 100, 200]\n}\n\n# Configurar GridSearchCV\ngrid_search = GridSearchCV(estimator=xgb_model, param_grid=param_grid, cv=3, scoring='accuracy')\n\n# Entrenar el modelo con ajuste de hiperparámetros\ngrid_search.fit(X_train, y_train)\n\n# Mostrar los mejores parámetros y resultados\nprint(f\"Mejores parámetros: {grid_search.best_params_}\")\nprint(f\"Mejor precisión: {grid_search.best_score_:.4f}\")\n```\n:::\n\n## Predicciones\n\n::: panel-tabset\n## R\n\n```{r}\n#| label: predicciones-Xgboost-r\n#| echo: false\n#| eval: false\n#| warning: false\n#| message: false\n#| error: false\n\nprediccion <- predict(model$finalModel, dtest, type = \"prob\")\n\n# Realizamos las predicciones de este ultimo arbol para la predicción de test\n## Si no decimos nada en type (type = prob), nos devolvera la probabilidad de \n## pertenecer a cada clase. \nrtest_matrix <- rtest %>% select(-RENTA) %>% as.matrix()\nprediccion <- predict(model$finalModel, rtest_matrix, type = \"class\")\n\n## Para ver la performance, realizaremos la matriz de confusión \ncaret::confusionMatrix(prediccion, as.factor(rtest$RENTA))\n```\n\n```{r}\n#| label: plot-confusionMatrix-Xgboost-r\n#| echo: false\n#| eval: false\n#| warning: false\n#| message: false\n#| error: false\n\nCM <- caret::confusionMatrix(prediccion, as.factor(rtest$RENTA)); CM <- data.frame(CM$table)\n\ngrafico <- ggplot(CM, aes(Prediction,Reference, fill= Freq)) +\n        geom_tile() + geom_text(aes(label=Freq)) +\n        scale_fill_gradient(low=\"white\", high=\"#009194\") +\n        labs(x = \"Reference\",y = \"Prediction\")\n\nplot(grafico)\n```\n\n## Python\n\n```{python}\n#| label: predicciones-Xgboost-python\n#| echo: false\n#| eval: false\n#| warning: false\n#| message: false\n#| error: false \n\n# Realizar predicciones\ny_pred = model.predict(dtest)\n\n# Convertir las predicciones de flotante a enteros (si se usa 'multi:softmax')\ny_pred = np.round(y_pred).astype(int)\n\n```\n\n```{python}\n#| label: plot-confusionMatrix-Xgboost-python\n#| echo: false\n#| eval: false\n#| warning: false\n#| message: false\n#| error: false\n\n# Confusion matrix\ncf_matrix = confusion_matrix(pyy_test, y_pred)\nsns.heatmap(cf_matrix, annot=True, fmt='d', cmap='Blues', cbar=False)\n```\n:::\n","srcMarkdownNoYaml":"\n\n# Descripción del problema\n\nEn este ejemplo se entrena un árbol de regresión para predecir el precio unitario de la vivienda en Madrid. Para ello se utilizan los datos de viviendas a la venta en Madrid publicados en Idealista durante el año 2018. Estos datos están incluidos en el paquete `idealista18`. Las variables que contienen nuestra base de datos son las siguientes:\n\n-   ***\"ASSETID\" :*** Identificador único del activo\n\n-   ***\"PERIOD\" :*** Fecha AAAAMM, indica el trimestre en el que se extrajo el anuncio, utilizamos AAAA03 para el 1.er trimestre, AAAA06 para el 2.º, AAAA09 para el 3.er y AAAA12 para el 4.º\n\n-   ***\"PRICE\" :*** Precio de venta del anuncio en idealista expresado en euros\n\n-   ***\"UNITPRICE\" :*** Precio en euros por metro cuadrado\n\n-   ***\"CONSTRUCTEDAREA\" :*** Superficie construida de la casa en metros cuadrados\n\n-   ***\"ROOMNUMBER\" :*** Número de habitaciones\n\n-   ***\"BATHNUMBER\" :*** Número de baños\n\n-   ***\"HASTERRACE\" :*** Variable ficticia para terraza (toma 1 si hay una terraza, 0 en caso contrario)\n\n-   ***\"HASLIFT\" :*** Variable ficticia para ascensor (toma 1 si hay ascensor en el edificio, 0 en caso contrario)\n\n-   ***\"HASAIRCONDITIONING\" :*** Variable ficticia para Aire Acondicionado (toma 1 si hay una Aire Acondicionado, 0 en caso contrario)\n\n-   ***\"AMENITYID\" :*** Indica las comodidades incluidas (1 - sin muebles, sin comodidades de cocina, 2 - comodidades de cocina, sin muebles, 3 - comodidades de cocina, muebles)\n\n-   ***\"HASPARKINGSPACE\" :*** Variable ficticia para estacionamiento (toma 1 si el estacionamiento está incluido en el anuncio, 0 en caso contrario)\n\n-   ***\"ISPARKINGSPACEINCLUDEDINPRICE\" :*** Variable ficticia para estacionamiento (toma 1 si el estacionamiento está incluido en el anuncio, 0 en caso contrario)\n\n-   ***\"PARKINGSPACEPRICE\" :*** Precio de plaza de parking en euros\n\n-   ***\"HASNORTHORIENTATION\" :*** Variable ficticia para orientación (toma 1 si la orientación es Norte en el anuncio, 0 en caso contrario) - *Nota importante: las características de orientación no son características ortogonales, una casa orientada al norte también puede estar orientada al este*\n\n-   ***\"HASSOUTHORIENTATION\" :*** Variable ficticia para orientación (toma 1 si la orientación es Sur en el anuncio, 0 en caso contrario) - *Nota importante: las características de orientación no son características ortogonales, una casa orientada al norte también puede estar orientada al este*\n\n-   ***\"HASEASTORIENTATION\" :*** Variable ficticia para orientación (toma 1 si la orientación es Este en el anuncio, 0 en caso contrario) - *Nota importante: las características de orientación no son características ortogonales, una casa orientada al norte también puede estar orientada al este*\n\n-   ***\"HASWESTORIENTATION\" :*** Variable ficticia para orientación (toma 1 si la orientación es Oeste en el anuncio, 0 en caso contrario) - *Nota importante: las características de orientación no son características ortogonales, una casa orientada al norte también puede estar orientada al este*\n\n-   ***\"HASBOXROOM\" :*** Variable ficticia para boxroom (toma 1 si boxroom está incluido en el anuncio, 0 en caso contrario)\n\n-   ***\"HASWARDROBE\" :*** Variable ficticia para vestuario (toma 1 si el vestuario está incluido en el anuncio, 0 en caso contrario)\n\n-   ***\"HASSWIMMINGPOOL\" :*** Variable ficticia para piscina (toma 1 si la piscina está incluida en el anuncio, 0 en caso contrario)\n\n-   ***\"HASDOORMAN\" :*** Variable ficticia para portero (toma 1 si hay un portero en el edificio, 0 en caso contrario)\n\n-   ***\"HASGARDEN\" :*** Variable ficticia para jardín (toma 1 si hay un jardín en el edificio, 0 en caso contrario)\n\n-   ***\"ISDUPLEX\" :*** Variable ficticia para dúplex (toma 1 si es un dúplex, 0 en caso contrario)\n\n-   ***\"ISSTUDIO\" :*** Variable ficticia para piso de soltero (estudio en español) (toma 1 si es un piso para una sola persona, 0 en caso contrario)\n\n-   ***\"ISINTOPFLOOR\" :*** Variable ficticia que indica si el apartamento está ubicado en el piso superior (toma 1 en el piso superior, 0 en caso contrario)\n\n-   ***\"CONSTRUCTIONYEAR\" :*** Año de construcción (fuente: anunciante)\n\n-   ***\"FLOORCLEAN\" :*** Indica el número de piso del apartamento comenzando desde el valor 0 para la planta baja (fuente: anunciante)\n\n-   ***\"FLATLOCATIONID\" :*** Indica el tipo de vistas que tiene el piso (1 - exterior, 2 - interior)\n\n-   ***\"CADCONSTRUCTIONYEAR\" :*** Año de construcción según fuente catastral (fuente: catastro), tenga en cuenta que esta cifra puede diferir de la proporcionada por el anunciante\n\n-   ***\"CADMAXBUILDINGFLOOR\" :*** Superficie máxima del edificio (fuente: catastro)\n\n-   ***\"CADDWELLINGCOUNT\" :*** Recuento de viviendas en el edificio (fuente: catastro)\n\n-   ***\"CADASTRALQUALITYID\" :*** Calidad catastral (fuente: catastro)\n\n-   ***\"BUILTTYPEID_1\" :*** Valor ficticio para estado del piso: 1 obra nueva 0 en caso contrario (fuente: anunciante)\n\n-   ***\"BUILTTYPEID_2\" :*** Valor ficticio para condición plana: 1 segundero a restaurar 0 en caso contrario (fuente: anunciante)\n\n-   ***\"BUILTTYPEID_3\" :*** Valor ficticio para estado plano: 1 de segunda mano en buen estado 0 en caso contrario (fuente: anunciante)\n\n-   ***\"DISTANCE_TO_CITY_CENTER\" :*** Distancia al centro de la ciudad en km\n\n-   ***\"DISTANCE_TO_METRO\" :*** Distancia istancia a una parada de metro en km.\n\n-   ***\"DISTANCE_TO_DIAGONAL\" :*** Distancia a la Avenida Diagonal en km; Diagonal es una calle principal que corta la ciudad en diagonal a la cuadrícula de calles.\n\n-   ***\"LONGITUDE\" :*** Longitud del activo\n\n-   ***\"LATITUDE\" :*** Latitud del activo\n\n-   ***\"geometry\" :*** Geometría de características simples en latitud y longitud.\n\n**Fuente**: [Idealista](https://www.idealista.com/)\n\n```{r}\n#| label: cargar-datos\n#| echo: true\n#| eval: false\n#| warning: false\n#| message: false\n#| error: false\n\nlibrary(\"idealista18\")\nBCN <- get(data(\"Barcelona_Sale\"))\n```\n\n```{r}\n#| label: geo-BCN\n#| echo: true\n#| eval: false\n#| warning: false\n#| message: false\n#| error: false\n\n# Filtramos la epoca a Navidad\nBCN <- BCN[which(BCN$PERIOD == \"201812\"), ]\n\npisos_sf_BCN <- st_as_sf(BCN, coords = c(\"LONGITUDE\", \"LATITUDE\"), crs = 4326)\n\n# Leer shapefile de secciones censales\nsecciones <- st_read(\"C:/Users/sergi/Downloads/Shapefile/seccionado_2024/SECC_CE_20240101.shp\")\n\n# Transformar pisos al sistema de referencia de las secciones censales\npisos_sf_BCN <- st_transform(pisos_sf_BCN, crs = st_crs(secciones))\n\n# Hacer el match entre pisos y secciones censales\npisos_con_seccion <- st_join(pisos_sf_BCN, secciones, join = st_within)\n\n# Convertir a dataframe para exportar\nBCN <- as.data.frame(pisos_con_seccion)\n\nrm(Barcelona_Sale, Barcelona_Polygons, Barcelona_POIS, pisos_con_seccion, pisos_sf_BCN, secciones); gc()\n```\n\n```{r}\n#| label: cargar-renta-media\n#| echo: true\n#| eval: false\n#| warning: false\n#| message: false\n#| error: false\n\nrentaMedia <- read.csv(\"https://raw.githubusercontent.com/miguel-angel-monjas/spain-datasets/refs/heads/master/data/Renta%20media%20en%20Espa%C3%B1a.csv\")\n# NOs quedamos con los datos que nos interesa de Barcelona\nrentaMedia <- rentaMedia[which(rentaMedia$Provincia == \"Barcelona\" & rentaMedia$Tipo.de.elemento == \"sección\"), ]\nrentaMedia$Código.de.territorio <- paste0(\"0\", rentaMedia$Código.de.territorio)\n```\n\n```{r}\n#| label: unir-informacion\n#| echo: true\n#| eval: false\n#| warning: false\n#| message: false\n#| error: false\n\ncols <- c(\"Renta.media.por.persona\", \"Renta.media.por.hogar\")\n\nm <- match(BCN$CUSEC, rentaMedia$Código.de.territorio)\nBCN[, cols] <- rentaMedia[m, cols]\n```\n\n```{r}\n#| label: cargar-datos-final\n#| echo: false\n#| eval: true\n#| warning: false\n#| message: false\n#| error: false\n\npath <- 'https://raw.githubusercontent.com/ramIA-lab/MLforEducation/refs/heads/main/material/trees_ensambleMethods/idealista18_BCN_conRenta.csv'\nBCN <- read.csv2(path)\n```\n\n```{r}\n#| label: cargar-paquetes-r\n#| echo: false\n#| warning: false\n#| message: false\n#| error: false\n\nlibrary(dplyr)\nlibrary(tidyr)\n```\n\n# Preprocessing de los datos\n\n```{r}\n#| label: preprocessing-R\n#| echo: true\n#| warning: false\n#| message: false\n#| error: false\n\nBCN <- BCN %>%\n  select(-X, -PRICE, -LONGITUDE, -LATITUDE, -geometry, -CONSTRUCTIONYEAR, \n         -ASSETID, -PERIOD, -CUSEC, -CSEC, -CMUN, -CPRO, -CCA, -CUDIS, -CLAU2, \n         -NPRO, -NCA, -CNUT0, -CNUT1, -CNUT2, -CNUT3, -NMUN, -Shape_Leng,\n         -Shape_Area, -geometry, -CUMUN, -CADASTRALQUALITYID) %>%\n  mutate(\n    across(\n      .cols = starts_with(c(\"HAS\", \"IS\")),\n      .fns = ~ case_when(. == 0 ~ \"No\", . == 1 ~ \"Si\"),\n      .names = \"{.col}\"), \n    AMENITYID = case_when(\n      AMENITYID == 1 ~ \"SinMuebleSinCocina\", AMENITYID == 2 ~ \"CocinaSinMuebles\", \n      AMENITYID == 3 ~ \"CocinaMuebles\"), \n    FLATLOCATIONID = case_when(\n      FLATLOCATIONID == 1 ~ \"exterior\", FLATLOCATIONID == 2 ~ \"interior\", \n      .default = \"noInfo\"),\n    BUILTTYPEID_1 = case_when(\n      BUILTTYPEID_1 == 0 ~ \"noObraNueva\", BUILTTYPEID_1 == 1 ~ \"obraNueva\"),\n    BUILTTYPEID_2 = case_when(\n      BUILTTYPEID_2 == 0 ~ \"noRestaurar\", BUILTTYPEID_2 == 1 ~ \"Restaurar\"),\n    BUILTTYPEID_3 = case_when(\n      BUILTTYPEID_3 == 0 ~ \"noSegundaMano\", BUILTTYPEID_3 == 1 ~ \"SegundaMano\"),\n    FLOORCLEAN = replace_na(FLOORCLEAN, 0),\n    CDIS = case_when(\n      CDIS == 1 ~ \"Ciutat-Vella\", CDIS == 2 ~ \"Eixample\", CDIS == 3 ~ \"Sants-Montjuic\", \n      CDIS == 4 ~ \"Les Corts\", CDIS == 5 ~ \"Sarrià-Sant Gervasi\", \n      CDIS == 6 ~ \"Gràcia\", CDIS == 7 ~ \"Horta-Guinardó\", CDIS == 8 ~ \"Nou Barris\",\n      CDIS == 9 ~ \"Sant Andreu\", CDIS == 10 ~ \"Sant Martí\"),\n    RENTA = case_when(\n     Renta.media.por.hogar < 30000 ~ \"Baja\",\n     Renta.media.por.hogar >= 30000 & Renta.media.por.hogar <= 50000 ~ \"Media\",\n     Renta.media.por.hogar > 50000 ~ \"Alta\"\n    )\n  ) %>%\n  select(-Renta.media.por.hogar, -Renta.media.por.persona)\n```\n\n## Análisi descriptivo de los datos\n\n```{r}\n#| label: descriptiva-R\n#| echo: true\n#| warning: false\n#| message: false\n#| error: false\n\n## Descriptiva de los datos\nlibrary(DataExplorer)\nlibrary(lubridate)\nlibrary(dplyr)\n\n## Data Manipulation\nlibrary(reshape2)\n\n## Plotting\nlibrary(ggplot2)\n\n## Descripción completa\nDataExplorer::introduce(BCN)\n\n## Descripción de la bbdd\nplot_intro(BCN)\n\n## Descripción de los missings\nplot_missing(BCN)\n\n## Descripción de las varaibles categoricas\nplot_bar(BCN)\n\n## Descripción variables numéricas\nplot_histogram(BCN)\nplot_density(BCN)\nplot_qq(BCN)\nplot_correlation(BCN)\n```\n\n# Data Manipulation\n\n::: panel-tabset\n## R\n\n```{r}\n#| label: gestion-datos-R\n#| echo: true\n#| warning: false\n#| message: false\n#| error: false\n\nlibrary(caret)\n\nset.seed(1994)\n\nindex <- caret::createDataPartition(BCN$UNITPRICE, p = 0.8, list = FALSE)\nrtrain <- BCN %>% slice(index) %>% na.omit()\nrtest <- BCN %>% slice(-index) %>% na.omit()\n```\n\n## Python\n\n```{python}\n#| label: conversor-r_py\n#| echo: false\n#| warning: false\n#| message: false\n#| error: false\n\npyBCN = r.BCN\n```\n\n```{python}\n#| label: preprocessing-py\n#| echo: true\n#| warning: false\n#| message: false\n#| error: false\n\nfrom sklearn.preprocessing import LabelEncoder\n\n# Crear un LabelEncoder\nle = LabelEncoder()\n\n# Lista de variables a transformar\ncolumns_to_encode = ['HASTERRACE', 'HASLIFT', 'HASAIRCONDITIONING', 'AMENITYID',\n    'HASPARKINGSPACE', 'ISPARKINGSPACEINCLUDEDINPRICE', 'HASNORTHORIENTATION',\n    'HASSOUTHORIENTATION', 'HASEASTORIENTATION', 'HASWESTORIENTATION',\n    'HASBOXROOM', 'HASWARDROBE', 'HASSWIMMINGPOOL', 'HASDOORMAN', 'HASGARDEN',\n    'ISDUPLEX', 'ISSTUDIO', 'ISINTOPFLOOR', 'FLOORCLEAN', 'FLATLOCATIONID',\n    'CADMAXBUILDINGFLOOR', 'CADDWELLINGCOUNT','BUILTTYPEID_1', 'BUILTTYPEID_2', \n    'BUILTTYPEID_3', 'CDIS', 'RENTA']\n\n# Aplicar LabelEncoder a cada columna de la lista\nfor col in columns_to_encode:\n    if col in pyBCN.columns:  # Verificar que la columna existe en el DataFrame\n        pyBCN[col] = le.fit_transform(pyBCN[col].astype(str))  # Convertir a string si no es categórica\n\n```\n\n```{python}\n#| label: gestion-datos-Python\n#| echo: true\n#| warning: false\n#| message: false\n#| error: false\n\nfrom sklearn.model_selection import train_test_split\n\n# dividimos la base de datos \nX = pyBCN.drop(columns=[\"RENTA\"])\n\npyX_train, pyX_test, pyy_train, pyy_test = train_test_split(\n    X, pyBCN['RENTA'], test_size = 0.2, random_state = 1994)\n```\n\n```{python}\n#| label: normalizacion-python\n#| echo: true\n#| warning: false\n#| message: false\n#| error: false\n\nfrom sklearn.preprocessing import StandardScaler\n\n# Scale dataset\nsc = StandardScaler()\npyX_train = sc.fit_transform(pyX_train)\npyX_test = sc.fit_transform(pyX_test)\n```\n:::\n\n# Árboles de decisión\n\n## Creación del árbol\n\n::: panel-tabset\n## R\n\n```{r}\n#| label: arbol-training-r\n#| echo: true\n#| warning: false\n#| message: false\n#| error: false\n\nlibrary(rpart)\nlibrary(rpart.plot)\n\nset.seed(1994)\n\narbol <- rpart(RENTA ~ ., data = rtrain)\nsummary(arbol)\n```\n\n```{r}\n#| label: plot-arbol-r\n#| echo: true\n#| warning: false\n#| message: false\n#| error: false\n\nrpart.plot(arbol)\n```\n\n## Python\n\n```{python}\n#| label: arbol-training-python\n#| echo: true\n#| warning: false\n#| message: false\n#| error: false\n\n# Decision Tree Classification\nfrom sklearn.tree import DecisionTreeClassifier\n\nclassifier = DecisionTreeClassifier(criterion = 'entropy', random_state = 1994)\nclf = classifier.fit(pyX_train, pyy_train)\n```\n\n```{python}\n#| label: plot-arbol-python\n#| echo: true\n#| warning: false\n#| message: false\n#| error: false\n\nfrom sklearn import tree\n\ntree.plot_tree(clf)\n```\n:::\n\n## Creamos las predicciones\n\n::: panel-tabset\n## R\n\nAplicamos el modelo a nuestros valores de test.\n\n```{r}\n#| label: decisionsTree-predict-r\n#| echo: true\n#| warning: false\n#| message: false\n#| error: false\n\npredict(arbol, rtest[1:10, ])\n```\n\n```{r}\n#| label: decisionsTree-confMatrix-r\n#| echo: true\n#| warning: false\n#| message: false\n#| error: false\n\npredicciones <- predict(arbol, rtrain, type = \"class\")\ncaret::confusionMatrix(predicciones, as.factor(rtrain$RENTA))\n\npredicciones <- predict(arbol, rtest, type = \"class\")\ncaret::confusionMatrix(predicciones, as.factor(rtest$RENTA))\n```\n\n```{r}\n#| label: plot-confusionMatrix-decisionTree-r\n#| echo: true\n#| warning: false\n#| message: false\n#| error: false\n\nCM <- caret::confusionMatrix(predicciones, as.factor(rtest$RENTA)); CM <- data.frame(CM$table)\n\ngrafico <- ggplot(CM, aes(Prediction,Reference, fill= Freq)) +\n        geom_tile() + geom_text(aes(label=Freq)) +\n        scale_fill_gradient(low=\"white\", high=\"#009194\") +\n        labs(x = \"Reference\",y = \"Prediction\")\n\nplot(grafico)\n```\n\n## Python\n\n```{python}\n#| label: decisionsTree-predict-python\n#| echo: true\n#| warning: false\n#| message: false\n#| error: false\n\nimport pandas as pd\n\n# Prediction\ny_pred = classifier.predict(pyX_test)\n\nresults = pd.DataFrame({\n    'Real': pyy_test,  # Valores reales\n    'Predicho': y_pred  # Valores predichos\n})\n\n# Muestra los primeros 5 registros\nprint(results.head())  \n```\n\n```{python}\n#| label: decisionsTree-confMatrix-python\n#| echo: true\n#| warning: false\n#| message: false\n#| error: false\n#| \nfrom sklearn.metrics import classification_report\n\nprint(f'Classification Report: \\n{classification_report(pyy_test, y_pred)}')\n```\n\n```{python}\n#| label: plot-confMatrix-python\n#| echo: true\n#| warning: false\n#| message: false\n#| error: false\n\nfrom sklearn.metrics import confusion_matrix\nimport seaborn as sns\n\n# Confusion matrix\ncf_matrix = confusion_matrix(pyy_test, y_pred)\nsns.heatmap(cf_matrix, annot=True, fmt='d', cmap='Blues', cbar=False)\n```\n:::\n\n## Modelo de classificación con cross-evaluación\n\n::: panel-tabset\n## R\n\n```{r}\n#| label: decisionsTree-crossvalidation-r\n#| echo: true\n#| warning: false\n#| message: false\n#| error: false\n#| \n## Generamos los parámetros de control\ntrControl <- trainControl(method = \"cv\", number = 10, classProbs = TRUE,\n  summaryFunction = multiClassSummary)\n## En este caso, se realiza una cros-validación de 10 etapas\n\n# se fija una semilla aleatoria\nset.seed(1994)\n\n# se entrena el modelo\nmodel <- train(RENTA ~ .,  # . equivale a incluir todas las variables\n               data = rtrain,\n               method = \"rpart\",\n               metric = \"Accuracy\",\n               trControl = trControl)\n\n# Obtenemos los valores del árbol óptimo\nmodel$finalModel\n\n# Generamos el gráfico del árbol\nrpart.plot(model$finalModel)\n```\n\n```{r}\n#| label: plot-resultados-crossvalidation-r\n#| echo: true\n#| warning: false\n#| message: false\n#| error: false\n\nlibrary(reshape2)\n\n# A continuación generamos un gráfico que nos permite ver la variabilidad de los estadísticos\n# calculados\nggplot(melt(model$resample[,c(2:5, 7:9, 12:13)]), aes(x = variable, y = value, fill=variable)) +\n  geom_boxplot(show.legend=FALSE) +\n  xlab(NULL) + ylab(NULL)\n```\n\n## Python\n\n```{python}\n#| label: decisionsTree-crossvalidation-python\n#| echo: true\n#| warning: false\n#| message: false\n#| error: false\n\nfrom sklearn.tree import DecisionTreeClassifier\n\n# Modelo de árbol de decisión\nmodel = DecisionTreeClassifier(random_state=1994)\n\nfrom sklearn.model_selection import cross_val_score\n\n# Realizar validación cruzada con 5 folds\nscores = cross_val_score(model, pyX_train, pyy_train, cv=10, scoring = 'accuracy')  # Métrica: accuracy\n\n# Mostrar resultados\nprint(f\"Accuracy por fold: {scores}\")\nprint(f\"Accuracy promedio: {scores.mean():.4f}\")\n```\n:::\n\n## Realizando hiperparámetro tunning\n\n::: panel-tabset\n## R\n\n```{r}\n#| label: parametros-decisionTree-r\n#| echo: true\n#| warning: false\n#| message: false\n#| error: false\n\n# Detectamos cuales son los parámetros del modelo que podemos realizar hiperparámeter tunning\nmodelLookup(\"rpart\")\n```\n\n```{r}\n#| label: parametros-decisionTree-grid-r\n#| echo: true\n#| warning: false\n#| message: false\n#| error: false\n\n# Se especifica un rango de valores típicos para el hiperparámetro\ntuneGrid <- expand.grid(cp = seq(0.01,0.05,0.01))\n```\n\n```{r}\n#| label: hyperparam-decisionTree-r\n#| echo: true\n#| warning: false\n#| message: false\n#| error: false\n\n# se entrena el modelo\nset.seed(1994)\n\nmodel <- train(RENTA ~ .,\n               data = rtrain,\n               method = \"rpart\",\n               metric = \"Accuracy\",\n               trControl = trControl,\n               tuneGrid = tuneGrid)\n\n# Obtenemos la información del mejor modelo\nmodel$bestTune\n\n# Gráfico del árbol obtenido\nrpart.plot(model$finalModel)\n```\n\n## Python\n\n```{python}\n#| label: hyperparam-decisionTree-python\n#| echo: true\n#| warning: false\n#| message: false\n#| error: false\n\nfrom sklearn.model_selection import GridSearchCV\n\n# Definir rejilla de hiperparámetros\nparam_grid = {\n    'max_depth': [None, 5, 10],\n    'min_samples_split': [2, 5, 10],\n    'min_samples_leaf': [1, 2, 4]\n}\n\n# Declaramos el modelo\nmodel = DecisionTreeClassifier(random_state=1994)\n\n# Configurar GridSearch con validación cruzada\ngrid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=10, scoring='accuracy', n_jobs=-1)\n\n# Ajustar modelo\ngrid_search.fit(pyX_train, pyy_train)\n\n# Mostrar mejores parámetros\nprint(f\"Mejores parámetros: {grid_search.best_params_}\")\nprint(f\"Mejor accuracy: {grid_search.best_score_:.4f}\")\n\n\nfrom sklearn import tree\ntree.plot_tree(grid_search.best_estimator_)\n```\n:::\n\n## Como realizar poda de nuestro árbol\n\n::: panel-tabset\n## R\n\n```{r}\n#| label: poda-decisionTree-r\n#| echo: true\n#| warning: false\n#| message: false\n#| error: false\n\n# Con el objetivo de aumentar la generalidad del árbol y facilitar su interpretación, \n# se procede a reducir su tamaño podándolo. Para ello se establece el criterio de \n# que un nodo terminal tiene que tener, como mínimo, 50 observaciones.\nset.seed(1994)\nprunedtree <- rpart(RENTA ~ ., data = rtrain,\n                    cp= 0.01, control = rpart.control(minbucket = 50))\n\nrpart.plot(prunedtree)\n```\n\n## Python\n\nEn Python, la poda de un árbol de decisión se puede realizar ajustando los hiperparámetros del árbol durante su creación. Estos hiperparámetros controlan el crecimiento del árbol y, por lo tanto, actúan como técnicas de poda preventiva o postpoda.\n\n`scikit-learn` no implementa poda dinámica directa (como ocurre en algunos otros frameworks), pero puedes limitar el tamaño del árbol y evitar sobreajuste mediante los siguientes métodos.\n\n### Poda Preventiva (Pre-pruning)\n\n**Poda preventiva** consiste en detener el crecimiento del árbol antes de que se haga demasiado grande. Esto se logra ajustando hiperparámetros como:\n\n-   `max_depth`: Profundidad máxima del árbol\n-   `min_samples_split`: Número mínimo de muestras necesarias para dividir un nodo.\n-   `min_samples_leaf`: Número mínimo de muestras necesarias en una hoja.\n-   `max_leaf_nodes`: Número máximo de nodos hoja en el árbol.\n\n```{python}\n#| label: podaPreventiva-decisionTree-python\n#| echo: true\n#| warning: false\n#| message: false\n#| error: false\n\n# Crear un árbol con poda preventiva\nmodel = DecisionTreeClassifier(\n    max_depth=3,              # Limitar la profundidad\n    min_samples_split=10,     # Mínimo 10 muestras para dividir un nodo\n    min_samples_leaf=5,       # Mínimo 5 muestras por hoja\n    random_state=42\n)\n\n# Entrenar el modelo\nmodel.fit(pyX_train, pyy_train)\n\n# Evaluar\nprint(f\"Accuracy en entrenamiento: {model.score(pyX_train, pyy_train):.4f}\")\nprint(f\"Accuracy en prueba: {model.score(pyX_test, pyy_test):.4f}\")\n\n# Graficamos el árbol podado\ntree.plot_tree(model)\n```\n\n### Poda Posterior (Post-Pruning) con `ccp_alpha`\n\nSe puedes realizar poda posterior usando cost **complexity pruning**. Esto implica ajustar el parámetro `ccp_alpha` (el parámetro de complejidad de coste).\n\nEl árbol generará múltiples subárboles podados para diferentes valores de `ccp_alpha`, y tú puedes elegir el más adecuado evaluando su desempeño.\n\n```{python}\n#| label: PostPruning-decisionTree-python\n#| echo: true\n#| warning: false\n#| message: false\n#| error: false\n\nimport matplotlib.pyplot as plt\n\n# Crear un árbol sin poda\nmodel = DecisionTreeClassifier(random_state=1994)\nmodel.fit(pyX_train, pyy_train)\n\n# Obtener valores de ccp_alpha\npath = model.cost_complexity_pruning_path(pyX_train, pyy_train)\nccp_alphas = path.ccp_alphas\nimpurities = path.impurities\n\n# Entrenar árboles para cada valor de ccp_alpha\nmodels = []\nfor ccp_alpha in ccp_alphas:\n    clf = DecisionTreeClassifier(random_state=42, ccp_alpha=ccp_alpha)\n    clf.fit(pyX_train, pyy_train)\n    models.append(clf)\n\n# Evaluar desempeño\ntrain_scores = [clf.score(pyX_train, pyy_train) for clf in models]\ntest_scores = [clf.score(pyX_test, pyy_test) for clf in models]\n\n# Graficar resultados\nplt.figure(figsize=(8, 6))\nplt.plot(ccp_alphas, train_scores, marker='o', label=\"Train Accuracy\", drawstyle=\"steps-post\")\nplt.plot(ccp_alphas, test_scores, marker='o', label=\"Test Accuracy\", drawstyle=\"steps-post\")\nplt.xlabel(\"ccp_alpha\")\nplt.ylabel(\"Accuracy\")\nplt.title(\"Accuracy vs ccp_alpha\")\nplt.legend()\nplt.grid()\nplt.show()\n```\n:::\n\n# Random Forest\n\n## Aplicación del modelo\n\n::: panel-tabset\n## R\n\n```{r}\n#| label: randomForest-r\n#| echo: true\n#| warning: false\n#| message: false\n#| error: false\n#| \n# Random Forest \nlibrary(randomForest)\n## devtools::install_github('araastat/reprtree') # Se instala 1 vez para poder printar graficos\nlibrary(reprtree)\n\nset.seed(1994)\narbol_rf <- randomForest(as.factor(RENTA) ~ .,  data = rtrain, ntree = 25)\n```\n\n```{r}\n#| label: observar-arbol-r\n#| echo: true\n#| warning: false\n#| message: false\n#| error: false\n\n# se observa el árbol número 20\ntree20 <- getTree(arbol_rf, 20, labelVar = TRUE)\nhead(tree20)\n\n## Sin embargo, el método por el que se representa gráficamente no es muy claro y\n## puede llevar a confusión o dificultar la interpretación del árbol. \n## Si se desea estudiar el árbol, hasta un cierto nivel, se puede incluir el argumento depth.\n## El árbol, ahora con una profundidad de 5 ramas.\nplot.getTree(arbol_rf, k = 20, depth = 5)\n```\n\n```{r}\n#| label: importance-matrix-r\n#| echo: true\n#| warning: false\n#| message: false\n#| error: false\n\nlibrary(vip)\nvip(arbol_rf)\n```\n\n## Python\n\n```{python}\n#| label: randomForest-python\n#| echo: true\n#| warning: false\n#| message: false\n#| error: false\n\nfrom sklearn.ensemble import RandomForestClassifier\n\nclf = RandomForestClassifier()\nclf.fit(pyX_train, pyy_train)\n```\n\n```{python}\n#| label: importance-matrix-python\n#| echo: true\n#| warning: false\n#| message: false\n#| error: false\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nresults = pd.DataFrame(clf.feature_importances_, index=pyBCN.columns[:-1]).sort_values(by=0, ascending=False)\n\n# Crear gráfico de barras horizontales\nplt.figure(figsize=(10, 8))\nplt.barh(results.index, results[0], color='skyblue')\n\n# Añadir etiquetas y título\nplt.xlabel('Importancia')\nplt.ylabel('Características')\nplt.title('Importancia de las Características')\nplt.grid(axis='x', linestyle='--', alpha=0.7)\nplt.show()\n```\n:::\n\n## Hiperparameter tunning de Random Forest\n\n::: panel-tabset\n## R\n\n```{r}\n#| label: parametros-RF-r\n#| echo: true\n#| warning: false\n#| message: false\n#| error: false\n\n# Identificamos los parámetros que podemos tunnerar\nmodelLookup(\"rf\")\n```\n\n```{r}\n#| label: gridtunning-RF-r\n#| echo: true\n#| warning: false\n#| message: false\n#| error: false\n\n# Se especifica un rango de valores posibles de mtry\ntuneGrid <- expand.grid(mtry = c(1, 2, 5, 10))\ntuneGrid\n```\n\n```{r}\n#| label: tunning-RF-r\n#| echo: true\n#| warning: false\n#| message: false\n#| error: false\n\n# se fija la semilla aleatoria\nset.seed(1994)\n\n# se entrena el modelo\nmodel <- train(RENTA ~ ., data = rtrain, \n               ntree = 20,\n               method = \"rf\", metric = \"Accuracy\",\n               tuneGrid = tuneGrid,\n               trControl = trainControl(classProbs = TRUE))\n\n# Visualizamos los hiperparámetros obtenidos \nmodel$results\n```\n\n## Python\n\n### Ajuste de hiperparámetros con `GridSearchCV`\n\nEl `GridSearchCV` realiza una búsqueda exhaustiva sobre un conjunto de parámetros especificados. Probará todas las combinaciones posibles de hiperparámetros.\n\n```{python}\n#| label: GridSearchCV-RF-python\n#| echo: true\n#| eval: false\n#| warning: false\n#| message: false\n#| error: false\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n\n# Definir los parámetros para la búsqueda\nparam_dist = {\n    'n_estimators': [150, 200],        # Número de árboles en el bosque\n    'max_depth': [None, 10, 20],            # Profundidad máxima del árbol\n    'min_samples_split': [2, 5, 10],            # Número mínimo de muestras para dividir un nodo\n    'min_samples_leaf': [1, 2, 4],               # Número mínimo de muestras en una hoja\n    'max_features': ['auto'],      # Número de características a considerar para dividir un nodo\n    'bootstrap': [True]                      # Si usar bootstrap para los árboles\n}\n\n# Crear el modelo RandomForest\nrf = RandomForestClassifier(random_state = 1994)\n\n# Usar GridSearchCV para encontrar el mejor conjunto de parámetros\ngrid_search = GridSearchCV(estimator = rf, param_grid = param_grid, cv = 10, n_jobs = -1, verbose = 0)\n\n# Ajustar el modelo con los datos de entrenamiento\ngrid_search.fit(pyX_train, pyy_train)\n\n# Mostrar los mejores parámetros encontrados\nprint(\"Mejores parámetros encontrados:\", grid_search.best_params_)\n\ntree.plot_tree(grid_search.best_estimator_)\n```\n\n### Ajuste de Hiperparámetros con `RandomizedSearchCV`\n\n`RandomizedSearchCV` es una técnica más eficiente que `GridSearchCV`, ya que no prueba todas las combinaciones posibles, sino un número limitado de combinaciones aleatorias dentro de un rango definido. Esto es útil si el espacio de búsqueda es grande y quieres evitar un tiempo de cómputo muy largo.\n\n```{python}\n#| label: RandomizedSearchCV-RF-python\n#| echo: true\n#| eval: false\n#| warning: false\n#| message: false\n#| error: false\n\n# Definir los parámetros para la búsqueda aleatoria\nparam_dist = {\n    'n_estimators': [150, 200],        # Número de árboles en el bosque\n    'max_depth': [None, 10, 20],            # Profundidad máxima del árbol\n    'min_samples_split': [2, 5, 10],            # Número mínimo de muestras para dividir un nodo\n    'min_samples_leaf': [1, 2, 4],               # Número mínimo de muestras en una hoja\n    'max_features': ['auto'],      # Número de características a considerar para dividir un nodo\n    'bootstrap': [True]                      # Si usar bootstrap para los árboles\n}\n\n# Usar RandomizedSearchCV para búsqueda aleatoria\nrandom_search = RandomizedSearchCV(estimator=rf, param_distributions=param_dist, \n                                   n_iter=50, cv=10, n_jobs=-1, random_state=1994)\n\n# Ajustar el modelo con los datos de entrenamiento\nrandom_search.fit(X_train, y_train)\n\n# Mostrar los mejores parámetros encontrados\nprint(\"Mejores parámetros encontrados:\", random_search.best_params_)\n\ntree.plot_tree(random_search.best_estimator_)\n```\n:::\n\n## Predicciones del algoritmo\n\n::: panel-tabset\n## R\n\n```{r}\n#| label: predict-RF-r\n#| echo: true\n#| warning: false\n#| message: false\n#| error: false\n\nprediccion <- predict(arbol_rf, rtrain, type = \"class\")\ncaret::confusionMatrix(prediccion, as.factor(rtrain$RENTA))\n\n# Realizamos las predicciones de este ultimo arbol para la predicción de test\n## Si no decimos nada en type (type = prob), nos devolvera la probabilidad de \n## pertenecer a cada clase. \nprediccion <- predict(arbol_rf, rtest, type = \"class\")\n## Para ver la performance, realizaremos la matriz de confusión \ncaret::confusionMatrix(prediccion, as.factor(rtest$RENTA))\n```\n\n```{r}\n#| label: plot-confusionMatrix-RF-r\n#| echo: true\n#| warning: false\n#| message: false\n#| error: false\n\nCM <- caret::confusionMatrix(prediccion, as.factor(rtest$RENTA)); CM <- data.frame(CM$table)\n\ngrafico <- ggplot(CM, aes(Prediction,Reference, fill= Freq)) +\n        geom_tile() + geom_text(aes(label=Freq)) +\n        scale_fill_gradient(low=\"white\", high=\"#009194\") +\n        labs(x = \"Reference\",y = \"Prediction\")\n\nplot(grafico)\n```\n\n## Python\n\n```{python}\n#| label: predict-RF-python\n#| echo: true\n#| warning: false\n#| message: false\n#| error: false\n\npreds = clf.predict(pyX_test)\nprint(f'Classification Report: \\n{classification_report(pyy_test, preds)}')\n```\n\n```{python}\n#| label: plot-confusionMatrix-RF-python\n#| echo: true\n#| warning: false\n#| message: false\n#| error: false\n\n# Confusion matrix\ncf_matrix = confusion_matrix(pyy_test, preds)\nsns.heatmap(cf_matrix, annot=True, fmt='d', cmap='Blues', cbar=False)\n```\n:::\n\n# eXtrem Gradient Boosting (XGBoost)\n\n## Aplicamos el algoritmo con cross-validation e hiperparameter tunning\n\n::: panel-tabset\n\n## R\n\nPara aplicar los modelos de XGBoost es necesario pasar los datos categoricos en dummies. Una variable *dummy* (también conocida como cualitativa o binaria) es aquella que toma el valor 1 o 0 para indicar la presencia o ausencia de una cierta característica o condición. \n\n```{r}\n#| label: creacionDummies-Xgboost-r\n#| echo: false\n#| eval: false\n#| warning: false\n#| message: false\n#| error: false\n\nlibrary(xgboost)\nlibrary(Matrix)\nlibrary(fastDummies)\n\n\n# Creamos las dummies de cada base de datos \ndtrain <- sparse.model.matrix(RENTA ~ ., data = rtrain)[,-1]\ndtest <- sparse.model.matrix(RENTA ~ ., data = rtest)[,-1]\n\nlibrary(dplyr)\nlibrary(fastDummies)\n\n# Definir el nombre de la variable respuesta\nrespuesta_var <- \"RENTA\"\n\n# Identificar las columnas categóricas (factor o character)\ncategorical_vars <- rtrain %>%\n  select(-all_of(respuesta_var)) %>%\n  select(where(~ is.factor(.) || is.character(.))) %>%\n  names()\n\n# Generar dummies y procesar el dataframe\ndtrain <- rtrain %>%\n  select(-all_of(categorical_vars)) %>% # Eliminar las columnas categóricas originales\n  bind_cols(dummy_cols(df, select_columns = categorical_vars, remove_selected_columns = TRUE)) %>%\n  bind_cols(select(df, all_of(respuesta_var))) # Volver a añadir la variable respuesta\n\n# https://rpubs.com/mharris/multiclass_xgboost\n# https://cran.r-project.org/web/packages/xgboost/vignettes/discoverYourData.html\n```\n\n```{r}\n#| label: parametros-Xgboost-r\n#| echo: false\n#| eval: false\n#| warning: false\n#| message: false\n#| error: false\n\n# Miramos cuales son los parámetros de este algoritmo \nhead(modelLookup(\"xgbTree\"),4)\n\n# se determina la semilla aleatoria\nset.seed(1994)\n\n# Todas las variables excepto eta són por defecto\ntuneGrid <- expand.grid(nrounds = c(5),                   # Menor cantidad de rondas \n                        max_depth = c(6),                 # Profundidad más baja para los árboles\n                        eta = c(0.01, 0.1, 0.3, 0.5),     # Tasa de aprendizaje más alta\n                        gamma = c(1),                     # Regularización\n                        colsample_bytree = c(1),          # Submuestreo de características\n                        min_child_weight = c(1),          # Regularización adicional\n                        subsample = c(1))                 # Submuestreo de datos\n                                         \n```\n\n```{r}\n#| label: algoritmo-Xgboost-r\n#| echo: false\n#| eval: false\n#| warning: false\n#| message: false\n#| error: false\n\n## se determina la semilla aleatoria\nset.seed(1994)\n\n## se entrena el modelo\nmodel <- train(as.factor(RENTA) ~. , \n               data = rtrain, \n               method = \"xgbTree\", \n               objective = \"multi:softprob\",\n               metric = \"Accuracy\",\n               trControl = trainControl(classProbs = TRUE, method = \"cv\", number = 10, \n                                        verboseIter = FALSE), \n               tuneGrid = tuneGrid)\n\n# se muestra la salida del modelo\nmodel$bestTune\n```\n\n```{r}\n#| label: importance-Xgboost-r\n#| echo: false\n#| eval: false\n#| warning: false\n#| message: false\n#| error: false\n\nlibrary(vip)\nvip(model$finalModel)\n```\n\n## Python\n\n```{python}\n#| label: tratamientoDatos-Xgboost-python\n#| echo: false\n#| eval: false\n#| warning: false\n#| message: false\n#| error: false\n\nimport xgboost as xgb\n\n# Convertir los datos a DMatrix\ndtrain = xgb.DMatrix(pyX_train, label = pyy_train)\ndtest = xgb.DMatrix(pyX_test, label = pyy_test)\n```\n\n```{python}\n#| label: parametros-Xgboost-python\n#| echo: false\n#| eval: false\n#| warning: false\n#| message: false\n#| error: false\n\nparams = {\n    'objective': 'multi:softmax',  # Problema de clasificación multiclase\n    'num_class': 3,                # Número de clases (para el conjunto de datos Iris)\n    'max_depth': 3,                # Profundidad máxima de los árboles\n    'eta': 0.1,                    # Tasa de aprendizaje\n    'eval_metric': 'merror'        # Métrica de evaluación: error en clasificación\n}\n```\n\n```{python}\n#| label: algoritmo-Xgboost-python\n#| echo: false\n#| eval: false\n#| warning: false\n#| message: false\n#| error: false\n\n# Entrenar el modelo\nnum_round = 50  # Número de iteraciones (rounds) de boosting\nmodel = xgb.train(params, dtrain, num_round)\n```\n\n```{python}\n#| label: importance-Xgboost-python\n#| echo: false\n#| eval: false\n#| warning: false\n#| message: false\n#| error: false\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nresults = pd.DataFrame(model.feature_importances_, index=pyBCN.columns[:-1]).sort_values(by=0, ascending=False)\n\n# Crear gráfico de barras horizontales\nplt.figure(figsize=(10, 8))\nplt.barh(results.index, results[0], color='skyblue')\n\n# Añadir etiquetas y título\nplt.xlabel('Importancia')\nplt.ylabel('Características')\nplt.title('Importancia de las Características')\nplt.grid(axis='x', linestyle='--', alpha=0.7)\nplt.show()\n```\n\nSi quisieramos hacerlo en cross validación hariamos lo siguiente:\n\n```{python}\n#| label: cv-Xgboost-python\n#| echo: false\n#| eval: false\n#| warning: false\n#| message: false\n#| error: false\n\nfrom sklearn.model_selection import GridSearchCV\nfrom xgboost import XGBClassifier\n\n# Definir el modelo\nxgb_model = XGBClassifier(objective='multi:softmax', num_class=3)\n\n# Definir los hiperparámetros a ajustar\nparam_grid = {\n    'max_depth': [3, 5, 7],\n    'eta': [0.01, 0.1, 0.3],\n    'n_estimators': [50, 100, 200]\n}\n\n# Configurar GridSearchCV\ngrid_search = GridSearchCV(estimator=xgb_model, param_grid=param_grid, cv=3, scoring='accuracy')\n\n# Entrenar el modelo con ajuste de hiperparámetros\ngrid_search.fit(X_train, y_train)\n\n# Mostrar los mejores parámetros y resultados\nprint(f\"Mejores parámetros: {grid_search.best_params_}\")\nprint(f\"Mejor precisión: {grid_search.best_score_:.4f}\")\n```\n:::\n\n## Predicciones\n\n::: panel-tabset\n## R\n\n```{r}\n#| label: predicciones-Xgboost-r\n#| echo: false\n#| eval: false\n#| warning: false\n#| message: false\n#| error: false\n\nprediccion <- predict(model$finalModel, dtest, type = \"prob\")\n\n# Realizamos las predicciones de este ultimo arbol para la predicción de test\n## Si no decimos nada en type (type = prob), nos devolvera la probabilidad de \n## pertenecer a cada clase. \nrtest_matrix <- rtest %>% select(-RENTA) %>% as.matrix()\nprediccion <- predict(model$finalModel, rtest_matrix, type = \"class\")\n\n## Para ver la performance, realizaremos la matriz de confusión \ncaret::confusionMatrix(prediccion, as.factor(rtest$RENTA))\n```\n\n```{r}\n#| label: plot-confusionMatrix-Xgboost-r\n#| echo: false\n#| eval: false\n#| warning: false\n#| message: false\n#| error: false\n\nCM <- caret::confusionMatrix(prediccion, as.factor(rtest$RENTA)); CM <- data.frame(CM$table)\n\ngrafico <- ggplot(CM, aes(Prediction,Reference, fill= Freq)) +\n        geom_tile() + geom_text(aes(label=Freq)) +\n        scale_fill_gradient(low=\"white\", high=\"#009194\") +\n        labs(x = \"Reference\",y = \"Prediction\")\n\nplot(grafico)\n```\n\n## Python\n\n```{python}\n#| label: predicciones-Xgboost-python\n#| echo: false\n#| eval: false\n#| warning: false\n#| message: false\n#| error: false \n\n# Realizar predicciones\ny_pred = model.predict(dtest)\n\n# Convertir las predicciones de flotante a enteros (si se usa 'multi:softmax')\ny_pred = np.round(y_pred).astype(int)\n\n```\n\n```{python}\n#| label: plot-confusionMatrix-Xgboost-python\n#| echo: false\n#| eval: false\n#| warning: false\n#| message: false\n#| error: false\n\n# Confusion matrix\ncf_matrix = confusion_matrix(pyy_test, y_pred)\nsns.heatmap(cf_matrix, annot=True, fmt='d', cmap='Blues', cbar=False)\n```\n:::\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"knitr"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../../styles.css"],"toc":true,"number-sections":true,"output-file":"DecisionsTree_RandomForest_XGBoost.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.6.37","resources":"../../material/ANN/","theme":"cerulean","title":"Árboles de Decisión, Random Forest y XGBoost","author":"Dante Conti, Sergi Ramirez, (c) IDEAI","date":"`r Sys.Date()`","date-modified":"`r Sys.Date()`","editor":"visual"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}
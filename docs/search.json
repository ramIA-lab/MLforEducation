[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Métodos Estadísticos para la mineria de datos",
    "section": "",
    "text": "La filosofía de la minería de datos trata de la conversión de datos en conocimiento para la toma de decisiones, y como tal constituye la fase central del proceso de extracción de conocimiento a partir de bases de datos. La minería de datos es un punto de encuentro de diferentes disciplinas:\nJuntas permiten afrontar muchos problemas actuales en cuanto al tratamiento de la información.\nLa asignatura introduce las técnicas más usuales para la resolución de tres tipos de problemas fundamentales: el análisis de datos binarios (transacciones), el análisis de datos científicos (por ejemplo, de genómica) y el análisis de datos de empresas; los cuales configuran buena parte de los problemas actuales que trata la minería de datos.\nComo objetivo paralelo hay utilizar la R, un potente en torno a programación libre.\nEsta web está creada por Dante Conti y Sergi Ramírez, (c) 2024"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "material/trees_ensambleMethods/DecisionsTree_RandomForest_XGBoost.html",
    "href": "material/trees_ensambleMethods/DecisionsTree_RandomForest_XGBoost.html",
    "title": "Árboles de Decisión, Random Forest y XGBoost",
    "section": "",
    "text": "En este ejemplo se entrena un árbol de regresión para predecir el precio unitario de la vivienda en Madrid. Para ello se utilizan los datos de viviendas a la venta en Madrid publicados en Idealista durante el año 2018. Estos datos están incluidos en el paquete idealista18. Las variables que contienen nuestra base de datos son las siguientes:\n\n“ASSETID” : Identificador único del activo\n“PERIOD” : Fecha AAAAMM, indica el trimestre en el que se extrajo el anuncio, utilizamos AAAA03 para el 1.er trimestre, AAAA06 para el 2.º, AAAA09 para el 3.er y AAAA12 para el 4.º\n“PRICE” : Precio de venta del anuncio en idealista expresado en euros\n“UNITPRICE” : Precio en euros por metro cuadrado\n“CONSTRUCTEDAREA” : Superficie construida de la casa en metros cuadrados\n“ROOMNUMBER” : Número de habitaciones\n“BATHNUMBER” : Número de baños\n“HASTERRACE” : Variable ficticia para terraza (toma 1 si hay una terraza, 0 en caso contrario)\n“HASLIFT” : Variable ficticia para ascensor (toma 1 si hay ascensor en el edificio, 0 en caso contrario)\n“HASAIRCONDITIONING” : Variable ficticia para Aire Acondicionado (toma 1 si hay una Aire Acondicionado, 0 en caso contrario)\n“AMENITYID” : Indica las comodidades incluidas (1 - sin muebles, sin comodidades de cocina, 2 - comodidades de cocina, sin muebles, 3 - comodidades de cocina, muebles)\n“HASPARKINGSPACE” : Variable ficticia para estacionamiento (toma 1 si el estacionamiento está incluido en el anuncio, 0 en caso contrario)\n“ISPARKINGSPACEINCLUDEDINPRICE” : Variable ficticia para estacionamiento (toma 1 si el estacionamiento está incluido en el anuncio, 0 en caso contrario)\n“PARKINGSPACEPRICE” : Precio de plaza de parking en euros\n“HASNORTHORIENTATION” : Variable ficticia para orientación (toma 1 si la orientación es Norte en el anuncio, 0 en caso contrario) - Nota importante: las características de orientación no son características ortogonales, una casa orientada al norte también puede estar orientada al este\n“HASSOUTHORIENTATION” : Variable ficticia para orientación (toma 1 si la orientación es Sur en el anuncio, 0 en caso contrario) - Nota importante: las características de orientación no son características ortogonales, una casa orientada al norte también puede estar orientada al este\n“HASEASTORIENTATION” : Variable ficticia para orientación (toma 1 si la orientación es Este en el anuncio, 0 en caso contrario) - Nota importante: las características de orientación no son características ortogonales, una casa orientada al norte también puede estar orientada al este\n“HASWESTORIENTATION” : Variable ficticia para orientación (toma 1 si la orientación es Oeste en el anuncio, 0 en caso contrario) - Nota importante: las características de orientación no son características ortogonales, una casa orientada al norte también puede estar orientada al este\n“HASBOXROOM” : Variable ficticia para boxroom (toma 1 si boxroom está incluido en el anuncio, 0 en caso contrario)\n“HASWARDROBE” : Variable ficticia para vestuario (toma 1 si el vestuario está incluido en el anuncio, 0 en caso contrario)\n“HASSWIMMINGPOOL” : Variable ficticia para piscina (toma 1 si la piscina está incluida en el anuncio, 0 en caso contrario)\n“HASDOORMAN” : Variable ficticia para portero (toma 1 si hay un portero en el edificio, 0 en caso contrario)\n“HASGARDEN” : Variable ficticia para jardín (toma 1 si hay un jardín en el edificio, 0 en caso contrario)\n“ISDUPLEX” : Variable ficticia para dúplex (toma 1 si es un dúplex, 0 en caso contrario)\n“ISSTUDIO” : Variable ficticia para piso de soltero (estudio en español) (toma 1 si es un piso para una sola persona, 0 en caso contrario)\n“ISINTOPFLOOR” : Variable ficticia que indica si el apartamento está ubicado en el piso superior (toma 1 en el piso superior, 0 en caso contrario)\n“CONSTRUCTIONYEAR” : Año de construcción (fuente: anunciante)\n“FLOORCLEAN” : Indica el número de piso del apartamento comenzando desde el valor 0 para la planta baja (fuente: anunciante)\n“FLATLOCATIONID” : Indica el tipo de vistas que tiene el piso (1 - exterior, 2 - interior)\n“CADCONSTRUCTIONYEAR” : Año de construcción según fuente catastral (fuente: catastro), tenga en cuenta que esta cifra puede diferir de la proporcionada por el anunciante\n“CADMAXBUILDINGFLOOR” : Superficie máxima del edificio (fuente: catastro)\n“CADDWELLINGCOUNT” : Recuento de viviendas en el edificio (fuente: catastro)\n“CADASTRALQUALITYID” : Calidad catastral (fuente: catastro)\n“BUILTTYPEID_1” : Valor ficticio para estado del piso: 1 obra nueva 0 en caso contrario (fuente: anunciante)\n“BUILTTYPEID_2” : Valor ficticio para condición plana: 1 segundero a restaurar 0 en caso contrario (fuente: anunciante)\n“BUILTTYPEID_3” : Valor ficticio para estado plano: 1 de segunda mano en buen estado 0 en caso contrario (fuente: anunciante)\n“DISTANCE_TO_CITY_CENTER” : Distancia al centro de la ciudad en km\n“DISTANCE_TO_METRO” : Distancia istancia a una parada de metro en km.\n“DISTANCE_TO_DIAGONAL” : Distancia a la Avenida Diagonal en km; Diagonal es una calle principal que corta la ciudad en diagonal a la cuadrícula de calles.\n“LONGITUDE” : Longitud del activo\n“LATITUDE” : Latitud del activo\n“geometry” : Geometría de características simples en latitud y longitud.\n\nFuente: Idealista\n\nlibrary(\"idealista18\")\nBCN &lt;- get(data(\"Barcelona_Sale\"))\n\n\n# Filtramos la epoca a Navidad\nBCN &lt;- BCN[which(BCN$PERIOD == \"201812\"), ]\n\npisos_sf_BCN &lt;- st_as_sf(BCN, coords = c(\"LONGITUDE\", \"LATITUDE\"), crs = 4326)\n\n# Leer shapefile de secciones censales\nsecciones &lt;- st_read(\"C:/Users/sergi/Downloads/Shapefile/seccionado_2024/SECC_CE_20240101.shp\")\n\n# Transformar pisos al sistema de referencia de las secciones censales\npisos_sf_BCN &lt;- st_transform(pisos_sf_BCN, crs = st_crs(secciones))\n\n# Hacer el match entre pisos y secciones censales\npisos_con_seccion &lt;- st_join(pisos_sf_BCN, secciones, join = st_within)\n\n# Convertir a dataframe para exportar\nBCN &lt;- as.data.frame(pisos_con_seccion)\n\nrm(Barcelona_Sale, Barcelona_Polygons, Barcelona_POIS, pisos_con_seccion, pisos_sf_BCN, secciones); gc()\n\n\nrentaMedia &lt;- read.csv(\"https://raw.githubusercontent.com/miguel-angel-monjas/spain-datasets/refs/heads/master/data/Renta%20media%20en%20Espa%C3%B1a.csv\")\n# NOs quedamos con los datos que nos interesa de Barcelona\nrentaMedia &lt;- rentaMedia[which(rentaMedia$Provincia == \"Barcelona\" & rentaMedia$Tipo.de.elemento == \"sección\"), ]\nrentaMedia$Código.de.territorio &lt;- paste0(\"0\", rentaMedia$Código.de.territorio)\n\n\ncols &lt;- c(\"Renta.media.por.persona\", \"Renta.media.por.hogar\")\n\nm &lt;- match(BCN$CUSEC, rentaMedia$Código.de.territorio)\nBCN[, cols] &lt;- rentaMedia[m, cols]\nEsta web está creada por Dante Conti y Sergi Ramírez, (c) 2024"
  },
  {
    "objectID": "material/trees_ensambleMethods/DecisionsTree_RandomForest_XGBoost.html#análisi-descriptivo-de-los-datos",
    "href": "material/trees_ensambleMethods/DecisionsTree_RandomForest_XGBoost.html#análisi-descriptivo-de-los-datos",
    "title": "Árboles de Decisión, Random Forest y XGBoost",
    "section": "2.1 Análisi descriptivo de los datos",
    "text": "2.1 Análisi descriptivo de los datos\n\n## Descriptiva de los datos\nlibrary(DataExplorer)\nlibrary(lubridate)\nlibrary(dplyr)\n\n## Data Manipulation\nlibrary(reshape2)\n\n## Plotting\nlibrary(ggplot2)\n\n## Descripción completa\nDataExplorer::introduce(BCN)\n\n   rows columns discrete_columns continuous_columns all_missing_columns\n1 23334      36               24                 12                   0\n  total_missing_values complete_rows total_observations memory_usage\n1                    0         23334             840024      6078888\n\n## Descripción de la bbdd\nplot_intro(BCN)\n\n\n\n\n\n\n\n## Descripción de los missings\nplot_missing(BCN)\n\n\n\n\n\n\n\n## Descripción de las varaibles categoricas\nplot_bar(BCN)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n## Descripción variables numéricas\nplot_histogram(BCN)\n\n\n\n\n\n\n\nplot_density(BCN)\n\n\n\n\n\n\n\nplot_qq(BCN)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nplot_correlation(BCN)"
  },
  {
    "objectID": "material/trees_ensambleMethods/DecisionsTree_RandomForest_XGBoost.html#creación-del-árbol",
    "href": "material/trees_ensambleMethods/DecisionsTree_RandomForest_XGBoost.html#creación-del-árbol",
    "title": "Árboles de Decisión, Random Forest y XGBoost",
    "section": "4.1 Creación del árbol",
    "text": "4.1 Creación del árbol\n\nRPython\n\n\n\nlibrary(rpart)\nlibrary(rpart.plot)\n\nset.seed(1994)\n\narbol &lt;- rpart(RENTA ~ ., data = rtrain)\nsummary(arbol)\n\nCall:\nrpart(formula = RENTA ~ ., data = rtrain)\n  n= 18669 \n\n          CP nsplit rel error    xerror        xstd\n1 0.38211183      0 1.0000000 1.0000000 0.008293935\n2 0.06753946      1 0.6178882 0.6178882 0.007426365\n3 0.04447571      2 0.5503487 0.5503487 0.007149373\n4 0.02214609      4 0.4613973 0.4643338 0.006727872\n5 0.01884253      5 0.4392512 0.4425548 0.006607381\n6 0.01186835      6 0.4204087 0.4267711 0.006516230\n7 0.01162364      8 0.3966720 0.4034014 0.006375036\n8 0.01015539     10 0.3734247 0.3792977 0.006221129\n9 0.01000000     12 0.3531139 0.3676740 0.006143722\n\nVariable importance\n                   CDIS DISTANCE_TO_CITY_CENTER    DISTANCE_TO_DIAGONAL \n                     40                      24                      17 \n    CADCONSTRUCTIONYEAR         CONSTRUCTEDAREA               UNITPRICE \n                      8                       3                       3 \n      DISTANCE_TO_METRO              BATHNUMBER              ROOMNUMBER \n                      2                       1                       1 \n\nNode number 1: 18669 observations,    complexity param=0.3821118\n  predicted class=Media  expected loss=0.4377846  P(node) =1\n    class counts:  1941  6232 10496\n   probabilities: 0.104 0.334 0.562 \n  left son=2 (4541 obs) right son=3 (14128 obs)\n  Primary splits:\n      CDIS                    splits as  LRRRRLRRRR, improve=2615.4350, (0 missing)\n      DISTANCE_TO_DIAGONAL    &lt; 1.697581   to the right, improve=1710.4180, (0 missing)\n      HASLIFT                 splits as  LR, improve= 961.7684, (0 missing)\n      DISTANCE_TO_CITY_CENTER &lt; 1.146692   to the left,  improve= 885.2803, (0 missing)\n      CADCONSTRUCTIONYEAR     &lt; 1900.5     to the left,  improve= 800.8061, (0 missing)\n  Surrogate splits:\n      DISTANCE_TO_CITY_CENTER &lt; 1.146692   to the left,  agree=0.853, adj=0.396, (0 split)\n      CADCONSTRUCTIONYEAR     &lt; 1900.5     to the left,  agree=0.823, adj=0.273, (0 split)\n      DISTANCE_TO_DIAGONAL    &lt; 4.28574    to the right, agree=0.775, adj=0.076, (0 split)\n      CONSTRUCTEDAREA         &lt; 46.5       to the left,  agree=0.769, adj=0.052, (0 split)\n      ROOMNUMBER              &lt; 11         to the right, agree=0.757, adj=0.001, (0 split)\n\nNode number 2: 4541 observations,    complexity param=0.02214609\n  predicted class=Baja   expected loss=0.156133  P(node) =0.2432375\n    class counts:     0  3832   709\n   probabilities: 0.000 0.844 0.156 \n  left son=4 (4294 obs) right son=5 (247 obs)\n  Primary splits:\n      DISTANCE_TO_CITY_CENTER &lt; 0.4629643  to the right, improve=263.54560, (0 missing)\n      DISTANCE_TO_DIAGONAL    &lt; 1.293729   to the right, improve= 85.60347, (0 missing)\n      CDIS                    splits as  L----R----, improve= 66.85504, (0 missing)\n      HASLIFT                 splits as  LR, improve= 65.14028, (0 missing)\n      CADCONSTRUCTIONYEAR     &lt; 1950.5     to the left,  improve= 50.10187, (0 missing)\n\nNode number 3: 14128 observations,    complexity param=0.06753946\n  predicted class=Media  expected loss=0.3072622  P(node) =0.7567625\n    class counts:  1941  2400  9787\n   probabilities: 0.137 0.170 0.693 \n  left son=6 (1990 obs) right son=7 (12138 obs)\n  Primary splits:\n      CDIS                 splits as  -RRRL-RRRL, improve=903.3696, (0 missing)\n      DISTANCE_TO_DIAGONAL &lt; 2.370657   to the right, improve=788.5626, (0 missing)\n      CONSTRUCTEDAREA      &lt; 137.5      to the right, improve=434.3222, (0 missing)\n      UNITPRICE            &lt; 2898.227   to the left,  improve=369.9583, (0 missing)\n      BATHNUMBER           &lt; 2.5        to the right, improve=353.8536, (0 missing)\n  Surrogate splits:\n      CONSTRUCTEDAREA   &lt; 223.5      to the right, agree=0.866, adj=0.052, (0 split)\n      BATHNUMBER        &lt; 3.5        to the right, agree=0.864, adj=0.038, (0 split)\n      DISTANCE_TO_METRO &lt; 1.634495   to the right, agree=0.860, adj=0.006, (0 split)\n      ROOMNUMBER        &lt; 6.5        to the right, agree=0.859, adj=0.003, (0 split)\n      UNITPRICE         &lt; 6992.805   to the right, agree=0.859, adj=0.001, (0 split)\n\nNode number 4: 4294 observations,    complexity param=0.01015539\n  predicted class=Baja   expected loss=0.1152771  P(node) =0.230007\n    class counts:     0  3799   495\n   probabilities: 0.000 0.885 0.115 \n  left son=8 (3278 obs) right son=9 (1016 obs)\n  Primary splits:\n      DISTANCE_TO_CITY_CENTER &lt; 3.316192   to the left,  improve=107.18450, (0 missing)\n      CDIS                    splits as  L----R----, improve=107.18450, (0 missing)\n      DISTANCE_TO_DIAGONAL    &lt; 2.818623   to the left,  improve= 91.99937, (0 missing)\n      CADCONSTRUCTIONYEAR     &lt; 1950.5     to the left,  improve= 66.10102, (0 missing)\n      HASLIFT                 splits as  LR, improve= 49.70642, (0 missing)\n  Surrogate splits:\n      DISTANCE_TO_DIAGONAL &lt; 2.826385   to the left,  agree=0.989, adj=0.955, (0 split)\n      CADCONSTRUCTIONYEAR  &lt; 1950.5     to the left,  agree=0.886, adj=0.517, (0 split)\n      UNITPRICE            &lt; 2840.561   to the right, agree=0.876, adj=0.475, (0 split)\n      CADDWELLINGCOUNT     &lt; 30.5       to the left,  agree=0.789, adj=0.106, (0 split)\n      CADMAXBUILDINGFLOOR  &lt; 7.5        to the left,  agree=0.776, adj=0.053, (0 split)\n\nNode number 5: 247 observations\n  predicted class=Media  expected loss=0.1336032  P(node) =0.01323049\n    class counts:     0    33   214\n   probabilities: 0.000 0.134 0.866 \n\nNode number 6: 1990 observations,    complexity param=0.01162364\n  predicted class=Alta   expected loss=0.3613065  P(node) =0.1065938\n    class counts:  1271     0   719\n   probabilities: 0.639 0.000 0.361 \n  left son=12 (1077 obs) right son=13 (913 obs)\n  Primary splits:\n      CONSTRUCTEDAREA      &lt; 119.5      to the right, improve=121.31850, (0 missing)\n      BATHNUMBER           &lt; 2.5        to the right, improve= 87.03903, (0 missing)\n      DISTANCE_TO_DIAGONAL &lt; 0.5534786  to the left,  improve= 85.36516, (0 missing)\n      CDIS                 splits as  ----R----L, improve= 68.18003, (0 missing)\n      UNITPRICE            &lt; 5287.749   to the right, improve= 60.24694, (0 missing)\n  Surrogate splits:\n      ROOMNUMBER                    &lt; 3.5        to the right, agree=0.790, adj=0.542, (0 split)\n      BATHNUMBER                    &lt; 1.5        to the right, agree=0.747, adj=0.448, (0 split)\n      HASTERRACE                    splits as  RL, agree=0.647, adj=0.230, (0 split)\n      HASPARKINGSPACE               splits as  RL, agree=0.641, adj=0.217, (0 split)\n      ISPARKINGSPACEINCLUDEDINPRICE splits as  RL, agree=0.641, adj=0.217, (0 split)\n\nNode number 7: 12138 observations,    complexity param=0.04447571\n  predicted class=Media  expected loss=0.2529247  P(node) =0.6501687\n    class counts:   670  2400  9068\n   probabilities: 0.055 0.198 0.747 \n  left son=14 (2820 obs) right son=15 (9318 obs)\n  Primary splits:\n      DISTANCE_TO_DIAGONAL    &lt; 2.370657   to the right, improve=832.9688, (0 missing)\n      CDIS                    splits as  -RRL--RLL-, improve=615.9082, (0 missing)\n      UNITPRICE               &lt; 2898.227   to the left,  improve=380.3717, (0 missing)\n      DISTANCE_TO_CITY_CENTER &lt; 3.940257   to the right, improve=338.4368, (0 missing)\n      CONSTRUCTEDAREA         &lt; 80.5       to the left,  improve=178.0100, (0 missing)\n  Surrogate splits:\n      DISTANCE_TO_CITY_CENTER &lt; 4.058752   to the right, agree=0.897, adj=0.558, (0 split)\n      CDIS                    splits as  -RRL--LRR-, agree=0.816, adj=0.206, (0 split)\n      UNITPRICE               &lt; 2753.972   to the left,  agree=0.811, adj=0.184, (0 split)\n      DISTANCE_TO_METRO       &lt; 0.7218963  to the right, agree=0.780, adj=0.051, (0 split)\n      CADMAXBUILDINGFLOOR     &lt; 26.5       to the right, agree=0.769, adj=0.005, (0 split)\n\nNode number 8: 3278 observations\n  predicted class=Baja   expected loss=0.05308115  P(node) =0.1755852\n    class counts:     0  3104   174\n   probabilities: 0.000 0.947 0.053 \n\nNode number 9: 1016 observations,    complexity param=0.01015539\n  predicted class=Baja   expected loss=0.3159449  P(node) =0.05442177\n    class counts:     0   695   321\n   probabilities: 0.000 0.684 0.316 \n  left son=18 (712 obs) right son=19 (304 obs)\n  Primary splits:\n      DISTANCE_TO_DIAGONAL    &lt; 3.420441   to the right, improve=181.26100, (0 missing)\n      DISTANCE_TO_CITY_CENTER &lt; 4.8151     to the right, improve=162.58570, (0 missing)\n      UNITPRICE               &lt; 2586.19    to the left,  improve= 62.53053, (0 missing)\n      HASLIFT                 splits as  LR, improve= 54.20484, (0 missing)\n      CONSTRUCTEDAREA         &lt; 74.5       to the left,  improve= 31.09944, (0 missing)\n  Surrogate splits:\n      DISTANCE_TO_CITY_CENTER &lt; 5.091757   to the right, agree=0.958, adj=0.859, (0 split)\n      UNITPRICE               &lt; 3251.667   to the left,  agree=0.775, adj=0.247, (0 split)\n      CADCONSTRUCTIONYEAR     &lt; 1954.5     to the right, agree=0.708, adj=0.023, (0 split)\n      BUILTTYPEID_1           splits as  LR, agree=0.707, adj=0.020, (0 split)\n      DISTANCE_TO_METRO       &lt; 0.02808259 to the right, agree=0.707, adj=0.020, (0 split)\n\nNode number 12: 1077 observations\n  predicted class=Alta   expected loss=0.2005571  P(node) =0.05768922\n    class counts:   861     0   216\n   probabilities: 0.799 0.000 0.201 \n\nNode number 13: 913 observations,    complexity param=0.01162364\n  predicted class=Media  expected loss=0.449069  P(node) =0.0489046\n    class counts:   410     0   503\n   probabilities: 0.449 0.000 0.551 \n  left son=26 (495 obs) right son=27 (418 obs)\n  Primary splits:\n      CDIS                    splits as  ----R----L, improve=47.94928, (0 missing)\n      DISTANCE_TO_DIAGONAL    &lt; 0.5540544  to the left,  improve=44.18423, (0 missing)\n      UNITPRICE               &lt; 5289.352   to the right, improve=37.28880, (0 missing)\n      DISTANCE_TO_METRO       &lt; 0.2951583  to the right, improve=24.68798, (0 missing)\n      DISTANCE_TO_CITY_CENTER &lt; 2.14678    to the left,  improve=21.64891, (0 missing)\n  Surrogate splits:\n      CADDWELLINGCOUNT        &lt; 30.5       to the left,  agree=0.729, adj=0.409, (0 split)\n      DISTANCE_TO_DIAGONAL    &lt; 1.062684   to the right, agree=0.705, adj=0.356, (0 split)\n      UNITPRICE               &lt; 4821.591   to the right, agree=0.677, adj=0.294, (0 split)\n      CADMAXBUILDINGFLOOR     &lt; 8.5        to the left,  agree=0.662, adj=0.261, (0 split)\n      DISTANCE_TO_CITY_CENTER &lt; 3.575557   to the left,  agree=0.623, adj=0.177, (0 split)\n\nNode number 14: 2820 observations,    complexity param=0.04447571\n  predicted class=Baja   expected loss=0.4553191  P(node) =0.1510525\n    class counts:    95  1536  1189\n   probabilities: 0.034 0.545 0.422 \n  left son=28 (2010 obs) right son=29 (810 obs)\n  Primary splits:\n      CDIS                    splits as  -RRL--RLL-, improve=202.04190, (0 missing)\n      DISTANCE_TO_CITY_CENTER &lt; 2.380396   to the left,  improve=170.89780, (0 missing)\n      DISTANCE_TO_DIAGONAL    &lt; 3.446593   to the left,  improve=103.80040, (0 missing)\n      CADCONSTRUCTIONYEAR     &lt; 1971.5     to the left,  improve= 77.57975, (0 missing)\n      CONSTRUCTEDAREA         &lt; 86.5       to the left,  improve= 62.41583, (0 missing)\n  Surrogate splits:\n      DISTANCE_TO_CITY_CENTER &lt; 5.730574   to the left,  agree=0.819, adj=0.370, (0 split)\n      DISTANCE_TO_DIAGONAL    &lt; 3.716013   to the left,  agree=0.759, adj=0.160, (0 split)\n      CONSTRUCTEDAREA         &lt; 130.5      to the left,  agree=0.716, adj=0.012, (0 split)\n      BATHNUMBER              &lt; 2.5        to the left,  agree=0.715, adj=0.006, (0 split)\n      DISTANCE_TO_METRO       &lt; 0.01826087 to the right, agree=0.714, adj=0.005, (0 split)\n\nNode number 15: 9318 observations,    complexity param=0.01186835\n  predicted class=Media  expected loss=0.1544323  P(node) =0.4991162\n    class counts:   575   864  7879\n   probabilities: 0.062 0.093 0.846 \n  left son=30 (523 obs) right son=31 (8795 obs)\n  Primary splits:\n      DISTANCE_TO_CITY_CENTER &lt; 0.871618   to the left,  improve=157.24670, (0 missing)\n      CDIS                    splits as  -RRL--RLL-, improve=145.36290, (0 missing)\n      CADDWELLINGCOUNT        &lt; 167.5      to the right, improve= 73.55115, (0 missing)\n      CONSTRUCTEDAREA         &lt; 135.5      to the right, improve= 56.59289, (0 missing)\n      DISTANCE_TO_DIAGONAL    &lt; 1.820647   to the right, improve= 55.50302, (0 missing)\n  Surrogate splits:\n      BATHNUMBER &lt; 0.5        to the left,  agree=0.944, adj=0.002, (0 split)\n\nNode number 18: 712 observations\n  predicted class=Baja   expected loss=0.1207865  P(node) =0.03813809\n    class counts:     0   626    86\n   probabilities: 0.000 0.879 0.121 \n\nNode number 19: 304 observations\n  predicted class=Media  expected loss=0.2269737  P(node) =0.01628368\n    class counts:     0    69   235\n   probabilities: 0.000 0.227 0.773 \n\nNode number 26: 495 observations\n  predicted class=Alta   expected loss=0.4020202  P(node) =0.02651454\n    class counts:   296     0   199\n   probabilities: 0.598 0.000 0.402 \n\nNode number 27: 418 observations\n  predicted class=Media  expected loss=0.2727273  P(node) =0.02239006\n    class counts:   114     0   304\n   probabilities: 0.273 0.000 0.727 \n\nNode number 28: 2010 observations,    complexity param=0.01884253\n  predicted class=Baja   expected loss=0.3278607  P(node) =0.1076651\n    class counts:    35  1351   624\n   probabilities: 0.017 0.672 0.310 \n  left son=56 (1748 obs) right son=57 (262 obs)\n  Primary splits:\n      DISTANCE_TO_DIAGONAL    &lt; 3.446593   to the left,  improve=135.93570, (0 missing)\n      DISTANCE_TO_CITY_CENTER &lt; 2.380396   to the left,  improve=112.73140, (0 missing)\n      DISTANCE_TO_METRO       &lt; 0.3243732  to the left,  improve= 83.82446, (0 missing)\n      CADCONSTRUCTIONYEAR     &lt; 1967.5     to the left,  improve= 70.86276, (0 missing)\n      CADMAXBUILDINGFLOOR     &lt; 7.5        to the left,  improve= 37.15447, (0 missing)\n  Surrogate splits:\n      DISTANCE_TO_CITY_CENTER &lt; 5.208402   to the left,  agree=0.949, adj=0.611, (0 split)\n      DISTANCE_TO_METRO       &lt; 0.7342067  to the left,  agree=0.904, adj=0.267, (0 split)\n      PARKINGSPACEPRICE       &lt; 28451      to the left,  agree=0.872, adj=0.015, (0 split)\n      CADMAXBUILDINGFLOOR     &lt; 1.5        to the right, agree=0.872, adj=0.015, (0 split)\n\nNode number 29: 810 observations\n  predicted class=Media  expected loss=0.3024691  P(node) =0.04338743\n    class counts:    60   185   565\n   probabilities: 0.074 0.228 0.698 \n\nNode number 30: 523 observations,    complexity param=0.01186835\n  predicted class=Media  expected loss=0.4760994  P(node) =0.02801436\n    class counts:   249     0   274\n   probabilities: 0.476 0.000 0.524 \n  left son=60 (284 obs) right son=61 (239 obs)\n  Primary splits:\n      DISTANCE_TO_DIAGONAL    &lt; 0.990107   to the left,  improve=165.999900, (0 missing)\n      DISTANCE_TO_METRO       &lt; 0.1705538  to the right, improve= 38.560880, (0 missing)\n      DISTANCE_TO_CITY_CENTER &lt; 0.44858    to the right, improve= 36.859370, (0 missing)\n      UNITPRICE               &lt; 4976.906   to the right, improve= 12.669910, (0 missing)\n      CONSTRUCTEDAREA         &lt; 124.5      to the right, improve=  9.569507, (0 missing)\n  Surrogate splits:\n      DISTANCE_TO_METRO       &lt; 0.1699124  to the right, agree=0.744, adj=0.439, (0 split)\n      DISTANCE_TO_CITY_CENTER &lt; 0.4805939  to the right, agree=0.728, adj=0.406, (0 split)\n      UNITPRICE               &lt; 4976.906   to the right, agree=0.642, adj=0.218, (0 split)\n      CADCONSTRUCTIONYEAR     &lt; 1941.5     to the left,  agree=0.587, adj=0.096, (0 split)\n      CADDWELLINGCOUNT        &lt; 24.5       to the left,  agree=0.583, adj=0.088, (0 split)\n\nNode number 31: 8795 observations\n  predicted class=Media  expected loss=0.1353042  P(node) =0.4711018\n    class counts:   326   864  7605\n   probabilities: 0.037 0.098 0.865 \n\nNode number 56: 1748 observations\n  predicted class=Baja   expected loss=0.2580092  P(node) =0.09363115\n    class counts:    35  1297   416\n   probabilities: 0.020 0.742 0.238 \n\nNode number 57: 262 observations\n  predicted class=Media  expected loss=0.2061069  P(node) =0.01403396\n    class counts:     0    54   208\n   probabilities: 0.000 0.206 0.794 \n\nNode number 60: 284 observations\n  predicted class=Alta   expected loss=0.1584507  P(node) =0.01521238\n    class counts:   239     0    45\n   probabilities: 0.842 0.000 0.158 \n\nNode number 61: 239 observations\n  predicted class=Media  expected loss=0.041841  P(node) =0.01280197\n    class counts:    10     0   229\n   probabilities: 0.042 0.000 0.958 \n\n\n\nrpart.plot(arbol)\n\n\n\n\n\n\n\n\n\n\n\n# Decision Tree Classification\nfrom sklearn.tree import DecisionTreeClassifier\n\nclassifier = DecisionTreeClassifier(criterion = 'entropy', random_state = 1994)\nclf = classifier.fit(pyX_train, pyy_train)\n\n\nfrom sklearn import tree\n\ntree.plot_tree(clf)"
  },
  {
    "objectID": "material/trees_ensambleMethods/DecisionsTree_RandomForest_XGBoost.html#creamos-las-predicciones",
    "href": "material/trees_ensambleMethods/DecisionsTree_RandomForest_XGBoost.html#creamos-las-predicciones",
    "title": "Árboles de Decisión, Random Forest y XGBoost",
    "section": "4.2 Creamos las predicciones",
    "text": "4.2 Creamos las predicciones\n\nRPython\n\n\nAplicamos el modelo a nuestros valores de test.\n\npredict(arbol, rtest[1:10, ])\n\n   Alta      Baja      Media\n1     0 0.9469189 0.05308115\n2     0 0.9469189 0.05308115\n3     0 0.9469189 0.05308115\n4     0 0.9469189 0.05308115\n5     0 0.9469189 0.05308115\n6     0 0.1336032 0.86639676\n7     0 0.9469189 0.05308115\n8     0 0.9469189 0.05308115\n9     0 0.9469189 0.05308115\n10    0 0.1336032 0.86639676\n\n\n\npredicciones &lt;- predict(arbol, rtrain, type = \"class\")\ncaret::confusionMatrix(predicciones, as.factor(rtrain$RENTA))\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction Alta Baja Media\n     Alta  1396    0   460\n     Baja    35 5027   676\n     Media  510 1205  9360\n\nOverall Statistics\n                                          \n               Accuracy : 0.8454          \n                 95% CI : (0.8401, 0.8506)\n    No Information Rate : 0.5622          \n    P-Value [Acc &gt; NIR] : &lt; 2.2e-16       \n                                          \n                  Kappa : 0.7207          \n                                          \n Mcnemar's Test P-Value : &lt; 2.2e-16       \n\nStatistics by Class:\n\n                     Class: Alta Class: Baja Class: Media\nSensitivity              0.71922      0.8066       0.8918\nSpecificity              0.97250      0.9428       0.7902\nPos Pred Value           0.75216      0.8761       0.8451\nNeg Pred Value           0.96758      0.9068       0.8504\nPrevalence               0.10397      0.3338       0.5622\nDetection Rate           0.07478      0.2693       0.5014\nDetection Prevalence     0.09942      0.3074       0.5932\nBalanced Accuracy        0.84586      0.8747       0.8410\n\npredicciones &lt;- predict(arbol, rtest, type = \"class\")\ncaret::confusionMatrix(predicciones, as.factor(rtest$RENTA))\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction Alta Baja Media\n     Alta   311    1   148\n     Baja     5 1250   171\n     Media  151  266  2362\n\nOverall Statistics\n                                          \n               Accuracy : 0.8409          \n                 95% CI : (0.8301, 0.8513)\n    No Information Rate : 0.5747          \n    P-Value [Acc &gt; NIR] : &lt; 2.2e-16       \n                                          \n                  Kappa : 0.7099          \n                                          \n Mcnemar's Test P-Value : 3.415e-05       \n\nStatistics by Class:\n\n                     Class: Alta Class: Baja Class: Media\nSensitivity              0.66595      0.8240       0.8810\nSpecificity              0.96451      0.9441       0.7898\nPos Pred Value           0.67609      0.8766       0.8499\nNeg Pred Value           0.96290      0.9176       0.8309\nPrevalence               0.10011      0.3252       0.5747\nDetection Rate           0.06667      0.2680       0.5063\nDetection Prevalence     0.09861      0.3057       0.5957\nBalanced Accuracy        0.81523      0.8840       0.8354\n\n\n\nCM &lt;- caret::confusionMatrix(predicciones, as.factor(rtest$RENTA)); CM &lt;- data.frame(CM$table)\n\ngrafico &lt;- ggplot(CM, aes(Prediction,Reference, fill= Freq)) +\n        geom_tile() + geom_text(aes(label=Freq)) +\n        scale_fill_gradient(low=\"white\", high=\"#009194\") +\n        labs(x = \"Reference\",y = \"Prediction\")\n\nplot(grafico)\n\n\n\n\n\n\n\n\n\n\n\nimport pandas as pd\n\n# Prediction\ny_pred = classifier.predict(pyX_test)\n\nresults = pd.DataFrame({\n    'Real': pyy_test,  # Valores reales\n    'Predicho': y_pred  # Valores predichos\n})\n\n# Muestra los primeros 5 registros\nprint(results.head())  \n\n       Real  Predicho\n2399      2         2\n18420     0         0\n22515     2         2\n20265     1         1\n10432     1         1\n\n\n\nfrom sklearn.metrics import classification_report\n\nprint(f'Classification Report: \\n{classification_report(pyy_test, y_pred)}')\n\nClassification Report: \n              precision    recall  f1-score   support\n\n           0       0.85      0.82      0.84       488\n           1       0.92      0.91      0.91      1522\n           2       0.92      0.93      0.92      2657\n\n    accuracy                           0.91      4667\n   macro avg       0.89      0.89      0.89      4667\nweighted avg       0.91      0.91      0.91      4667\n\n\n\nfrom sklearn.metrics import confusion_matrix\nimport seaborn as sns\n\n# Confusion matrix\ncf_matrix = confusion_matrix(pyy_test, y_pred)\nsns.heatmap(cf_matrix, annot=True, fmt='d', cmap='Blues', cbar=False)"
  },
  {
    "objectID": "material/trees_ensambleMethods/DecisionsTree_RandomForest_XGBoost.html#modelo-de-classificación-con-cross-evaluación",
    "href": "material/trees_ensambleMethods/DecisionsTree_RandomForest_XGBoost.html#modelo-de-classificación-con-cross-evaluación",
    "title": "Árboles de Decisión, Random Forest y XGBoost",
    "section": "4.3 Modelo de classificación con cross-evaluación",
    "text": "4.3 Modelo de classificación con cross-evaluación\n\nRPython\n\n\n\n## Generamos los parámetros de control\ntrControl &lt;- trainControl(method = \"cv\", number = 10, classProbs = TRUE,\n  summaryFunction = multiClassSummary)\n## En este caso, se realiza una cros-validación de 10 etapas\n\n# se fija una semilla aleatoria\nset.seed(1994)\n\n# se entrena el modelo\nmodel &lt;- train(RENTA ~ .,  # . equivale a incluir todas las variables\n               data = rtrain,\n               method = \"rpart\",\n               metric = \"Accuracy\",\n               trControl = trControl)\n\n# Obtenemos los valores del árbol óptimo\nmodel$finalModel\n\nn= 18669 \n\nnode), split, n, loss, yval, (yprob)\n      * denotes terminal node\n\n 1) root 18669 8173 Media (0.10396915 0.33381542 0.56221544)  \n   2) DISTANCE_TO_DIAGONAL&gt;=1.697581 9078 3793 Baja (0.02599692 0.58217669 0.39182639)  \n     4) DISTANCE_TO_CITY_CENTER&lt; 1.182486 2168  144 Baja (0.00000000 0.93357934 0.06642066) *\n     5) DISTANCE_TO_CITY_CENTER&gt;=1.182486 6910 3497 Media (0.03415340 0.47192475 0.49392185)  \n      10) DISTANCE_TO_DIAGONAL&gt;=2.350599 4436 1689 Baja (0.02705140 0.61925158 0.35369702) *\n      11) DISTANCE_TO_DIAGONAL&lt; 2.350599 2474  630 Media (0.04688763 0.20776071 0.74535166) *\n   3) DISTANCE_TO_DIAGONAL&lt; 1.697581 9591 2652 Media (0.17777083 0.09873840 0.72349077) *\n\n# Generamos el gráfico del árbol\nrpart.plot(model$finalModel)\n\n\n\n\n\n\n\n\n\nlibrary(reshape2)\n\n# A continuación generamos un gráfico que nos permite ver la variabilidad de los estadísticos\n# calculados\nggplot(melt(model$resample[,c(2:5, 7:9, 12:13)]), aes(x = variable, y = value, fill=variable)) +\n  geom_boxplot(show.legend=FALSE) +\n  xlab(NULL) + ylab(NULL)\n\n\n\n\n\n\n\n\n\n\n\nfrom sklearn.tree import DecisionTreeClassifier\n\n# Modelo de árbol de decisión\nmodel = DecisionTreeClassifier(random_state=1994)\n\nfrom sklearn.model_selection import cross_val_score\n\n# Realizar validación cruzada con 5 folds\nscores = cross_val_score(model, pyX_train, pyy_train, cv=10, scoring = 'accuracy')  # Métrica: accuracy\n\n# Mostrar resultados\nprint(f\"Accuracy por fold: {scores}\")\n\nAccuracy por fold: [0.90626674 0.90680236 0.8993037  0.90305303 0.91215854 0.90787359\n 0.89769684 0.90836013 0.914791   0.91425509]\n\nprint(f\"Accuracy promedio: {scores.mean():.4f}\")\n\nAccuracy promedio: 0.9071"
  },
  {
    "objectID": "material/trees_ensambleMethods/DecisionsTree_RandomForest_XGBoost.html#realizando-hiperparámetro-tunning",
    "href": "material/trees_ensambleMethods/DecisionsTree_RandomForest_XGBoost.html#realizando-hiperparámetro-tunning",
    "title": "Árboles de Decisión, Random Forest y XGBoost",
    "section": "4.4 Realizando hiperparámetro tunning",
    "text": "4.4 Realizando hiperparámetro tunning\n\nRPython\n\n\n\n# Detectamos cuales son los parámetros del modelo que podemos realizar hiperparámeter tunning\nmodelLookup(\"rpart\")\n\n  model parameter                label forReg forClass probModel\n1 rpart        cp Complexity Parameter   TRUE     TRUE      TRUE\n\n\n\n# Se especifica un rango de valores típicos para el hiperparámetro\ntuneGrid &lt;- expand.grid(cp = seq(0.01,0.05,0.01))\n\n\n# se entrena el modelo\nset.seed(1994)\n\nmodel &lt;- train(RENTA ~ .,\n               data = rtrain,\n               method = \"rpart\",\n               metric = \"Accuracy\",\n               trControl = trControl,\n               tuneGrid = tuneGrid)\n\n# Obtenemos la información del mejor modelo\nmodel$bestTune\n\n    cp\n1 0.01\n\n# Gráfico del árbol obtenido\nrpart.plot(model$finalModel)\n\n\n\n\n\n\n\n\n\n\n\nfrom sklearn.model_selection import GridSearchCV\n\n# Definir rejilla de hiperparámetros\nparam_grid = {\n    'max_depth': [None, 5, 10],\n    'min_samples_split': [2, 5, 10],\n    'min_samples_leaf': [1, 2, 4]\n}\n\n# Declaramos el modelo\nmodel = DecisionTreeClassifier(random_state=1994)\n\n# Configurar GridSearch con validación cruzada\ngrid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=10, scoring='accuracy', n_jobs=-1)\n\n# Ajustar modelo\ngrid_search.fit(pyX_train, pyy_train)\n\nGridSearchCV(cv=10, estimator=DecisionTreeClassifier(random_state=1994),\n             n_jobs=-1,\n             param_grid={'max_depth': [None, 5, 10],\n                         'min_samples_leaf': [1, 2, 4],\n                         'min_samples_split': [2, 5, 10]},\n             scoring='accuracy')In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  GridSearchCV?Documentation for GridSearchCViFittedGridSearchCV(cv=10, estimator=DecisionTreeClassifier(random_state=1994),\n             n_jobs=-1,\n             param_grid={'max_depth': [None, 5, 10],\n                         'min_samples_leaf': [1, 2, 4],\n                         'min_samples_split': [2, 5, 10]},\n             scoring='accuracy') best_estimator_: DecisionTreeClassifierDecisionTreeClassifier(random_state=1994)  DecisionTreeClassifier?Documentation for DecisionTreeClassifierDecisionTreeClassifier(random_state=1994) \n\n# Mostrar mejores parámetros\nprint(f\"Mejores parámetros: {grid_search.best_params_}\")\n\nMejores parámetros: {'max_depth': None, 'min_samples_leaf': 1, 'min_samples_split': 2}\n\nprint(f\"Mejor accuracy: {grid_search.best_score_:.4f}\")\n\nMejor accuracy: 0.9071\n\n\n\nfrom sklearn import tree\ntree.plot_tree(grid_search.best_estimator_)"
  },
  {
    "objectID": "material/trees_ensambleMethods/DecisionsTree_RandomForest_XGBoost.html#como-realizar-poda-de-nuestro-árbol",
    "href": "material/trees_ensambleMethods/DecisionsTree_RandomForest_XGBoost.html#como-realizar-poda-de-nuestro-árbol",
    "title": "Árboles de Decisión, Random Forest y XGBoost",
    "section": "4.5 Como realizar poda de nuestro árbol",
    "text": "4.5 Como realizar poda de nuestro árbol\n\nRPython\n\n\n\n# Con el objetivo de aumentar la generalidad del árbol y facilitar su interpretación, \n# se procede a reducir su tamaño podándolo. Para ello se establece el criterio de \n# que un nodo terminal tiene que tener, como mínimo, 50 observaciones.\nset.seed(1994)\nprunedtree &lt;- rpart(RENTA ~ ., data = rtrain,\n                    cp= 0.01, control = rpart.control(minbucket = 50))\n\nrpart.plot(prunedtree)\n\n\n\n\n\n\n\n\n\n\nEn Python, la poda de un árbol de decisión se puede realizar ajustando los hiperparámetros del árbol durante su creación. Estos hiperparámetros controlan el crecimiento del árbol y, por lo tanto, actúan como técnicas de poda preventiva o postpoda.\nscikit-learn no implementa poda dinámica directa (como ocurre en algunos otros frameworks), pero puedes limitar el tamaño del árbol y evitar sobreajuste mediante los siguientes métodos.\n\n4.5.1 Poda Preventiva (Pre-pruning)\nPoda preventiva consiste en detener el crecimiento del árbol antes de que se haga demasiado grande. Esto se logra ajustando hiperparámetros como:\n\nmax_depth: Profundidad máxima del árbol\nmin_samples_split: Número mínimo de muestras necesarias para dividir un nodo.\nmin_samples_leaf: Número mínimo de muestras necesarias en una hoja.\nmax_leaf_nodes: Número máximo de nodos hoja en el árbol.\n\n\n# Crear un árbol con poda preventiva\nmodel = DecisionTreeClassifier(\n    max_depth=3,              # Limitar la profundidad\n    min_samples_split=10,     # Mínimo 10 muestras para dividir un nodo\n    min_samples_leaf=5,       # Mínimo 5 muestras por hoja\n    random_state=42\n)\n\n# Entrenar el modelo\nmodel.fit(pyX_train, pyy_train)\n\nDecisionTreeClassifier(max_depth=3, min_samples_leaf=5, min_samples_split=10,\n                       random_state=42)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  DecisionTreeClassifier?Documentation for DecisionTreeClassifieriFittedDecisionTreeClassifier(max_depth=3, min_samples_leaf=5, min_samples_split=10,\n                       random_state=42) \n\n# Evaluar\nprint(f\"Accuracy en entrenamiento: {model.score(pyX_train, pyy_train):.4f}\")\n\nAccuracy en entrenamiento: 0.7919\n\nprint(f\"Accuracy en prueba: {model.score(pyX_test, pyy_test):.4f}\")\n\nAccuracy en prueba: 0.7932\n\n\n# Graficamos el árbol podado\ntree.plot_tree(model)\n\n\n\n\n\n\n\n\n\n\n4.5.2 Poda Posterior (Post-Pruning) con ccp_alpha\nSe puedes realizar poda posterior usando cost complexity pruning. Esto implica ajustar el parámetro ccp_alpha (el parámetro de complejidad de coste).\nEl árbol generará múltiples subárboles podados para diferentes valores de ccp_alpha, y tú puedes elegir el más adecuado evaluando su desempeño.\n\nimport matplotlib.pyplot as plt\n\n# Crear un árbol sin poda\nmodel = DecisionTreeClassifier(random_state=1994)\nmodel.fit(pyX_train, pyy_train)\n\nDecisionTreeClassifier(random_state=1994)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  DecisionTreeClassifier?Documentation for DecisionTreeClassifieriFittedDecisionTreeClassifier(random_state=1994) \n\n# Obtener valores de ccp_alpha\npath = model.cost_complexity_pruning_path(pyX_train, pyy_train)\nccp_alphas = path.ccp_alphas\nimpurities = path.impurities\n\n# Entrenar árboles para cada valor de ccp_alpha\nmodels = []\nfor ccp_alpha in ccp_alphas:\n    clf = DecisionTreeClassifier(random_state=42, ccp_alpha=ccp_alpha)\n    clf.fit(pyX_train, pyy_train)\n    models.append(clf)\n\nDecisionTreeClassifier(ccp_alpha=0.12113075107966081, random_state=42)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  DecisionTreeClassifier?Documentation for DecisionTreeClassifieriFittedDecisionTreeClassifier(ccp_alpha=0.12113075107966081, random_state=42) \n\n# Evaluar desempeño\ntrain_scores = [clf.score(pyX_train, pyy_train) for clf in models]\ntest_scores = [clf.score(pyX_test, pyy_test) for clf in models]\n\n# Graficar resultados\nplt.figure(figsize=(8, 6))\nplt.plot(ccp_alphas, train_scores, marker='o', label=\"Train Accuracy\", drawstyle=\"steps-post\")\nplt.plot(ccp_alphas, test_scores, marker='o', label=\"Test Accuracy\", drawstyle=\"steps-post\")\nplt.xlabel(\"ccp_alpha\")\nplt.ylabel(\"Accuracy\")\nplt.title(\"Accuracy vs ccp_alpha\")\nplt.legend()\nplt.grid()\nplt.show()"
  },
  {
    "objectID": "material/trees_ensambleMethods/DecisionsTree_RandomForest_XGBoost.html#aplicación-del-modelo",
    "href": "material/trees_ensambleMethods/DecisionsTree_RandomForest_XGBoost.html#aplicación-del-modelo",
    "title": "Árboles de Decisión, Random Forest y XGBoost",
    "section": "5.1 Aplicación del modelo",
    "text": "5.1 Aplicación del modelo\n\nRPython\n\n\n\n# Random Forest \nlibrary(randomForest)\n## devtools::install_github('araastat/reprtree') # Se instala 1 vez para poder printar graficos\nlibrary(reprtree)\n\nset.seed(1994)\narbol_rf &lt;- randomForest(as.factor(RENTA) ~ .,  data = rtrain, ntree = 25)\n\n\n# se observa el árbol número 20\ntree20 &lt;- getTree(arbol_rf, 20, labelVar = TRUE)\nhead(tree20)\n\n  left daughter right daughter               split var split point status\n1             2              3         CONSTRUCTEDAREA  74.5000000      1\n2             4              5       DISTANCE_TO_METRO   0.7023313      1\n3             6              7              BATHNUMBER   2.5000000      1\n4             8              9             HASWARDROBE   1.5000000      1\n5            10             11       DISTANCE_TO_METRO   1.6344953      1\n6            12             13 DISTANCE_TO_CITY_CENTER   1.1452018      1\n  prediction\n1       &lt;NA&gt;\n2       &lt;NA&gt;\n3       &lt;NA&gt;\n4       &lt;NA&gt;\n5       &lt;NA&gt;\n6       &lt;NA&gt;\n\n## Sin embargo, el método por el que se representa gráficamente no es muy claro y\n## puede llevar a confusión o dificultar la interpretación del árbol. \n## Si se desea estudiar el árbol, hasta un cierto nivel, se puede incluir el argumento depth.\n## El árbol, ahora con una profundidad de 5 ramas.\nplot.getTree(arbol_rf, k = 20, depth = 5)\n\n\n\n\n\n\n\n\n\nlibrary(vip)\nvip(arbol_rf)\n\n\n\n\n\n\n\n\n\n\n\nfrom sklearn.ensemble import RandomForestClassifier\n\nclf = RandomForestClassifier()\nclf.fit(pyX_train, pyy_train)\n\nRandomForestClassifier()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  RandomForestClassifier?Documentation for RandomForestClassifieriFittedRandomForestClassifier() \n\n\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nresults = pd.DataFrame(clf.feature_importances_, index=pyBCN.columns[:-1]).sort_values(by=0, ascending=False)\n\n# Crear gráfico de barras horizontales\nplt.figure(figsize=(10, 8))\nplt.barh(results.index, results[0], color='skyblue')\n\n# Añadir etiquetas y título\nplt.xlabel('Importancia')\nplt.ylabel('Características')\nplt.title('Importancia de las Características')\nplt.grid(axis='x', linestyle='--', alpha=0.7)\nplt.show()"
  },
  {
    "objectID": "material/trees_ensambleMethods/DecisionsTree_RandomForest_XGBoost.html#hiperparameter-tunning-de-random-forest",
    "href": "material/trees_ensambleMethods/DecisionsTree_RandomForest_XGBoost.html#hiperparameter-tunning-de-random-forest",
    "title": "Árboles de Decisión, Random Forest y XGBoost",
    "section": "5.2 Hiperparameter tunning de Random Forest",
    "text": "5.2 Hiperparameter tunning de Random Forest\n\nRPython\n\n\n\n# Identificamos los parámetros que podemos tunnerar\nmodelLookup(\"rf\")\n\n  model parameter                         label forReg forClass probModel\n1    rf      mtry #Randomly Selected Predictors   TRUE     TRUE      TRUE\n\n\n\n# Se especifica un rango de valores posibles de mtry\ntuneGrid &lt;- expand.grid(mtry = c(1, 2, 5, 10))\ntuneGrid\n\n  mtry\n1    1\n2    2\n3    5\n4   10\n\n\n\n# se fija la semilla aleatoria\nset.seed(1994)\n\n# se entrena el modelo\nmodel &lt;- train(RENTA ~ ., data = rtrain, \n               ntree = 20,\n               method = \"rf\", metric = \"Accuracy\",\n               tuneGrid = tuneGrid,\n               trControl = trainControl(classProbs = TRUE))\n\n# Visualizamos los hiperparámetros obtenidos \nmodel$results\n\n  mtry  Accuracy     Kappa  AccuracySD     KappaSD\n1    1 0.7114013 0.4050249 0.021913133 0.053223045\n2    2 0.8424843 0.7083545 0.005692328 0.011410794\n3    5 0.9015691 0.8223990 0.003130659 0.006087154\n4   10 0.9143875 0.8460688 0.002978894 0.005415628\n\n\n\n\n\n5.2.1 Ajuste de hiperparámetros con GridSearchCV\nEl GridSearchCV realiza una búsqueda exhaustiva sobre un conjunto de parámetros especificados. Probará todas las combinaciones posibles de hiperparámetros.\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n\n# Definir los parámetros para la búsqueda\nparam_dist = {\n    'n_estimators': [150, 200],        # Número de árboles en el bosque\n    'max_depth': [None, 10, 20],            # Profundidad máxima del árbol\n    'min_samples_split': [2, 5, 10],            # Número mínimo de muestras para dividir un nodo\n    'min_samples_leaf': [1, 2, 4],               # Número mínimo de muestras en una hoja\n    'max_features': ['auto'],      # Número de características a considerar para dividir un nodo\n    'bootstrap': [True]                      # Si usar bootstrap para los árboles\n}\n\n# Crear el modelo RandomForest\nrf = RandomForestClassifier(random_state = 1994)\n\n# Usar GridSearchCV para encontrar el mejor conjunto de parámetros\ngrid_search = GridSearchCV(estimator = rf, param_grid = param_grid, cv = 10, n_jobs = -1, verbose = 0)\n\n# Ajustar el modelo con los datos de entrenamiento\ngrid_search.fit(pyX_train, pyy_train)\n\n# Mostrar los mejores parámetros encontrados\nprint(\"Mejores parámetros encontrados:\", grid_search.best_params_)\n\ntree.plot_tree(grid_search.best_estimator_)\n\n\n\n5.2.2 Ajuste de Hiperparámetros con RandomizedSearchCV\nRandomizedSearchCV es una técnica más eficiente que GridSearchCV, ya que no prueba todas las combinaciones posibles, sino un número limitado de combinaciones aleatorias dentro de un rango definido. Esto es útil si el espacio de búsqueda es grande y quieres evitar un tiempo de cómputo muy largo.\n\n# Definir los parámetros para la búsqueda aleatoria\nparam_dist = {\n    'n_estimators': [150, 200],        # Número de árboles en el bosque\n    'max_depth': [None, 10, 20],            # Profundidad máxima del árbol\n    'min_samples_split': [2, 5, 10],            # Número mínimo de muestras para dividir un nodo\n    'min_samples_leaf': [1, 2, 4],               # Número mínimo de muestras en una hoja\n    'max_features': ['auto'],      # Número de características a considerar para dividir un nodo\n    'bootstrap': [True]                      # Si usar bootstrap para los árboles\n}\n\n# Usar RandomizedSearchCV para búsqueda aleatoria\nrandom_search = RandomizedSearchCV(estimator=rf, param_distributions=param_dist, \n                                   n_iter=50, cv=10, n_jobs=-1, random_state=1994)\n\n# Ajustar el modelo con los datos de entrenamiento\nrandom_search.fit(X_train, y_train)\n\n# Mostrar los mejores parámetros encontrados\nprint(\"Mejores parámetros encontrados:\", random_search.best_params_)\n\ntree.plot_tree(random_search.best_estimator_)"
  },
  {
    "objectID": "material/trees_ensambleMethods/DecisionsTree_RandomForest_XGBoost.html#predicciones-del-algoritmo",
    "href": "material/trees_ensambleMethods/DecisionsTree_RandomForest_XGBoost.html#predicciones-del-algoritmo",
    "title": "Árboles de Decisión, Random Forest y XGBoost",
    "section": "5.3 Predicciones del algoritmo",
    "text": "5.3 Predicciones del algoritmo\n\nRPython\n\n\n\nprediccion &lt;- predict(arbol_rf, rtrain, type = \"class\")\ncaret::confusionMatrix(prediccion, as.factor(rtrain$RENTA))\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction  Alta  Baja Media\n     Alta   1931     0     3\n     Baja      0  6211     7\n     Media    10    21 10486\n\nOverall Statistics\n                                         \n               Accuracy : 0.9978         \n                 95% CI : (0.997, 0.9984)\n    No Information Rate : 0.5622         \n    P-Value [Acc &gt; NIR] : &lt; 2.2e-16      \n                                         \n                  Kappa : 0.9961         \n                                         \n Mcnemar's Test P-Value : NA             \n\nStatistics by Class:\n\n                     Class: Alta Class: Baja Class: Media\nSensitivity               0.9948      0.9966       0.9990\nSpecificity               0.9998      0.9994       0.9962\nPos Pred Value            0.9984      0.9989       0.9971\nNeg Pred Value            0.9994      0.9983       0.9988\nPrevalence                0.1040      0.3338       0.5622\nDetection Rate            0.1034      0.3327       0.5617\nDetection Prevalence      0.1036      0.3331       0.5633\nBalanced Accuracy         0.9973      0.9980       0.9976\n\n# Realizamos las predicciones de este ultimo arbol para la predicción de test\n## Si no decimos nada en type (type = prob), nos devolvera la probabilidad de \n## pertenecer a cada clase. \nprediccion &lt;- predict(arbol_rf, rtest, type = \"class\")\n## Para ver la performance, realizaremos la matriz de confusión \ncaret::confusionMatrix(prediccion, as.factor(rtest$RENTA))\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction Alta Baja Media\n     Alta   344    1    52\n     Baja     1 1382    76\n     Media  122  134  2553\n\nOverall Statistics\n                                        \n               Accuracy : 0.9173        \n                 95% CI : (0.909, 0.925)\n    No Information Rate : 0.5747        \n    P-Value [Acc &gt; NIR] : &lt; 2.2e-16     \n                                        \n                  Kappa : 0.8478        \n                                        \n Mcnemar's Test P-Value : 1.382e-09     \n\nStatistics by Class:\n\n                     Class: Alta Class: Baja Class: Media\nSensitivity              0.73662      0.9110       0.9523\nSpecificity              0.98737      0.9755       0.8710\nPos Pred Value           0.86650      0.9472       0.9089\nNeg Pred Value           0.97118      0.9579       0.9310\nPrevalence               0.10011      0.3252       0.5747\nDetection Rate           0.07374      0.2962       0.5473\nDetection Prevalence     0.08510      0.3128       0.6021\nBalanced Accuracy        0.86200      0.9433       0.9116\n\n\n\nCM &lt;- caret::confusionMatrix(prediccion, as.factor(rtest$RENTA)); CM &lt;- data.frame(CM$table)\n\ngrafico &lt;- ggplot(CM, aes(Prediction,Reference, fill= Freq)) +\n        geom_tile() + geom_text(aes(label=Freq)) +\n        scale_fill_gradient(low=\"white\", high=\"#009194\") +\n        labs(x = \"Reference\",y = \"Prediction\")\n\nplot(grafico)\n\n\n\n\n\n\n\n\n\n\n\npreds = clf.predict(pyX_test)\nprint(f'Classification Report: \\n{classification_report(pyy_test, preds)}')\n\nClassification Report: \n              precision    recall  f1-score   support\n\n           0       0.90      0.77      0.83       488\n           1       0.95      0.90      0.93      1522\n           2       0.91      0.96      0.93      2657\n\n    accuracy                           0.92      4667\n   macro avg       0.92      0.88      0.90      4667\nweighted avg       0.92      0.92      0.92      4667\n\n\n\n# Confusion matrix\ncf_matrix = confusion_matrix(pyy_test, preds)\nsns.heatmap(cf_matrix, annot=True, fmt='d', cmap='Blues', cbar=False)"
  },
  {
    "objectID": "material/trees_ensambleMethods/DecisionsTree_RandomForest_XGBoost.html#aplicamos-el-algoritmo-con-cross-validation-e-hiperparameter-tunning",
    "href": "material/trees_ensambleMethods/DecisionsTree_RandomForest_XGBoost.html#aplicamos-el-algoritmo-con-cross-validation-e-hiperparameter-tunning",
    "title": "Árboles de Decisión, Random Forest y XGBoost",
    "section": "6.1 Aplicamos el algoritmo con cross-validation e hiperparameter tunning",
    "text": "6.1 Aplicamos el algoritmo con cross-validation e hiperparameter tunning\n\nRPython\n\n\nPara aplicar los modelos de XGBoost es necesario pasar los datos categoricos en dummies. Una variable dummy (también conocida como cualitativa o binaria) es aquella que toma el valor 1 o 0 para indicar la presencia o ausencia de una cierta característica o condición.\n\n\nSi quisieramos hacerlo en cross validación hariamos lo siguiente:"
  },
  {
    "objectID": "material/trees_ensambleMethods/DecisionsTree_RandomForest_XGBoost.html#predicciones",
    "href": "material/trees_ensambleMethods/DecisionsTree_RandomForest_XGBoost.html#predicciones",
    "title": "Árboles de Decisión, Random Forest y XGBoost",
    "section": "6.2 Predicciones",
    "text": "6.2 Predicciones\n\nRPython"
  },
  {
    "objectID": "index.html#introducción-a-la-mineria-de-datos",
    "href": "index.html#introducción-a-la-mineria-de-datos",
    "title": "Métodos Estadísticos para la mineria de datos",
    "section": "1 Introducción a la mineria de datos",
    "text": "1 Introducción a la mineria de datos\nLa minería de datos es el proceso de extraer patrones, tendencias y conocimientos útiles a partir de grandes volúmenes de datos. Combina estadística, aprendizaje automático y bases de datos para ayudar a resolver problemas en diversas áreas, como negocios, ciencia y tecnología.\nTeoria\nLaboratorio"
  },
  {
    "objectID": "index.html#introducción-a-la-mineria-de-datos-1",
    "href": "index.html#introducción-a-la-mineria-de-datos-1",
    "title": "Métodos Estadísticos para la mineria de datos",
    "section": "2 [Introducción a la mineria de datos] ()",
    "text": "2 [Introducción a la mineria de datos] ()"
  },
  {
    "objectID": "index.html#introducción-a-la-mineria-de-datos-2",
    "href": "index.html#introducción-a-la-mineria-de-datos-2",
    "title": "Métodos Estadísticos para la mineria de datos",
    "section": "3 [Introducción a la mineria de datos] ()",
    "text": "3 [Introducción a la mineria de datos] ()"
  },
  {
    "objectID": "index.html#introducción-a-la-mineria-de-datos-3",
    "href": "index.html#introducción-a-la-mineria-de-datos-3",
    "title": "Métodos Estadísticos para la mineria de datos",
    "section": "4 [Introducción a la mineria de datos] ()",
    "text": "4 [Introducción a la mineria de datos] ()"
  },
  {
    "objectID": "index.html#introducción-a-la-mineria-de-datos-4",
    "href": "index.html#introducción-a-la-mineria-de-datos-4",
    "title": "Métodos Estadísticos para la mineria de datos",
    "section": "5 [Introducción a la mineria de datos] ()",
    "text": "5 [Introducción a la mineria de datos] ()"
  },
  {
    "objectID": "index.html#introducción-a-la-mineria-de-datos-5",
    "href": "index.html#introducción-a-la-mineria-de-datos-5",
    "title": "Métodos Estadísticos para la mineria de datos",
    "section": "6 [Introducción a la mineria de datos] ()",
    "text": "6 [Introducción a la mineria de datos] ()"
  },
  {
    "objectID": "index.html#introducción-a-la-mineria-de-datos-6",
    "href": "index.html#introducción-a-la-mineria-de-datos-6",
    "title": "Métodos Estadísticos para la mineria de datos",
    "section": "7 [Introducción a la mineria de datos] ()",
    "text": "7 [Introducción a la mineria de datos] ()"
  },
  {
    "objectID": "index.html#introducción-a-la-mineria-de-datos-7",
    "href": "index.html#introducción-a-la-mineria-de-datos-7",
    "title": "Métodos Estadísticos para la mineria de datos",
    "section": "8 [Introducción a la mineria de datos] ()",
    "text": "8 [Introducción a la mineria de datos] ()"
  },
  {
    "objectID": "index.html#introducción-a-la-mineria-de-datos-8",
    "href": "index.html#introducción-a-la-mineria-de-datos-8",
    "title": "Métodos Estadísticos para la mineria de datos",
    "section": "9 [Introducción a la mineria de datos] ()",
    "text": "9 [Introducción a la mineria de datos] ()"
  },
  {
    "objectID": "index.html#introducción-a-la-mineria-de-datos-9",
    "href": "index.html#introducción-a-la-mineria-de-datos-9",
    "title": "Métodos Estadísticos para la mineria de datos",
    "section": "10 [Introducción a la mineria de datos] ()",
    "text": "10 [Introducción a la mineria de datos] ()"
  },
  {
    "objectID": "index.html#introducción-a-la-mineria-de-datos-material",
    "href": "index.html#introducción-a-la-mineria-de-datos-material",
    "title": "Métodos Estadísticos para la mineria de datos",
    "section": "11 [Introducción a la mineria de datos] (material/)",
    "text": "11 [Introducción a la mineria de datos] (material/)"
  },
  {
    "objectID": "index.html#árboles-de-decisión-y-métodos-de-ensamblado",
    "href": "index.html#árboles-de-decisión-y-métodos-de-ensamblado",
    "title": "Métodos Estadísticos para la mineria de datos",
    "section": "12 Árboles de decisión y métodos de ensamblado",
    "text": "12 Árboles de decisión y métodos de ensamblado\npráctica"
  },
  {
    "objectID": "index.html#clustering",
    "href": "index.html#clustering",
    "title": "Métodos Estadísticos para la mineria de datos",
    "section": "2 Clustering",
    "text": "2 Clustering\nEl clustering agrupa datos similares en clústeres basados en características compartidas. Es útil para descubrir patrones ocultos y segmentar conjuntos de datos, comúnmente aplicado en marketing, biología y análisis de redes.\nTeoria\nLaboratorio"
  },
  {
    "objectID": "index.html#visualización-de-datos",
    "href": "index.html#visualización-de-datos",
    "title": "Métodos Estadísticos para la mineria de datos",
    "section": "3 Visualización de datos",
    "text": "3 Visualización de datos\nLa visualización de datos convierte información compleja en gráficos y representaciones visuales claras, facilitando la interpretación y comunicación de resultados. Herramientas como gráficos de dispersión, histogramas y mapas de calor son fundamentales.\n\n3.1 Analisis de componentes principales (ACP)\nEl ACP reduce la dimensionalidad de los datos al identificar las combinaciones lineales más relevantes de las variables originales, conservando la mayor parte de la variación. Se usa para simplificar datos y facilitar su interpretación.\nTeoria\nLaboratorio\n\n\n3.2 Analisis de correspondiencias múltiples (ACM)\nEl ACM analiza tablas de datos categóricos para identificar relaciones entre categorías, visualizando patrones en mapas bidimensionales que facilitan la interpretación.\nTeoria\nLaboratorio"
  },
  {
    "objectID": "index.html#reglas-de-asociación",
    "href": "index.html#reglas-de-asociación",
    "title": "Métodos Estadísticos para la mineria de datos",
    "section": "4 Reglas de asociación",
    "text": "4 Reglas de asociación\nEste método identifica relaciones significativas entre variables en grandes bases de datos. Es clave en aplicaciones como los sistemas de recomendación y análisis de cestas de mercado.\nTeoria\nLaboratorio"
  },
  {
    "objectID": "index.html#reglas-de-clasificación",
    "href": "index.html#reglas-de-clasificación",
    "title": "Métodos Estadísticos para la mineria de datos",
    "section": "5 Reglas de clasificación",
    "text": "5 Reglas de clasificación\nLos modelos de clasificación asignan datos a categorías predefinidas basándose en patrones aprendidos. Es ampliamente usado en diagnóstico médico, detección de fraudes y análisis de texto.\n\n5.1 Lineal Discriminant Analysis (LDA) y Quadratic Discriminant Analysis (QDA)\nAmbos métodos buscan separar categorías utilizando fronteras de decisión basadas en estadísticas. LDA asume varianzas iguales entre clases, mientras que QDA permite varianzas diferentes.\nTeoria\nLaboratorio\n\n\n5.2 Naives Bayes\nUn clasificador basado en probabilidad que asume independencia entre las características. Es eficiente y se aplica en problemas como clasificación de texto y detección de spam.\nTeoria\nLaboratorio"
  },
  {
    "objectID": "index.html#métodos-particionales",
    "href": "index.html#métodos-particionales",
    "title": "Métodos Estadísticos para la mineria de datos",
    "section": "6 Métodos particionales",
    "text": "6 Métodos particionales\nDividen datos en subconjuntos o particiones, a menudo mediante árboles de decisión y técnicas relacionadas.\n\n6.1 Decisions Tree\nModelo gráfico que toma decisiones en base a condiciones secuenciales. Es intuitivo y útil en clasificación y regresión.\n\n\n6.2 Random Forest\nCombina múltiples árboles de decisión para mejorar precisión y reducir sobreajuste. Es robusto y adecuado para tareas de clasificación y regresión.\n\n\n6.3 Bagging & Boosting\nMétodos de ensamblado que mejoran el rendimiento combinando múltiples modelos. Bagging reduce la variabilidad, mientras que Boosting optimiza errores iterativamente.\nTeoria\nLaboratorio"
  },
  {
    "objectID": "index.html#métodos-flexibles-de-discriminación",
    "href": "index.html#métodos-flexibles-de-discriminación",
    "title": "Métodos Estadísticos para la mineria de datos",
    "section": "7 Métodos flexibles de discriminación",
    "text": "7 Métodos flexibles de discriminación\n\n7.1 Support Vectors Machines (SVM)\nSeparan clases usando hiperplanos óptimos en un espacio de alta dimensionalidad. Son efectivas en problemas no lineales y clasificación compleja.\nTeoria\nLaboratorio"
  },
  {
    "objectID": "index.html#deep-learning",
    "href": "index.html#deep-learning",
    "title": "Métodos Estadísticos para la mineria de datos",
    "section": "8 Deep Learning",
    "text": "8 Deep Learning\nEl aprendizaje profundo utiliza redes neuronales para modelar datos complejos. Es ampliamente aplicado en reconocimiento de imágenes, procesamiento de lenguaje natural y más.\n\n8.1 Redes neuronales: Discriminación pel perceptrón multicapa\nLas redes multicapa, basadas en múltiples capas de neuronas interconectadas, resuelven problemas no lineales con alta precisión.\n\n\n8.2 Redes neuronales convolucionales\nEspecializadas en procesar datos con estructura espacial, como imágenes. Extraen automáticamente características relevantes para tareas como clasificación de imágenes y visión por computadora.\nTeoria\nDatos de deportes\nDetección de imagenes deportivas\nDreamBooth (parte 1)\nImportante: Para poder hacer uso de este script es necesario que tengas:\n\nEntre 2 y 3 fotos de cuerpo entero\nEntre 3 y 5 fotos de medio cuerpo\nEntorno a 10 fotos de cara\n\nDreamBooth (parte 2)\nDreamBooth (completo)"
  },
  {
    "objectID": "material/ANN/02_DreamBooth_parte2.html",
    "href": "material/ANN/02_DreamBooth_parte2.html",
    "title": "DREAMBOOTH 🤖",
    "section": "",
    "text": "%load_ext autoreload\n%autoreload 2\n\nThe autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload\nDreambooth es un modelo de generación de aprendizaje profundo, y que fue desarrollado en 2022 por un grupo de investigadores de Google Research y la Universidad de Boston. La misión de esta tecnología es la de poder entrenar a modelos de inteligencia artificial para personalizarlo según tus necesidades.\nEsta web está creada por Dante Conti y Sergi Ramírez, (c) 2024"
  },
  {
    "objectID": "material/ANN/02_DreamBooth_parte2.html#como-funciona",
    "href": "material/ANN/02_DreamBooth_parte2.html#como-funciona",
    "title": "DREAMBOOTH 🤖",
    "section": "¿Como funciona? 🔩",
    "text": "¿Como funciona? 🔩\nEl funcionamiento de esta técnica funciona en tres pasos.\n\nEn primer lugar, necesitas un modelo de difusión preentrenado, que es uno de esos sistemas de inteligencia artificial que pueden crear imágenes a partir de texto. Por ejemplo, se puede usar:\n\n\nStable Diffusion\nDALL-E\nMidjourney\n\nsiempre y cuando funcionen con el proceso de ruido y denoising.\nLo que hace esta técnica es crear una imagen completamente ruidosa, y luego ir quitando ese ruido reconstruyendo en el proceso una imagen totalmente original que se parezca a lo que le has pedido por texto. Pues es en este punto en el que Dreambooth ayudará con un modelo entrenado para que puedas obtener imágenes de sujetos concretos.\n\nel segundo paso, en el que necesitas un conjunto de imágenes del sujeto con el que quieres personalizar la IA. Puede ser un estilo, una cara, o lo que sea. Se recomienda tener un set de unas 8 o 10 imágenes como mínimo para poder entrenar el modelo.\n\nEntonces, lo que hace Dreambooth es utilizar este set de imágenes para entrenar al modelo de difusión, entrenar a la IA para que sepa reconocer lo que hay en ellas. Puede reconocer tu cara para luego poder dibujarla desde cero, así como un estilo o una posición.\n\nUna vez has usado Dreambooth para entrenar a la IA, este sistema usará las imágenes del sujeto como punto de partida para el proceso de crear la imagen aleatoria, permitiendo que la IA tenga más información sobre cómo es el sujeto que quieres dibujar, y que así pueda hacer imágenes que se parezcan a él.\n\n\n\n\nImagen"
  },
  {
    "objectID": "material/ANN/02_DreamBooth_parte2.html#instalació-de-paquets",
    "href": "material/ANN/02_DreamBooth_parte2.html#instalació-de-paquets",
    "title": "DREAMBOOTH 🤖",
    "section": "Instalació de paquets",
    "text": "Instalació de paquets\n\nInstal·leu el paquet de Python Py-Dreambooth tal com es mostra a continuació.\n\n\n!pip install -q py_dreambooth"
  },
  {
    "objectID": "material/ANN/02_DreamBooth_parte2.html#importa-mòduls",
    "href": "material/ANN/02_DreamBooth_parte2.html#importa-mòduls",
    "title": "DREAMBOOTH 🤖",
    "section": "Importa mòduls",
    "text": "Importa mòduls\n\nHi ha diversos tipus de classes de model, però estaràs utilitzant el model més bàsic, el model Stable Diffusion Dreambooth SDDreamboothModel, però no t’has de preocupar per això ara mateix. 🤷‍♂️\n\n\nfrom py_dreambooth.dataset import LocalDataset\nfrom py_dreambooth.model import SdDreamboothModel\nfrom py_dreambooth.predictor import LocalPredictor\nfrom py_dreambooth.trainer import LocalTrainer\nfrom py_dreambooth.utils.image_helpers import display_images\nfrom py_dreambooth.utils.prompt_helpers import make_prompt\n\nMontem la relació entre el google drive i el quadern de jupyter\n\nfrom google.colab import drive\ndrive.mount('/content/drive')\n\nDrive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True)."
  },
  {
    "objectID": "material/ANN/02_DreamBooth_parte2.html#preparem-les-dades",
    "href": "material/ANN/02_DreamBooth_parte2.html#preparem-les-dades",
    "title": "DREAMBOOTH 🤖",
    "section": "Preparem les dades 📸",
    "text": "Preparem les dades 📸\n\nDATA_DIR = \"/content/drive/MyDrive/ANN/DREAMBOOTH/data\"  # el directori amb fotos per a que el model s'entreni\nOUTPUT_DIR = \"/content/drive/MyDrive/ANN/DREAMBOOTH/models\"  # El directori on s'ubicaran els fitxers de model entrenats\n\ndataset = LocalDataset(DATA_DIR)\n\n\nMolt important! En el DATA_DIR definit anteriorment, posar les imatges (jpg o png) del subjecte que es vol entrenar.\nPer a aquesta tasca, necessitareu entre 10 i 20 solos, selfies d’alta qualitat preses amb diferents fons, il·luminació i expressions facials. Crec que es pot trobar un gran exemple al repositori de GitHub de Joe Penna.\n\n\n\n\nSamples\n\n\n\nUtilitzeu el mètode de processament d’imatges següent per retallar les imatges en un quadrat centrat a la cara. Si el subjecte que el model està tractant d’aprendre no és una persona (per exemple, un gos), estableix l’argument detect_face argumentant com a False.\n\n\ndataset = dataset.preprocess_images(detect_face=True)\n\nA total of 8 images were found.\n\n\n 38%|███▊      | 3/8 [00:00&lt;00:00,  6.17it/s]\n\n\nNo faces detected in the image '443008034_395930086752782_7217331932050061307_n.jpg'.\nNo faces detected in the image '440173417_1436258973657135_9081022692963550822_n.jpg'.\n\n\n 75%|███████▌  | 6/8 [00:00&lt;00:00,  6.41it/s]\n\n\nNo faces detected in the image '429164819_452647503759342_2302826312178258320_n.jpg'.\n\n\n100%|██████████| 8/8 [00:01&lt;00:00,  5.50it/s]\n\n\nA total of 5 images were preprocessed and stored in the path '/content/drive/MyDrive/ANN/DREAMBOOTH/data_preproc'."
  },
  {
    "objectID": "material/ANN/02_DreamBooth_parte2.html#carregar-el-model",
    "href": "material/ANN/02_DreamBooth_parte2.html#carregar-el-model",
    "title": "DREAMBOOTH 🤖",
    "section": "Carregar el model 🤖",
    "text": "Carregar el model 🤖\n\nSi reinicieu el nucli del bloc de notes i voleu tornar a carregar els models que ja heu entrenat, podeu fer-ho de la següent manera.\n\n\npredictor = LocalPredictor(model, OUTPUT_DIR)"
  },
  {
    "objectID": "material/ANN/02_DreamBooth_parte2.html#crea-imatges-com-vulgueu",
    "href": "material/ANN/02_DreamBooth_parte2.html#crea-imatges-com-vulgueu",
    "title": "DREAMBOOTH 🤖",
    "section": "Crea imatges com vulgueu! 💃",
    "text": "Crea imatges com vulgueu! 💃\n\nUtilitzeu les indicacions per crear qualsevol imatge que vulgueu. El text de l’indicatiu ha de contenir el nom de l’assumpte i el nom de la classe definits anteriorment.\nTens problemes per arribar amb un bon prompte? No et preocupis. Podeu utilitzar la funció make_prompt per a generar una sol·licitud comissariada a l’atzar. Mira això. 🙆‍♀️\nLa creació de grans imatges pren paciència. Juga amb les indicacions, però si la qualitat de la pròpia generació és problemàtica, és possible que hagis de tornar a entrenar amb millors dades i paràmetres d’entrenament més adequats.\n\n\n%%time\nprompt = f\"A photo of {SUBJECT_NAME} {CLASS_NAME} with Simpsons\"\n# prompt = next(make_prompt(SUBJECT_NAME, CLASS_NAME))\n\nprint(f\"The prompt is as follows:\\n{prompt}\")\n\nimages = predictor.predict(\n    prompt,\n    height = 768,\n    width = 512,\n    num_images_per_prompt = 5,\n)\n\ndisplay_images(images, fig_size = 10)\n\nThe prompt is as follows:\nA photo of mire person with Simpsons\n\n\n\n\n\n\n\n\n\n\n\n\nCPU times: user 45.9 s, sys: 3.72 s, total: 49.6 s\nWall time: 50.3 s"
  },
  {
    "objectID": "material/ANN/02_DreamBooth_parte2.html#bibliografia",
    "href": "material/ANN/02_DreamBooth_parte2.html#bibliografia",
    "title": "DREAMBOOTH 🤖",
    "section": "Bibliografia 💃",
    "text": "Bibliografia 💃\n\nStable Diffusion\nDreamBooth\nPy-Dreambooth"
  },
  {
    "objectID": "material/ANN/01_DreamBooth_parte1.html",
    "href": "material/ANN/01_DreamBooth_parte1.html",
    "title": "DREAMBOOTH 🤖",
    "section": "",
    "text": "%load_ext autoreload\n%autoreload 2\n\nThe autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload\nDreambooth es un modelo de generación de aprendizaje profundo, y que fue desarrollado en 2022 por un grupo de investigadores de Google Research y la Universidad de Boston. La misión de esta tecnología es la de poder entrenar a modelos de inteligencia artificial para personalizarlo según tus necesidades.\nEsta web está creada por Dante Conti y Sergi Ramírez, (c) 2024"
  },
  {
    "objectID": "material/ANN/01_DreamBooth_parte1.html#como-funciona",
    "href": "material/ANN/01_DreamBooth_parte1.html#como-funciona",
    "title": "DREAMBOOTH 🤖",
    "section": "¿Como funciona? 🔩",
    "text": "¿Como funciona? 🔩\nEl funcionamiento de esta técnica funciona en tres pasos.\n\nEn primer lugar, necesitas un modelo de difusión preentrenado, que es uno de esos sistemas de inteligencia artificial que pueden crear imágenes a partir de texto. Por ejemplo, se puede usar:\n\n\nStable Diffusion\nDALL-E\nMidjourney\n\nsiempre y cuando funcionen con el proceso de ruido y denoising.\nLo que hace esta técnica es crear una imagen completamente ruidosa, y luego ir quitando ese ruido reconstruyendo en el proceso una imagen totalmente original que se parezca a lo que le has pedido por texto. Pues es en este punto en el que Dreambooth ayudará con un modelo entrenado para que puedas obtener imágenes de sujetos concretos.\n\nel segundo paso, en el que necesitas un conjunto de imágenes del sujeto con el que quieres personalizar la IA. Puede ser un estilo, una cara, o lo que sea. Se recomienda tener un set de unas 8 o 10 imágenes como mínimo para poder entrenar el modelo.\n\nEntonces, lo que hace Dreambooth es utilizar este set de imágenes para entrenar al modelo de difusión, entrenar a la IA para que sepa reconocer lo que hay en ellas. Puede reconocer tu cara para luego poder dibujarla desde cero, así como un estilo o una posición.\n\nUna vez has usado Dreambooth para entrenar a la IA, este sistema usará las imágenes del sujeto como punto de partida para el proceso de crear la imagen aleatoria, permitiendo que la IA tenga más información sobre cómo es el sujeto que quieres dibujar, y que así pueda hacer imágenes que se parezcan a él.\n\n\n\n\nImagen"
  },
  {
    "objectID": "material/ANN/01_DreamBooth_parte1.html#instalació-de-paquets",
    "href": "material/ANN/01_DreamBooth_parte1.html#instalació-de-paquets",
    "title": "DREAMBOOTH 🤖",
    "section": "Instalació de paquets",
    "text": "Instalació de paquets\n\nInstal·leu el paquet de Python Py-Dreambooth tal com es mostra a continuació.\n\n\n!pip install -q py_dreambooth"
  },
  {
    "objectID": "material/ANN/01_DreamBooth_parte1.html#importa-mòduls",
    "href": "material/ANN/01_DreamBooth_parte1.html#importa-mòduls",
    "title": "DREAMBOOTH 🤖",
    "section": "Importa mòduls",
    "text": "Importa mòduls\n\nHi ha diversos tipus de classes de model, però estaràs utilitzant el model més bàsic, el model Stable Diffusion Dreambooth SDDreamboothModel, però no t’has de preocupar per això ara mateix. 🤷‍♂️\n\n\nfrom py_dreambooth.dataset import LocalDataset\nfrom py_dreambooth.model import SdDreamboothModel\nfrom py_dreambooth.predictor import LocalPredictor\nfrom py_dreambooth.trainer import LocalTrainer\nfrom py_dreambooth.utils.image_helpers import display_images\nfrom py_dreambooth.utils.prompt_helpers import make_prompt\n\nMontem la relació entre el google drive i el quadern de jupyter\n\nfrom google.colab import drive\ndrive.mount('/content/drive')\n\nDrive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True)."
  },
  {
    "objectID": "material/ANN/01_DreamBooth_parte1.html#preparem-les-dades",
    "href": "material/ANN/01_DreamBooth_parte1.html#preparem-les-dades",
    "title": "DREAMBOOTH 🤖",
    "section": "Preparem les dades 📸",
    "text": "Preparem les dades 📸\n\nDATA_DIR = \"/content/drive/MyDrive/ANN/DREAMBOOTH/data\"  # el directori amb fotos per a que el model s'entreni\nOUTPUT_DIR = \"/content/drive/MyDrive/ANN/DREAMBOOTH/models\"  # El directori on s'ubicaran els fitxers de model entrenats\n\ndataset = LocalDataset(DATA_DIR)\n\n\nMolt important! En el DATA_DIR definit anteriorment, posar les imatges (jpg o png) del subjecte que es vol entrenar.\nPer a aquesta tasca, necessitareu entre 10 i 20 solos, selfies d’alta qualitat preses amb diferents fons, il·luminació i expressions facials. Crec que es pot trobar un gran exemple al repositori de GitHub de Joe Penna.\n\n\n\n\nSamples\n\n\n\nUtilitzeu el mètode de processament d’imatges següent per retallar les imatges en un quadrat centrat a la cara. Si el subjecte que el model està tractant d’aprendre no és una persona (per exemple, un gos), estableix l’argument detect_face argumentant com a False.\n\n\ndataset = dataset.preprocess_images(detect_face=True)\n\nA total of 8 images were found.\n\n\n 38%|███▊      | 3/8 [00:00&lt;00:00,  6.17it/s]\n\n\nNo faces detected in the image '443008034_395930086752782_7217331932050061307_n.jpg'.\nNo faces detected in the image '440173417_1436258973657135_9081022692963550822_n.jpg'.\n\n\n 75%|███████▌  | 6/8 [00:00&lt;00:00,  6.41it/s]\n\n\nNo faces detected in the image '429164819_452647503759342_2302826312178258320_n.jpg'.\n\n\n100%|██████████| 8/8 [00:01&lt;00:00,  5.50it/s]\n\n\nA total of 5 images were preprocessed and stored in the path '/content/drive/MyDrive/ANN/DREAMBOOTH/data_preproc'."
  },
  {
    "objectID": "material/ANN/01_DreamBooth_parte1.html#entrena-el-model",
    "href": "material/ANN/01_DreamBooth_parte1.html#entrena-el-model",
    "title": "DREAMBOOTH 🤖",
    "section": "Entrena el model 🤖",
    "text": "Entrena el model 🤖\n\nAra és el moment d’entrenar el model! Digues al model el nom del subjecte al qual vols entrenar (p. ex., Joe) i la classe a la qual pertany.\nEn definir un model, un dels arguments importants és quantes iteracions entrenar, o max.train.steps. S’accepta generalment que 800 a 1200 passos són apropiats per a una persona, i 200 a 400 passos són apropiats per a un animal no humà. El valor per defecte és 100 vegades el nombre de fotos que teniu. No cal que us preocupeu per això ara mateix ,🤷‍♂️, però si no us agraden els resultats de la imatge generada a continuació, aquest és el primer paràmetre a ajustar.\n\n\nSUBJECT_NAME = \"mire\"  # The name of the subject you want to learn\nCLASS_NAME = \"person\"  # The class to which the subject you want to learn belongs\n\nmodel = SdDreamboothModel(\n    subject_name = SUBJECT_NAME,\n    class_name = CLASS_NAME,\n    max_train_steps=400,\n)\n\ntrainer = LocalTrainer(output_dir = OUTPUT_DIR)\n\n\nEl temps d’entrenament del model pot ser tan curt com unes poques desenes de minuts o com diverses hores.\n\n\n%%time\npredictor = trainer.fit(model, dataset)\n\nThe model training has begun.\n'max_train_steps' is set to 400.\n\n\n╭─────────────────────────────── Traceback (most recent call last) ────────────────────────────────╮\n│ /usr/local/lib/python3.10/dist-packages/IPython/core/magics/execution.py:1335 in time            │\n│                                                                                                  │\n│   1332 │   │   else:                                                                             │\n│   1333 │   │   │   st = clock2()                                                                 │\n│   1334 │   │   │   try:                                                                          │\n│ ❱ 1335 │   │   │   │   exec(code, glob, local_ns)                                                │\n│   1336 │   │   │   │   out=None                                                                  │\n│   1337 │   │   │   │   # multi-line %%time case                                                  │\n│   1338 │   │   │   │   if expr_val is not None:                                                  │\n│ in &lt;module&gt;:1                                                                                    │\n│                                                                                                  │\n│ /usr/local/lib/python3.10/dist-packages/py_dreambooth/trainer.py:124 in fit                      │\n│                                                                                                  │\n│   121 │   │   │   f\"The model training has begun.\\n'max_train_steps' is set to {max_train_step   │\n│   122 │   │   │   self.logger,                                                                   │\n│   123 │   │   )                                                                                  │\n│ ❱ 124 │   │   _ = subprocess.run(shlex.split(command), check=True)                               │\n│   125 │   │   log_or_print(\"The model training has ended.\", self.logger)                         │\n│   126 │   │                                                                                      │\n│   127 │   │   predictor = LocalPredictor(model, self.output_dir, self.logger)                    │\n│                                                                                                  │\n│ /usr/lib/python3.10/subprocess.py:526 in run                                                     │\n│                                                                                                  │\n│    523 │   │   │   raise                                                                         │\n│    524 │   │   retcode = process.poll()                                                          │\n│    525 │   │   if check and retcode:                                                             │\n│ ❱  526 │   │   │   raise CalledProcessError(retcode, process.args,                               │\n│    527 │   │   │   │   │   │   │   │   │    output=stdout, stderr=stderr)                        │\n│    528 │   return CompletedProcess(process.args, retcode, stdout, stderr)                        │\n│    529                                                                                           │\n╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\nCalledProcessError: Command '['accelerate', 'launch', \n'/usr/local/lib/python3.10/dist-packages/py_dreambooth/scripts/train/train_sd_dreambooth.py', \n'--pretrained_model_name_or_path', 'stabilityai/stable-diffusion-2-1-base', '--instance_data_dir', \n'/content/drive/MyDrive/ANN/DREAMBOOTH/data_preproc', '--instance_prompt', 'a photo of mire person', \n'--num_class_images', '200', '--output_dir', '/content/drive/MyDrive/ANN/DREAMBOOTH/models', '--resolution', '768',\n'--train_batch_size', '1', '--learning_rate', '2e-06', '--lr_scheduler', 'constant', '--lr_warmup_steps', '0', \n'--validation_prompt', 'a photo of mire person with Eiffel Tower in the background', '--compress_output', 'False', \n'--with_prior_preservation', 'True', '--prior_loss_weight', '1.0', '--class_prompt', 'a photo of person', \n'--train_text_encoder', 'True', '--max_train_steps', '400', '--gradient_accumulation_steps', '1', \n'--gradient_checkpointing', 'True', '--use_8bit_adam', 'True', '--enable_xformers_memory_efficient_attention', \n'True', '--mixed_precision', 'fp16', '--set_grads_to_none', 'True', '--report_to', 'tensorboard']' returned \nnon-zero exit status 1."
  },
  {
    "objectID": "material/ANN/01_DreamBooth_parte1.html#bibliografia",
    "href": "material/ANN/01_DreamBooth_parte1.html#bibliografia",
    "title": "DREAMBOOTH 🤖",
    "section": "Bibliografia 💃",
    "text": "Bibliografia 💃\n\nStable Diffusion\nDreamBooth\nPy-Dreambooth"
  },
  {
    "objectID": "material/ANN/Ejercicio_CNN_Deportes.html",
    "href": "material/ANN/Ejercicio_CNN_Deportes.html",
    "title": "Convolutional Neural Networks",
    "section": "",
    "text": "import numpy as np\nimport os\nimport re\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report\n\n\n!pip install keras\n\nRequirement already satisfied: keras in /usr/local/lib/python3.10/dist-packages (3.5.0)\nRequirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from keras) (1.4.0)\nRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from keras) (1.26.4)\nRequirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras) (13.9.4)\nRequirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras) (0.0.8)\nRequirement already satisfied: h5py in /usr/local/lib/python3.10/dist-packages (from keras) (3.12.1)\nRequirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras) (0.13.1)\nRequirement already satisfied: ml-dtypes in /usr/local/lib/python3.10/dist-packages (from keras) (0.4.1)\nRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from keras) (24.2)\nRequirement already satisfied: typing-extensions&gt;=4.5.0 in /usr/local/lib/python3.10/dist-packages (from optree-&gt;keras) (4.12.2)\nRequirement already satisfied: markdown-it-py&gt;=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich-&gt;keras) (3.0.0)\nRequirement already satisfied: pygments&lt;3.0.0,&gt;=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich-&gt;keras) (2.18.0)\nRequirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py&gt;=2.2.0-&gt;rich-&gt;keras) (0.1.2)\n\n\n\nimport keras\nfrom keras.utils import to_categorical\nfrom keras import Input, Model\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Flatten, BatchNormalization, LeakyReLU\nfrom keras.layers import Conv2D, MaxPooling2D\n\nVamos a emparejar el notebook de python con el google drive\n\nfrom google.colab import drive\ndrive.mount('/content/drive')\n\nMounted at /content/drive\n\n\n\n\nLa red toma como entrada los pixeles de una imagen. Si tenemos una imagen con apenas 28×28 pixeles de alto y ancho, eso equivale a 784 neuronas. Y eso es si sólo tenemos 1 color (escala de grises). Si tuviéramos una imagen a color, necesitaríamos 3 canales (red, green, blue) y entonces usaríamos 28x28x3 = 2352 neuronas de entrada. Esa es nuestra capa de entrada.\n\n\n\nimagen\n\n\n\n\n\ndirname = os.path.join(os.getcwd(), 'drive/MyDrive/DOCENCIA/ANN/sports')\nimgpath = dirname + os.sep\n\n\nimages = []\ndirectories = []\ndircount = []\nprevRoot=''\ncant=0\n\nprint(\"leyendo imagenes de \",imgpath)\n\nfor root, dirnames, filenames in os.walk(imgpath):\n    for filename in filenames:\n        if re.search(\"\\.(jpg|jpeg|png|bmp|tiff)$\", filename):\n            cant=cant+1\n            filepath = os.path.join(root, filename)\n            image = plt.imread(filepath)\n            images.append(image)\n            b = \"Leyendo...\" + str(cant)\n            print (b, end=\"\\r\")\n            if prevRoot !=root:\n                print(root, cant)\n                prevRoot=root\n                directories.append(root)\n                dircount.append(cant)\n                cant=0\ndircount.append(cant)\n\ndircount = dircount[1:]\ndircount[0]=dircount[0]+1\nprint('Directorios leidos:',len(directories))\nprint(\"Imagenes en cada directorio\", dircount)\nprint('suma Total de imagenes en subdirs:',sum(dircount))\n\nleyendo imagenes de  /content/drive/MyDrive/DOCENCIA/ANN/sports/\n/content/drive/MyDrive/DOCENCIA/ANN/sports/ciclismo 1\n/content/drive/MyDrive/DOCENCIA/ANN/sports/f1 1000\n/content/drive/MyDrive/DOCENCIA/ANN/sports/futbol 1000\n/content/drive/MyDrive/DOCENCIA/ANN/sports/basket 1000\n/content/drive/MyDrive/DOCENCIA/ANN/sports/tenis 1000\nDirectorios leidos: 5\nImagenes en cada directorio [1001, 1000, 1000, 1000, 999]\nsuma Total de imagenes en subdirs: 5000\nEsta web está creada por Dante Conti y Sergi Ramírez, (c) 2024"
  },
  {
    "objectID": "material/ANN/Ejercicio_CNN_Deportes.html#imagenes-y-píxeles",
    "href": "material/ANN/Ejercicio_CNN_Deportes.html#imagenes-y-píxeles",
    "title": "Convolutional Neural Networks",
    "section": "",
    "text": "La red toma como entrada los pixeles de una imagen. Si tenemos una imagen con apenas 28×28 pixeles de alto y ancho, eso equivale a 784 neuronas. Y eso es si sólo tenemos 1 color (escala de grises). Si tuviéramos una imagen a color, necesitaríamos 3 canales (red, green, blue) y entonces usaríamos 28x28x3 = 2352 neuronas de entrada. Esa es nuestra capa de entrada.\n\n\n\nimagen\n\n\n\n\n\ndirname = os.path.join(os.getcwd(), 'drive/MyDrive/DOCENCIA/ANN/sports')\nimgpath = dirname + os.sep\n\n\nimages = []\ndirectories = []\ndircount = []\nprevRoot=''\ncant=0\n\nprint(\"leyendo imagenes de \",imgpath)\n\nfor root, dirnames, filenames in os.walk(imgpath):\n    for filename in filenames:\n        if re.search(\"\\.(jpg|jpeg|png|bmp|tiff)$\", filename):\n            cant=cant+1\n            filepath = os.path.join(root, filename)\n            image = plt.imread(filepath)\n            images.append(image)\n            b = \"Leyendo...\" + str(cant)\n            print (b, end=\"\\r\")\n            if prevRoot !=root:\n                print(root, cant)\n                prevRoot=root\n                directories.append(root)\n                dircount.append(cant)\n                cant=0\ndircount.append(cant)\n\ndircount = dircount[1:]\ndircount[0]=dircount[0]+1\nprint('Directorios leidos:',len(directories))\nprint(\"Imagenes en cada directorio\", dircount)\nprint('suma Total de imagenes en subdirs:',sum(dircount))\n\nleyendo imagenes de  /content/drive/MyDrive/DOCENCIA/ANN/sports/\n/content/drive/MyDrive/DOCENCIA/ANN/sports/ciclismo 1\n/content/drive/MyDrive/DOCENCIA/ANN/sports/f1 1000\n/content/drive/MyDrive/DOCENCIA/ANN/sports/futbol 1000\n/content/drive/MyDrive/DOCENCIA/ANN/sports/basket 1000\n/content/drive/MyDrive/DOCENCIA/ANN/sports/tenis 1000\nDirectorios leidos: 5\nImagenes en cada directorio [1001, 1000, 1000, 1000, 999]\nsuma Total de imagenes en subdirs: 5000"
  },
  {
    "objectID": "material/ANN/Ejercicio_CNN_Deportes.html#hacemos-el-one-hot-encoding-para-la-red",
    "href": "material/ANN/Ejercicio_CNN_Deportes.html#hacemos-el-one-hot-encoding-para-la-red",
    "title": "Convolutional Neural Networks",
    "section": "Hacemos el One-hot Encoding para la red",
    "text": "Hacemos el One-hot Encoding para la red\n\n# Change the labels from categorical to one-hot encoding\ntrain_Y_one_hot = to_categorical(train_Y)\ntest_Y_one_hot = to_categorical(test_Y)\n\n# Display the change for category label using one-hot encoding\nprint('Original label:', train_Y[0])\nprint('After conversion to one-hot:', train_Y_one_hot[0])\n\nOriginal label: 1\nAfter conversion to one-hot: [0. 1. 0. 0. 0.]"
  },
  {
    "objectID": "material/ANN/Ejercicio_CNN_Deportes.html#conjunto-de-kernels",
    "href": "material/ANN/Ejercicio_CNN_Deportes.html#conjunto-de-kernels",
    "title": "Convolutional Neural Networks",
    "section": "Conjunto de Kernels",
    "text": "Conjunto de Kernels\nCuando generamos nuestra matriz agregada, en realidad, no aplicaremos 1 sólo kernel, si no que tendremos muchos kernel (en su conjunto se llama filtros). Por ejemplo en esta primer convolución podríamos tener 32 filtros, con lo cual realmente obtendremos 32 matrices de salida (este conjunto se conoce como feature mapping), cada una de 28x28x1 dando un total del 25.088 neuronas para nuestra PRIMER CAPA OCULTA de neuronas.\n\n\nA medida que vamos desplazando el kernel y vamos obteniendo una nueva imagen filtrada por el kernel. En esta primer convolución y siguiendo con el ejemplo anterior, es como si obtuviéramos 32 imágenes filtradas nuevas. Estas imágenes nuevas lo que están “dibujando” son ciertas características de la imagen original. Esto ayudará en el futuro a poder distinguir un objeto de otro.\n\n\n\nimagen"
  },
  {
    "objectID": "material/ANN/Ejercicio_CNN_Deportes.html#max-pooling",
    "href": "material/ANN/Ejercicio_CNN_Deportes.html#max-pooling",
    "title": "Convolutional Neural Networks",
    "section": "Max-Pooling",
    "text": "Max-Pooling\nVamos a intentar explicarlo con un ejemplo: supongamos que haremos Max-pooling de tamaño 2×2. Esto quiere decir que recorreremos cada una de nuestras 32 imágenes de características obtenidas anteriormente de 28x28px de izquierda-derecha, arriba-abajo PERO en vez de tomar de a 1 pixel, tomaremos de “2×2” (2 de alto por 2 de ancho = 4 pixeles) e iremos preservando el valor “más alto” de entre esos 4 pixeles (por eso lo de “Max”). En este caso, usando 2×2, la imagen resultante es reducida “a la mitad”y quedará de 14×14 pixeles. Luego de este proceso de subsamplig nos quedarán 32 imágenes de 14×14, pasando de haber tenido 25.088 neuronas a 6.272, son bastantes menos y -en teoría- siguen almacenando la información más importante para detectar características deseadas.\n\n\n\nImagen\n\n\nMuy bien, pues esa ha sido una primer convolución: consiste de una entrada, un conjunto de filtros, generamos un mapa de características y hacemos un subsampling. Con lo cual, en el ejemplo de imágenes de 1 sólo color tendremos:\n\n\n\nImagen\n\n\nLa primer convolución es capaz de detectar características primitivas como lineas ó curvas. A medida que hagamos más capas con las convoluciones, los mapas de características serán capaces de reconocer formas más complejas, y el conjunto total de capas de convoluciones podrá ver.\nPues ahora deberemos hacer una Segunda convolución que será:\n\n\n\nImagen\n\n\nLa 3er convolución comenzará en tamaño 7×7 pixels y luego del max-pooling quedará en 3×3 con lo cual podríamos hacer sólo 1 convolución más. En este ejemplo empezamos con una imagen de 28x28px e hicimos 3 convoluciones. Si la imagen inicial hubiese sido mayor (de 224x224px) aún hubiéramos podido seguir haciendo convoluciones.\nLlegamos a la última convolución y nos queda el desenlace…\nPara terminar, tomaremos la última capa oculta a la que hicimos subsampling, que se dice que es tridimensional por tomar la forma -en nuestro ejemplo- 3x3x128 (alto,ancho,mapas) y la aplanamos, esto es que deja de ser tridimensional, y pasa a ser una capa de neuronas tradicionales, de las que ya conocíamos. Por ejemplo, podríamos aplanar (y conectar) a una nueva capa oculta de 100 neuronas feedforward.\n\n\n\nImagen\n\n\nEntonces, a esta nueva capa oculta tradicional, le aplicamos una función llamada Softmax que conecta contra la capa de salida final que tendrá la cantidad de neuronas correspondientes con las clases que estamos clasificando. Si clasificamos perros y gatos, serán 2 neuronas. Si es el dataset Mnist numérico serán 10 neuronas de salida. Si clasificamos coches, aviones ó barcos serán 3, etc.\nLas salidas al momento del entrenamiento tendrán el formato conocido como one-hot-encoding en el que para perros y gatos sera: [1,0] y [0,1], para coches, aviones ó barcos será [1,0,0]; [0,1,0];[0,0,1].\nY la función de Softmax se encarga de pasar a probabilidad (entre 0 y 1) a las neuronas de salida. Por ejemplo una salida [0,2 0,8] nos indica 20% probabilidades de que sea perro y 80% de que sea gato."
  },
  {
    "objectID": "material/ANN/Ejercicio_CNN_Deportes.html#backpropagation",
    "href": "material/ANN/Ejercicio_CNN_Deportes.html#backpropagation",
    "title": "Convolutional Neural Networks",
    "section": "Backpropagation",
    "text": "Backpropagation\nEl proceso es similar al de las redes tradicionales en las que tenemos una entrada y una salida esperada (por eso aprendizaje supervisado) y mediante el backpropagation mejoramos el valor de los pesos de las interconexiones entre capas de neuronas y a medida que iteramos esos pesos se ajustan hasta ser óptimos.\nEn el caso de la CNN, deberemos ajustar el valor de los pesos de los distintos kernels. Esto es una gran ventaja al momento del aprendizaje pues como vimos cada kernel es de un tamaño reducido, en nuestro ejemplo en la primer convolución es de tamaño de 3×3, eso son sólo 9 parámetros que debemos ajustar en 32 filtros dan un total de 288 parámetros. En comparación con los pesos entre dos capas de neuronas “tradicionales”: una de 748 y otra de 6272 en donde están TODAS interconectarlas con TODAS y eso equivaldría a tener que entrenar y ajustar más de 4,5 millones de pesos (repito: sólo para 1 capa)."
  },
  {
    "objectID": "material/ANN/Ejercicio_CNN_Deportes.html#arquitectura-básica",
    "href": "material/ANN/Ejercicio_CNN_Deportes.html#arquitectura-básica",
    "title": "Convolutional Neural Networks",
    "section": "Arquitectura básica",
    "text": "Arquitectura básica\nResumiendo: podemos decir que los elementos que usamos para crear CNNs son:\n\nEntrada: Serán los pixeles de la imagen. Serán alto, ancho y profundidad será 1 sólo color o 3 para Red,Green,Blue.\nCapa De Convolución: procesará la salida de neuronas que están conectadas en “regiones locales” de entrada (es decir pixeles cercanos), calculando el producto escalar entre sus pesos (valor de pixel) y una pequeña región a la que están conectados en el volumen de entrada. Aquí usaremos por ejemplo 32 filtros o la cantidad que decidamos y ese será el volumen de salida.\n“CAPA RELU” aplicará la función de activación en los elementos de la matriz.\nPOOL ó SUBSAMPLING: Hará una reducción en las dimensiones alto y ancho, pero se mantiene la profundidad.\nCAPA “TRADICIONAL” red de neuronas feedforward que conectará con la última capa de subsampling y finalizará con la cantidad de neuronas que queremos clasificar."
  },
  {
    "objectID": "material/ANN/Ejercicio_CNN_Deportes.html#declaración-de-parámetros",
    "href": "material/ANN/Ejercicio_CNN_Deportes.html#declaración-de-parámetros",
    "title": "Convolutional Neural Networks",
    "section": "Declaración de parámetros",
    "text": "Declaración de parámetros\n\n#declaramos variables con los parámetros de configuración de la red\nINIT_LR = 1e-3 # Valor inicial de learning rate. El valor 1e-3 corresponde con 0.001\nepochs = 10 # Cantidad de iteraciones completas al conjunto de imagenes de entrenamiento\nbatch_size = 64 # cantidad de imágenes que se toman a la vez en memoria"
  },
  {
    "objectID": "material/ANN/Ejercicio_CNN_Deportes.html#construcción-del-modelo",
    "href": "material/ANN/Ejercicio_CNN_Deportes.html#construcción-del-modelo",
    "title": "Convolutional Neural Networks",
    "section": "Construcción del modelo",
    "text": "Construcción del modelo\n\nsport_model = Sequential()\nsport_model.add(Input(shape = (21, 28, 3)))\nsport_model.add(Conv2D(32, kernel_size = (3, 3), activation = 'linear', padding = 'same'))\nsport_model.add(LeakyReLU(negative_slope = 0.1))\nsport_model.add(MaxPooling2D((2, 2), padding = 'same'))\nsport_model.add(Dropout(0.5))\nsport_model.add(Flatten())\nsport_model.add(Dense(32, activation = 'linear'))\nsport_model.add(LeakyReLU(negative_slope = 0.1))\nsport_model.add(Dropout(0.5))\nsport_model.add(Dense(nClasses, activation = 'softmax'))\n\n\nsport_model.summary()\n\nModel: \"sequential_1\"\n\n\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n┃ Layer (type)                         ┃ Output Shape                ┃         Param # ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n│ conv2d_1 (Conv2D)                    │ (None, 21, 28, 32)          │             896 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ leaky_re_lu_2 (LeakyReLU)            │ (None, 21, 28, 32)          │               0 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ max_pooling2d_1 (MaxPooling2D)       │ (None, 11, 14, 32)          │               0 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dropout_2 (Dropout)                  │ (None, 11, 14, 32)          │               0 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ flatten_1 (Flatten)                  │ (None, 4928)                │               0 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_2 (Dense)                      │ (None, 32)                  │         157,728 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ leaky_re_lu_3 (LeakyReLU)            │ (None, 32)                  │               0 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dropout_3 (Dropout)                  │ (None, 32)                  │               0 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_3 (Dense)                      │ (None, 5)                   │             165 │\n└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n\n\n\n Total params: 158,789 (620.27 KB)\n\n\n\n Trainable params: 158,789 (620.27 KB)\n\n\n\n Non-trainable params: 0 (0.00 B)\n\n\n\n\nfrom keras.utils import plot_model\n\nplot_model(sport_model, to_file='modelo_red_neuronal.png', show_shapes = True, show_layer_names = True)\n\n# Mostrar la imagen generada\nimg = plt.imread('modelo_red_neuronal.png')\nplt.imshow(img)\nplt.axis('off')  # Opcional: desactivar los ejes\nplt.show()\n\n\n\n\n\n\n\n\n\n# from tensorflow.keras.optimizers import Adagrad\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.optimizers.schedules import ExponentialDecay\n\n# Define un programador de tasa de aprendizaje\nlr_schedule = ExponentialDecay(\n    initial_learning_rate = INIT_LR,  # Tasa de aprendizaje inicial\n    decay_steps = 100,                # Número de pasos para aplicar el decaimiento\n    decay_rate = INIT_LR / 100,       # Factor de decaimiento (0.96 es un ejemplo)\n    staircase = False                 # `False` para un decaimiento continuo\n)\n\n\n# Compilamos el modelo\nsport_model.compile(loss = \"categorical_crossentropy\",\n                    optimizer = Adam(learning_rate = lr_schedule),\n                    metrics = ['accuracy'])"
  },
  {
    "objectID": "material/ANN/Ejercicio_CNN_Deportes.html#tensorflow",
    "href": "material/ANN/Ejercicio_CNN_Deportes.html#tensorflow",
    "title": "Convolutional Neural Networks",
    "section": "TENSORFLOW",
    "text": "TENSORFLOW\n\nimport tensorflow as tf\nfrom tensorflow.keras.callbacks import TensorBoard\n\n# Definir el callback de TensorBoard\ntensorboard_callback = TensorBoard(log_dir=\"./logs\", histogram_freq=1)\n\n\n# Compilar el modelo\nsport_model.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n\n# Entrenar el modelo con el callback\nsport_train = sport_model.fit(train_X, train_label, epochs = 10,\n                              callbacks = [tensorboard_callback],\n                              validation_data = (valid_X, valid_label))\n\nEpoch 1/10\n100/100 ━━━━━━━━━━━━━━━━━━━━ 4s 24ms/step - accuracy: 0.9707 - loss: 0.0519 - val_accuracy: 0.9900 - val_loss: 0.0153\nEpoch 2/10\n100/100 ━━━━━━━━━━━━━━━━━━━━ 4s 35ms/step - accuracy: 0.9760 - loss: 0.0437 - val_accuracy: 0.9875 - val_loss: 0.0167\nEpoch 3/10\n100/100 ━━━━━━━━━━━━━━━━━━━━ 3s 31ms/step - accuracy: 0.9758 - loss: 0.0464 - val_accuracy: 0.9912 - val_loss: 0.0142\nEpoch 4/10\n100/100 ━━━━━━━━━━━━━━━━━━━━ 2s 20ms/step - accuracy: 0.9744 - loss: 0.0399 - val_accuracy: 0.9900 - val_loss: 0.0144\nEpoch 5/10\n100/100 ━━━━━━━━━━━━━━━━━━━━ 2s 22ms/step - accuracy: 0.9780 - loss: 0.0379 - val_accuracy: 0.9912 - val_loss: 0.0127\nEpoch 6/10\n100/100 ━━━━━━━━━━━━━━━━━━━━ 2s 20ms/step - accuracy: 0.9818 - loss: 0.0367 - val_accuracy: 0.9937 - val_loss: 0.0125\nEpoch 7/10\n100/100 ━━━━━━━━━━━━━━━━━━━━ 2s 20ms/step - accuracy: 0.9846 - loss: 0.0299 - val_accuracy: 0.9950 - val_loss: 0.0110\nEpoch 8/10\n100/100 ━━━━━━━━━━━━━━━━━━━━ 4s 31ms/step - accuracy: 0.9848 - loss: 0.0332 - val_accuracy: 0.9912 - val_loss: 0.0138\nEpoch 9/10\n100/100 ━━━━━━━━━━━━━━━━━━━━ 4s 20ms/step - accuracy: 0.9890 - loss: 0.0269 - val_accuracy: 0.9925 - val_loss: 0.0112\nEpoch 10/10\n100/100 ━━━━━━━━━━━━━━━━━━━━ 2s 21ms/step - accuracy: 0.9879 - loss: 0.0291 - val_accuracy: 0.9937 - val_loss: 0.0117\n\n\n\n# Crear un callback de TensorBoard\nimport datetime\nlog_dir = \"/content/logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\ntensorboard_callback = TensorBoard(log_dir=log_dir, histogram_freq=1)\n\n\n# Iniciar TensorBoard en Colab\n%load_ext tensorboard\n%tensorboard --logdir /content/logs/fit\n\nThe tensorboard extension is already loaded. To reload it, use:\n  %reload_ext tensorboard\n\n\nReusing TensorBoard on port 6006 (pid 19935), started 0:01:09 ago. (Use '!kill 19935' to kill it.)"
  },
  {
    "objectID": "docs/material/ANN/01_DreamBooth_parte1.html",
    "href": "docs/material/ANN/01_DreamBooth_parte1.html",
    "title": "DREAMBOOTH 🤖",
    "section": "",
    "text": "%load_ext autoreload\n%autoreload 2\n\nThe autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload\nDreambooth es un modelo de generación de aprendizaje profundo, y que fue desarrollado en 2022 por un grupo de investigadores de Google Research y la Universidad de Boston. La misión de esta tecnología es la de poder entrenar a modelos de inteligencia artificial para personalizarlo según tus necesidades.\nEsta web está creada por Dante Conti y Sergi Ramírez, (c) 2024"
  },
  {
    "objectID": "docs/material/ANN/01_DreamBooth_parte1.html#como-funciona",
    "href": "docs/material/ANN/01_DreamBooth_parte1.html#como-funciona",
    "title": "DREAMBOOTH 🤖",
    "section": "¿Como funciona? 🔩",
    "text": "¿Como funciona? 🔩\nEl funcionamiento de esta técnica funciona en tres pasos.\n\nEn primer lugar, necesitas un modelo de difusión preentrenado, que es uno de esos sistemas de inteligencia artificial que pueden crear imágenes a partir de texto. Por ejemplo, se puede usar:\n\n\nStable Diffusion\nDALL-E\nMidjourney\n\nsiempre y cuando funcionen con el proceso de ruido y denoising.\nLo que hace esta técnica es crear una imagen completamente ruidosa, y luego ir quitando ese ruido reconstruyendo en el proceso una imagen totalmente original que se parezca a lo que le has pedido por texto. Pues es en este punto en el que Dreambooth ayudará con un modelo entrenado para que puedas obtener imágenes de sujetos concretos.\n\nel segundo paso, en el que necesitas un conjunto de imágenes del sujeto con el que quieres personalizar la IA. Puede ser un estilo, una cara, o lo que sea. Se recomienda tener un set de unas 8 o 10 imágenes como mínimo para poder entrenar el modelo.\n\nEntonces, lo que hace Dreambooth es utilizar este set de imágenes para entrenar al modelo de difusión, entrenar a la IA para que sepa reconocer lo que hay en ellas. Puede reconocer tu cara para luego poder dibujarla desde cero, así como un estilo o una posición.\n\nUna vez has usado Dreambooth para entrenar a la IA, este sistema usará las imágenes del sujeto como punto de partida para el proceso de crear la imagen aleatoria, permitiendo que la IA tenga más información sobre cómo es el sujeto que quieres dibujar, y que así pueda hacer imágenes que se parezcan a él.\n\n\n\n\nImagen"
  },
  {
    "objectID": "docs/material/ANN/01_DreamBooth_parte1.html#instalació-de-paquets",
    "href": "docs/material/ANN/01_DreamBooth_parte1.html#instalació-de-paquets",
    "title": "DREAMBOOTH 🤖",
    "section": "Instalació de paquets",
    "text": "Instalació de paquets\n\nInstal·leu el paquet de Python Py-Dreambooth tal com es mostra a continuació.\n\n\n!pip install -q py_dreambooth"
  },
  {
    "objectID": "docs/material/ANN/01_DreamBooth_parte1.html#importa-mòduls",
    "href": "docs/material/ANN/01_DreamBooth_parte1.html#importa-mòduls",
    "title": "DREAMBOOTH 🤖",
    "section": "Importa mòduls",
    "text": "Importa mòduls\n\nHi ha diversos tipus de classes de model, però estaràs utilitzant el model més bàsic, el model Stable Diffusion Dreambooth SDDreamboothModel, però no t’has de preocupar per això ara mateix. 🤷‍♂️\n\n\nfrom py_dreambooth.dataset import LocalDataset\nfrom py_dreambooth.model import SdDreamboothModel\nfrom py_dreambooth.predictor import LocalPredictor\nfrom py_dreambooth.trainer import LocalTrainer\nfrom py_dreambooth.utils.image_helpers import display_images\nfrom py_dreambooth.utils.prompt_helpers import make_prompt\n\nMontem la relació entre el google drive i el quadern de jupyter\n\nfrom google.colab import drive\ndrive.mount('/content/drive')\n\nDrive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True)."
  },
  {
    "objectID": "docs/material/ANN/01_DreamBooth_parte1.html#preparem-les-dades",
    "href": "docs/material/ANN/01_DreamBooth_parte1.html#preparem-les-dades",
    "title": "DREAMBOOTH 🤖",
    "section": "Preparem les dades 📸",
    "text": "Preparem les dades 📸\n\nDATA_DIR = \"/content/drive/MyDrive/ANN/DREAMBOOTH/data\"  # el directori amb fotos per a que el model s'entreni\nOUTPUT_DIR = \"/content/drive/MyDrive/ANN/DREAMBOOTH/models\"  # El directori on s'ubicaran els fitxers de model entrenats\n\ndataset = LocalDataset(DATA_DIR)\n\n\nMolt important! En el DATA_DIR definit anteriorment, posar les imatges (jpg o png) del subjecte que es vol entrenar.\nPer a aquesta tasca, necessitareu entre 10 i 20 solos, selfies d’alta qualitat preses amb diferents fons, il·luminació i expressions facials. Crec que es pot trobar un gran exemple al repositori de GitHub de Joe Penna.\n\n\n\n\nSamples\n\n\n\nUtilitzeu el mètode de processament d’imatges següent per retallar les imatges en un quadrat centrat a la cara. Si el subjecte que el model està tractant d’aprendre no és una persona (per exemple, un gos), estableix l’argument detect_face argumentant com a False.\n\n\ndataset = dataset.preprocess_images(detect_face=True)\n\nA total of 8 images were found.\n\n\n 38%|███▊      | 3/8 [00:00&lt;00:00,  6.17it/s]\n\n\nNo faces detected in the image '443008034_395930086752782_7217331932050061307_n.jpg'.\nNo faces detected in the image '440173417_1436258973657135_9081022692963550822_n.jpg'.\n\n\n 75%|███████▌  | 6/8 [00:00&lt;00:00,  6.41it/s]\n\n\nNo faces detected in the image '429164819_452647503759342_2302826312178258320_n.jpg'.\n\n\n100%|██████████| 8/8 [00:01&lt;00:00,  5.50it/s]\n\n\nA total of 5 images were preprocessed and stored in the path '/content/drive/MyDrive/ANN/DREAMBOOTH/data_preproc'."
  },
  {
    "objectID": "docs/material/ANN/01_DreamBooth_parte1.html#entrena-el-model",
    "href": "docs/material/ANN/01_DreamBooth_parte1.html#entrena-el-model",
    "title": "DREAMBOOTH 🤖",
    "section": "Entrena el model 🤖",
    "text": "Entrena el model 🤖\n\nAra és el moment d’entrenar el model! Digues al model el nom del subjecte al qual vols entrenar (p. ex., Joe) i la classe a la qual pertany.\nEn definir un model, un dels arguments importants és quantes iteracions entrenar, o max.train.steps. S’accepta generalment que 800 a 1200 passos són apropiats per a una persona, i 200 a 400 passos són apropiats per a un animal no humà. El valor per defecte és 100 vegades el nombre de fotos que teniu. No cal que us preocupeu per això ara mateix ,🤷‍♂️, però si no us agraden els resultats de la imatge generada a continuació, aquest és el primer paràmetre a ajustar.\n\n\nSUBJECT_NAME = \"mire\"  # The name of the subject you want to learn\nCLASS_NAME = \"person\"  # The class to which the subject you want to learn belongs\n\nmodel = SdDreamboothModel(\n    subject_name = SUBJECT_NAME,\n    class_name = CLASS_NAME,\n    max_train_steps=400,\n)\n\ntrainer = LocalTrainer(output_dir = OUTPUT_DIR)\n\n\nEl temps d’entrenament del model pot ser tan curt com unes poques desenes de minuts o com diverses hores.\n\n\n%%time\npredictor = trainer.fit(model, dataset)\n\nThe model training has begun.\n'max_train_steps' is set to 400.\n\n\n╭─────────────────────────────── Traceback (most recent call last) ────────────────────────────────╮\n│ /usr/local/lib/python3.10/dist-packages/IPython/core/magics/execution.py:1335 in time            │\n│                                                                                                  │\n│   1332 │   │   else:                                                                             │\n│   1333 │   │   │   st = clock2()                                                                 │\n│   1334 │   │   │   try:                                                                          │\n│ ❱ 1335 │   │   │   │   exec(code, glob, local_ns)                                                │\n│   1336 │   │   │   │   out=None                                                                  │\n│   1337 │   │   │   │   # multi-line %%time case                                                  │\n│   1338 │   │   │   │   if expr_val is not None:                                                  │\n│ in &lt;module&gt;:1                                                                                    │\n│                                                                                                  │\n│ /usr/local/lib/python3.10/dist-packages/py_dreambooth/trainer.py:124 in fit                      │\n│                                                                                                  │\n│   121 │   │   │   f\"The model training has begun.\\n'max_train_steps' is set to {max_train_step   │\n│   122 │   │   │   self.logger,                                                                   │\n│   123 │   │   )                                                                                  │\n│ ❱ 124 │   │   _ = subprocess.run(shlex.split(command), check=True)                               │\n│   125 │   │   log_or_print(\"The model training has ended.\", self.logger)                         │\n│   126 │   │                                                                                      │\n│   127 │   │   predictor = LocalPredictor(model, self.output_dir, self.logger)                    │\n│                                                                                                  │\n│ /usr/lib/python3.10/subprocess.py:526 in run                                                     │\n│                                                                                                  │\n│    523 │   │   │   raise                                                                         │\n│    524 │   │   retcode = process.poll()                                                          │\n│    525 │   │   if check and retcode:                                                             │\n│ ❱  526 │   │   │   raise CalledProcessError(retcode, process.args,                               │\n│    527 │   │   │   │   │   │   │   │   │    output=stdout, stderr=stderr)                        │\n│    528 │   return CompletedProcess(process.args, retcode, stdout, stderr)                        │\n│    529                                                                                           │\n╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\nCalledProcessError: Command '['accelerate', 'launch', \n'/usr/local/lib/python3.10/dist-packages/py_dreambooth/scripts/train/train_sd_dreambooth.py', \n'--pretrained_model_name_or_path', 'stabilityai/stable-diffusion-2-1-base', '--instance_data_dir', \n'/content/drive/MyDrive/ANN/DREAMBOOTH/data_preproc', '--instance_prompt', 'a photo of mire person', \n'--num_class_images', '200', '--output_dir', '/content/drive/MyDrive/ANN/DREAMBOOTH/models', '--resolution', '768',\n'--train_batch_size', '1', '--learning_rate', '2e-06', '--lr_scheduler', 'constant', '--lr_warmup_steps', '0', \n'--validation_prompt', 'a photo of mire person with Eiffel Tower in the background', '--compress_output', 'False', \n'--with_prior_preservation', 'True', '--prior_loss_weight', '1.0', '--class_prompt', 'a photo of person', \n'--train_text_encoder', 'True', '--max_train_steps', '400', '--gradient_accumulation_steps', '1', \n'--gradient_checkpointing', 'True', '--use_8bit_adam', 'True', '--enable_xformers_memory_efficient_attention', \n'True', '--mixed_precision', 'fp16', '--set_grads_to_none', 'True', '--report_to', 'tensorboard']' returned \nnon-zero exit status 1."
  },
  {
    "objectID": "docs/material/ANN/01_DreamBooth_parte1.html#bibliografia",
    "href": "docs/material/ANN/01_DreamBooth_parte1.html#bibliografia",
    "title": "DREAMBOOTH 🤖",
    "section": "Bibliografia 💃",
    "text": "Bibliografia 💃\n\nStable Diffusion\nDreamBooth\nPy-Dreambooth"
  },
  {
    "objectID": "material/ANN/Script_dreambooth.html",
    "href": "material/ANN/Script_dreambooth.html",
    "title": "Machine Learning para Data Scientist",
    "section": "",
    "text": "Instalamos las dependencias. Es posible que nos pida reiniciar el el propio colab! Saldrá un botón de reiniciar. Una vez hecho, volveis a cargar la linia y funcionará!\n\n!pip install py-dreambooth\n\nRequirement already satisfied: py-dreambooth in /usr/local/lib/python3.10/dist-packages (0.2.8)\nRequirement already satisfied: accelerate&gt;=0.23.0 in /usr/local/lib/python3.10/dist-packages (from py-dreambooth) (1.1.1)\nRequirement already satisfied: autocrop&gt;=1.3.0 in /usr/local/lib/python3.10/dist-packages (from py-dreambooth) (1.3.0)\nRequirement already satisfied: awscli&gt;=1.29.41 in /usr/local/lib/python3.10/dist-packages (from py-dreambooth) (1.36.20)\nRequirement already satisfied: bitsandbytes&gt;=0.41.0 in /usr/local/lib/python3.10/dist-packages (from py-dreambooth) (0.45.0)\nRequirement already satisfied: diffusers&gt;=0.24.0 in /usr/local/lib/python3.10/dist-packages (from py-dreambooth) (0.31.0)\nRequirement already satisfied: matplotlib&gt;=3.7.2 in /usr/local/lib/python3.10/dist-packages (from py-dreambooth) (3.8.0)\nRequirement already satisfied: peft&gt;=0.7.1 in /usr/local/lib/python3.10/dist-packages (from py-dreambooth) (0.13.2)\nRequirement already satisfied: pillow&gt;=9.4.0 in /usr/local/lib/python3.10/dist-packages (from py-dreambooth) (11.0.0)\nRequirement already satisfied: torch==2.0.1 in /usr/local/lib/python3.10/dist-packages (from py-dreambooth) (2.0.1)\nRequirement already satisfied: torchvision&gt;=0.15.2 in /usr/local/lib/python3.10/dist-packages (from py-dreambooth) (0.15.2)\nRequirement already satisfied: sagemaker&gt;=2.183.0 in /usr/local/lib/python3.10/dist-packages (from py-dreambooth) (2.237.0)\nRequirement already satisfied: tensorboard&gt;=2.13.0 in /usr/local/lib/python3.10/dist-packages (from py-dreambooth) (2.17.1)\nRequirement already satisfied: tqdm&gt;=4.65.0 in /usr/local/lib/python3.10/dist-packages (from py-dreambooth) (4.66.6)\nRequirement already satisfied: transformers&gt;=4.33.2 in /usr/local/lib/python3.10/dist-packages (from py-dreambooth) (4.46.3)\nRequirement already satisfied: wandb&gt;=0.15.11 in /usr/local/lib/python3.10/dist-packages (from py-dreambooth) (0.18.7)\nRequirement already satisfied: xformers&gt;=0.0.20 in /usr/local/lib/python3.10/dist-packages (from py-dreambooth) (0.0.22)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1-&gt;py-dreambooth) (3.16.1)\nRequirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1-&gt;py-dreambooth) (4.12.2)\nRequirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1-&gt;py-dreambooth) (1.13.1)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1-&gt;py-dreambooth) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1-&gt;py-dreambooth) (3.1.4)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1-&gt;py-dreambooth) (11.7.99)\nRequirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1-&gt;py-dreambooth) (11.7.99)\nRequirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1-&gt;py-dreambooth) (11.7.101)\nRequirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1-&gt;py-dreambooth) (8.5.0.96)\nRequirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1-&gt;py-dreambooth) (11.10.3.66)\nRequirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1-&gt;py-dreambooth) (10.9.0.58)\nRequirement already satisfied: nvidia-curand-cu11==10.2.10.91 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1-&gt;py-dreambooth) (10.2.10.91)\nRequirement already satisfied: nvidia-cusolver-cu11==11.4.0.1 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1-&gt;py-dreambooth) (11.4.0.1)\nRequirement already satisfied: nvidia-cusparse-cu11==11.7.4.91 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1-&gt;py-dreambooth) (11.7.4.91)\nRequirement already satisfied: nvidia-nccl-cu11==2.14.3 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1-&gt;py-dreambooth) (2.14.3)\nRequirement already satisfied: nvidia-nvtx-cu11==11.7.91 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1-&gt;py-dreambooth) (11.7.91)\nRequirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1-&gt;py-dreambooth) (2.0.0)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66-&gt;torch==2.0.1-&gt;py-dreambooth) (75.1.0)\nRequirement already satisfied: wheel in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66-&gt;torch==2.0.1-&gt;py-dreambooth) (0.45.1)\nRequirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0-&gt;torch==2.0.1-&gt;py-dreambooth) (3.30.5)\nRequirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0-&gt;torch==2.0.1-&gt;py-dreambooth) (18.1.8)\nRequirement already satisfied: huggingface-hub&gt;=0.21.0 in /usr/local/lib/python3.10/dist-packages (from accelerate&gt;=0.23.0-&gt;py-dreambooth) (0.26.3)\nRequirement already satisfied: numpy&lt;3.0.0,&gt;=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate&gt;=0.23.0-&gt;py-dreambooth) (1.26.4)\nRequirement already satisfied: packaging&gt;=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate&gt;=0.23.0-&gt;py-dreambooth) (24.2)\nRequirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate&gt;=0.23.0-&gt;py-dreambooth) (5.9.5)\nRequirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate&gt;=0.23.0-&gt;py-dreambooth) (6.0.2)\nRequirement already satisfied: safetensors&gt;=0.4.3 in /usr/local/lib/python3.10/dist-packages (from accelerate&gt;=0.23.0-&gt;py-dreambooth) (0.4.5)\nRequirement already satisfied: opencv-python-headless&lt;5,&gt;=3 in /usr/local/lib/python3.10/dist-packages (from autocrop&gt;=1.3.0-&gt;py-dreambooth) (4.10.0.84)\nRequirement already satisfied: botocore==1.35.79 in /usr/local/lib/python3.10/dist-packages (from awscli&gt;=1.29.41-&gt;py-dreambooth) (1.35.79)\nRequirement already satisfied: docutils&lt;0.17,&gt;=0.10 in /usr/local/lib/python3.10/dist-packages (from awscli&gt;=1.29.41-&gt;py-dreambooth) (0.16)\nRequirement already satisfied: s3transfer&lt;0.11.0,&gt;=0.10.0 in /usr/local/lib/python3.10/dist-packages (from awscli&gt;=1.29.41-&gt;py-dreambooth) (0.10.4)\nRequirement already satisfied: colorama&lt;0.4.7,&gt;=0.2.5 in /usr/local/lib/python3.10/dist-packages (from awscli&gt;=1.29.41-&gt;py-dreambooth) (0.4.6)\nRequirement already satisfied: rsa&lt;4.8,&gt;=3.1.2 in /usr/local/lib/python3.10/dist-packages (from awscli&gt;=1.29.41-&gt;py-dreambooth) (4.7.2)\nRequirement already satisfied: jmespath&lt;2.0.0,&gt;=0.7.1 in /usr/local/lib/python3.10/dist-packages (from botocore==1.35.79-&gt;awscli&gt;=1.29.41-&gt;py-dreambooth) (1.0.1)\nRequirement already satisfied: python-dateutil&lt;3.0.0,&gt;=2.1 in /usr/local/lib/python3.10/dist-packages (from botocore==1.35.79-&gt;awscli&gt;=1.29.41-&gt;py-dreambooth) (2.8.2)\nRequirement already satisfied: urllib3!=2.2.0,&lt;3,&gt;=1.25.4 in /usr/local/lib/python3.10/dist-packages (from botocore==1.35.79-&gt;awscli&gt;=1.29.41-&gt;py-dreambooth) (2.2.3)\nRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.10/dist-packages (from diffusers&gt;=0.24.0-&gt;py-dreambooth) (6.11.0)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from diffusers&gt;=0.24.0-&gt;py-dreambooth) (2024.9.11)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from diffusers&gt;=0.24.0-&gt;py-dreambooth) (2.32.3)\nRequirement already satisfied: contourpy&gt;=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib&gt;=3.7.2-&gt;py-dreambooth) (1.3.1)\nRequirement already satisfied: cycler&gt;=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib&gt;=3.7.2-&gt;py-dreambooth) (0.12.1)\nRequirement already satisfied: fonttools&gt;=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib&gt;=3.7.2-&gt;py-dreambooth) (4.55.1)\nRequirement already satisfied: kiwisolver&gt;=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib&gt;=3.7.2-&gt;py-dreambooth) (1.4.7)\nRequirement already satisfied: pyparsing&gt;=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib&gt;=3.7.2-&gt;py-dreambooth) (3.2.0)\nRequirement already satisfied: attrs&lt;24,&gt;=23.1.0 in /usr/local/lib/python3.10/dist-packages (from sagemaker&gt;=2.183.0-&gt;py-dreambooth) (23.2.0)\nRequirement already satisfied: boto3&lt;2.0,&gt;=1.35.75 in /usr/local/lib/python3.10/dist-packages (from sagemaker&gt;=2.183.0-&gt;py-dreambooth) (1.35.79)\nRequirement already satisfied: cloudpickle==2.2.1 in /usr/local/lib/python3.10/dist-packages (from sagemaker&gt;=2.183.0-&gt;py-dreambooth) (2.2.1)\nRequirement already satisfied: docker in /usr/local/lib/python3.10/dist-packages (from sagemaker&gt;=2.183.0-&gt;py-dreambooth) (7.1.0)\nRequirement already satisfied: fastapi in /usr/local/lib/python3.10/dist-packages (from sagemaker&gt;=2.183.0-&gt;py-dreambooth) (0.115.6)\nRequirement already satisfied: google-pasta in /usr/local/lib/python3.10/dist-packages (from sagemaker&gt;=2.183.0-&gt;py-dreambooth) (0.2.0)\nRequirement already satisfied: jsonschema in /usr/local/lib/python3.10/dist-packages (from sagemaker&gt;=2.183.0-&gt;py-dreambooth) (4.23.0)\nRequirement already satisfied: omegaconf&lt;2.3,&gt;=2.2 in /usr/local/lib/python3.10/dist-packages (from sagemaker&gt;=2.183.0-&gt;py-dreambooth) (2.2.3)\nRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from sagemaker&gt;=2.183.0-&gt;py-dreambooth) (2.2.2)\nRequirement already satisfied: pathos in /usr/local/lib/python3.10/dist-packages (from sagemaker&gt;=2.183.0-&gt;py-dreambooth) (0.3.3)\nRequirement already satisfied: platformdirs in /usr/local/lib/python3.10/dist-packages (from sagemaker&gt;=2.183.0-&gt;py-dreambooth) (4.3.6)\nRequirement already satisfied: protobuf&lt;5.0,&gt;=3.12 in /usr/local/lib/python3.10/dist-packages (from sagemaker&gt;=2.183.0-&gt;py-dreambooth) (4.25.5)\nRequirement already satisfied: sagemaker-core&lt;2.0.0,&gt;=1.0.17 in /usr/local/lib/python3.10/dist-packages (from sagemaker&gt;=2.183.0-&gt;py-dreambooth) (1.0.17)\nRequirement already satisfied: schema in /usr/local/lib/python3.10/dist-packages (from sagemaker&gt;=2.183.0-&gt;py-dreambooth) (0.7.7)\nRequirement already satisfied: smdebug-rulesconfig==1.0.1 in /usr/local/lib/python3.10/dist-packages (from sagemaker&gt;=2.183.0-&gt;py-dreambooth) (1.0.1)\nRequirement already satisfied: tblib&lt;4,&gt;=1.7.0 in /usr/local/lib/python3.10/dist-packages (from sagemaker&gt;=2.183.0-&gt;py-dreambooth) (3.0.0)\nRequirement already satisfied: uvicorn in /usr/local/lib/python3.10/dist-packages (from sagemaker&gt;=2.183.0-&gt;py-dreambooth) (0.32.1)\nRequirement already satisfied: absl-py&gt;=0.4 in /usr/local/lib/python3.10/dist-packages (from tensorboard&gt;=2.13.0-&gt;py-dreambooth) (1.4.0)\nRequirement already satisfied: grpcio&gt;=1.48.2 in /usr/local/lib/python3.10/dist-packages (from tensorboard&gt;=2.13.0-&gt;py-dreambooth) (1.68.1)\nRequirement already satisfied: markdown&gt;=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard&gt;=2.13.0-&gt;py-dreambooth) (3.7)\nRequirement already satisfied: six&gt;1.9 in /usr/local/lib/python3.10/dist-packages (from tensorboard&gt;=2.13.0-&gt;py-dreambooth) (1.16.0)\nRequirement already satisfied: tensorboard-data-server&lt;0.8.0,&gt;=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard&gt;=2.13.0-&gt;py-dreambooth) (0.7.2)\nRequirement already satisfied: werkzeug&gt;=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard&gt;=2.13.0-&gt;py-dreambooth) (3.1.3)\nRequirement already satisfied: tokenizers&lt;0.21,&gt;=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers&gt;=4.33.2-&gt;py-dreambooth) (0.20.3)\nRequirement already satisfied: click!=8.0.0,&gt;=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb&gt;=0.15.11-&gt;py-dreambooth) (8.1.7)\nRequirement already satisfied: docker-pycreds&gt;=0.4.0 in /usr/local/lib/python3.10/dist-packages (from wandb&gt;=0.15.11-&gt;py-dreambooth) (0.4.0)\nRequirement already satisfied: gitpython!=3.1.29,&gt;=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb&gt;=0.15.11-&gt;py-dreambooth) (3.1.43)\nRequirement already satisfied: sentry-sdk&gt;=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb&gt;=0.15.11-&gt;py-dreambooth) (2.19.0)\nRequirement already satisfied: setproctitle in /usr/local/lib/python3.10/dist-packages (from wandb&gt;=0.15.11-&gt;py-dreambooth) (1.3.4)\nRequirement already satisfied: gitdb&lt;5,&gt;=4.0.1 in /usr/local/lib/python3.10/dist-packages (from gitpython!=3.1.29,&gt;=1.0.0-&gt;wandb&gt;=0.15.11-&gt;py-dreambooth) (4.0.11)\nRequirement already satisfied: fsspec&gt;=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub&gt;=0.21.0-&gt;accelerate&gt;=0.23.0-&gt;py-dreambooth) (2024.10.0)\nRequirement already satisfied: zipp&gt;=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata-&gt;diffusers&gt;=0.24.0-&gt;py-dreambooth) (3.21.0)\nRequirement already satisfied: antlr4-python3-runtime==4.9.* in /usr/local/lib/python3.10/dist-packages (from omegaconf&lt;2.3,&gt;=2.2-&gt;sagemaker&gt;=2.183.0-&gt;py-dreambooth) (4.9.3)\nRequirement already satisfied: charset-normalizer&lt;4,&gt;=2 in /usr/local/lib/python3.10/dist-packages (from requests-&gt;diffusers&gt;=0.24.0-&gt;py-dreambooth) (3.4.0)\nRequirement already satisfied: idna&lt;4,&gt;=2.5 in /usr/local/lib/python3.10/dist-packages (from requests-&gt;diffusers&gt;=0.24.0-&gt;py-dreambooth) (3.10)\nRequirement already satisfied: certifi&gt;=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests-&gt;diffusers&gt;=0.24.0-&gt;py-dreambooth) (2024.8.30)\nRequirement already satisfied: pyasn1&gt;=0.1.3 in /usr/local/lib/python3.10/dist-packages (from rsa&lt;4.8,&gt;=3.1.2-&gt;awscli&gt;=1.29.41-&gt;py-dreambooth) (0.6.1)\nRequirement already satisfied: pydantic&lt;3.0.0,&gt;=2.0.0 in /usr/local/lib/python3.10/dist-packages (from sagemaker-core&lt;2.0.0,&gt;=1.0.17-&gt;sagemaker&gt;=2.183.0-&gt;py-dreambooth) (2.10.3)\nRequirement already satisfied: rich&lt;14.0.0,&gt;=13.0.0 in /usr/local/lib/python3.10/dist-packages (from sagemaker-core&lt;2.0.0,&gt;=1.0.17-&gt;sagemaker&gt;=2.183.0-&gt;py-dreambooth) (13.9.4)\nRequirement already satisfied: mock&lt;5.0,&gt;4.0 in /usr/local/lib/python3.10/dist-packages (from sagemaker-core&lt;2.0.0,&gt;=1.0.17-&gt;sagemaker&gt;=2.183.0-&gt;py-dreambooth) (4.0.3)\nRequirement already satisfied: jsonschema-specifications&gt;=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema-&gt;sagemaker&gt;=2.183.0-&gt;py-dreambooth) (2024.10.1)\nRequirement already satisfied: referencing&gt;=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema-&gt;sagemaker&gt;=2.183.0-&gt;py-dreambooth) (0.35.1)\nRequirement already satisfied: rpds-py&gt;=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema-&gt;sagemaker&gt;=2.183.0-&gt;py-dreambooth) (0.22.3)\nRequirement already satisfied: MarkupSafe&gt;=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug&gt;=1.0.1-&gt;tensorboard&gt;=2.13.0-&gt;py-dreambooth) (3.0.2)\nRequirement already satisfied: starlette&lt;0.42.0,&gt;=0.40.0 in /usr/local/lib/python3.10/dist-packages (from fastapi-&gt;sagemaker&gt;=2.183.0-&gt;py-dreambooth) (0.41.3)\nRequirement already satisfied: pytz&gt;=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas-&gt;sagemaker&gt;=2.183.0-&gt;py-dreambooth) (2024.2)\nRequirement already satisfied: tzdata&gt;=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas-&gt;sagemaker&gt;=2.183.0-&gt;py-dreambooth) (2024.2)\nRequirement already satisfied: ppft&gt;=1.7.6.9 in /usr/local/lib/python3.10/dist-packages (from pathos-&gt;sagemaker&gt;=2.183.0-&gt;py-dreambooth) (1.7.6.9)\nRequirement already satisfied: dill&gt;=0.3.9 in /usr/local/lib/python3.10/dist-packages (from pathos-&gt;sagemaker&gt;=2.183.0-&gt;py-dreambooth) (0.3.9)\nRequirement already satisfied: pox&gt;=0.3.5 in /usr/local/lib/python3.10/dist-packages (from pathos-&gt;sagemaker&gt;=2.183.0-&gt;py-dreambooth) (0.3.5)\nRequirement already satisfied: multiprocess&gt;=0.70.17 in /usr/local/lib/python3.10/dist-packages (from pathos-&gt;sagemaker&gt;=2.183.0-&gt;py-dreambooth) (0.70.17)\nRequirement already satisfied: mpmath&lt;1.4,&gt;=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy-&gt;torch==2.0.1-&gt;py-dreambooth) (1.3.0)\nRequirement already satisfied: h11&gt;=0.8 in /usr/local/lib/python3.10/dist-packages (from uvicorn-&gt;sagemaker&gt;=2.183.0-&gt;py-dreambooth) (0.14.0)\nRequirement already satisfied: smmap&lt;6,&gt;=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb&lt;5,&gt;=4.0.1-&gt;gitpython!=3.1.29,&gt;=1.0.0-&gt;wandb&gt;=0.15.11-&gt;py-dreambooth) (5.0.1)\nRequirement already satisfied: annotated-types&gt;=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic&lt;3.0.0,&gt;=2.0.0-&gt;sagemaker-core&lt;2.0.0,&gt;=1.0.17-&gt;sagemaker&gt;=2.183.0-&gt;py-dreambooth) (0.7.0)\nRequirement already satisfied: pydantic-core==2.27.1 in /usr/local/lib/python3.10/dist-packages (from pydantic&lt;3.0.0,&gt;=2.0.0-&gt;sagemaker-core&lt;2.0.0,&gt;=1.0.17-&gt;sagemaker&gt;=2.183.0-&gt;py-dreambooth) (2.27.1)\nRequirement already satisfied: markdown-it-py&gt;=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich&lt;14.0.0,&gt;=13.0.0-&gt;sagemaker-core&lt;2.0.0,&gt;=1.0.17-&gt;sagemaker&gt;=2.183.0-&gt;py-dreambooth) (3.0.0)\nRequirement already satisfied: pygments&lt;3.0.0,&gt;=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich&lt;14.0.0,&gt;=13.0.0-&gt;sagemaker-core&lt;2.0.0,&gt;=1.0.17-&gt;sagemaker&gt;=2.183.0-&gt;py-dreambooth) (2.18.0)\nRequirement already satisfied: anyio&lt;5,&gt;=3.4.0 in /usr/local/lib/python3.10/dist-packages (from starlette&lt;0.42.0,&gt;=0.40.0-&gt;fastapi-&gt;sagemaker&gt;=2.183.0-&gt;py-dreambooth) (3.7.1)\nRequirement already satisfied: sniffio&gt;=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio&lt;5,&gt;=3.4.0-&gt;starlette&lt;0.42.0,&gt;=0.40.0-&gt;fastapi-&gt;sagemaker&gt;=2.183.0-&gt;py-dreambooth) (1.3.1)\nRequirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio&lt;5,&gt;=3.4.0-&gt;starlette&lt;0.42.0,&gt;=0.40.0-&gt;fastapi-&gt;sagemaker&gt;=2.183.0-&gt;py-dreambooth) (1.2.2)\nRequirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py&gt;=2.2.0-&gt;rich&lt;14.0.0,&gt;=13.0.0-&gt;sagemaker-core&lt;2.0.0,&gt;=1.0.17-&gt;sagemaker&gt;=2.183.0-&gt;py-dreambooth) (0.1.2)\n\n\nIndicamos el directorio de nuestros datos y donde queremos que se guarde nuestro modelo. Necesario crear la carpeta de las imágenes si no la tenemos creada.\n\nfrom py_dreambooth.dataset import LocalDataset\nfrom py_dreambooth.model import SdDreamboothModel\nfrom py_dreambooth.trainer import LocalTrainer\nfrom py_dreambooth.utils.image_helpers import display_images\nfrom py_dreambooth.utils.prompt_helpers import make_prompt\n\nDATA_DIR = \"data\"  # The directory where you put your prepared photos\nOUTPUT_DIR = \"models\"\n\n'dataset = LocalDataset(DATA_DIR)\\ndataset = dataset.preprocess_images(detect_face=True)\\n\\nSUBJECT_NAME = \"&lt;YOUR-NAME&gt;\"  \\nCLASS_NAME = \"person\"\\n\\nmodel = SdDreamboothModel(subject_name=SUBJECT_NAME, class_name=CLASS_NAME)\\ntrainer = LocalTrainer(output_dir=OUTPUT_DIR)\\n\\npredictor = trainer.fit(model, dataset)\\n\\n# Use the prompt helper to create an awesome AI avatar!\\nprompt = next(make_prompt(SUBJECT_NAME, CLASS_NAME))\\n\\nimages = predictor.predict(\\n    prompt, height=768, width=512, num_images_per_prompt=2,\\n)\\n\\ndisplay_images(images, fig_size=10)'\n\n\nCreamos el dataset e indicamos nuestro nombre\n\ndataset = LocalDataset(DATA_DIR)\ndataset = dataset.preprocess_images(detect_face=True)\n\nSUBJECT_NAME = \"&lt;YOUR-NAME&gt;\"  #Incluir tu nombre sin los &lt;&gt;!\nCLASS_NAME = \"person\"\n\nA total of 8 images were found.\n\n\n100%|██████████| 8/8 [00:00&lt;00:00, 10.87it/s]\n\n\nA total of 8 images were preprocessed and stored in the path 'data_preproc'.\n\n\n\n\n\nCreamos el modelo y lo entrenamos con nuestros datos\nSi se aumenta el parámetro de max_train_steps tardará más el entrenamiento (50 son 12 min aprox)\n\nmodel = SdDreamboothModel(subject_name=SUBJECT_NAME, class_name=CLASS_NAME, max_train_steps=20)\ntrainer = LocalTrainer(output_dir=OUTPUT_DIR)\n\npredictor = trainer.fit(model, dataset)\n\nThe model training has begun.\n'max_train_steps' is set to 50.\nThe model training has ended.\n\n\n\n\n\nThe model has loaded from the directory, 'models'.\n\n\nCreamos el prompt i creamos las imágenes con nuestro modelo ya entrenado\n\n#prompt = next(make_prompt(SUBJECT_NAME, CLASS_NAME)) #Es para crear un prompt random!!\n\nprompt = f\"A hyper-realistic and stunning depiction of {SUBJECT_NAME} {CLASS_NAME}, capturing the person's charisma and charm, trending on Behance, intricate textures, vivid color palette, reminiscent of Alex Ross and Norman Rockwell\"\n\nimages = predictor.predict(\n    prompt, height=768, width=512, num_images_per_prompt=2,\n)\n\ndisplay_images(images, fig_size=10)\n\nWarning: the subject and class names are not included in the prompt.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEsta web está creada por Dante Conti y Sergi Ramírez, (c) 2024"
  }
]
[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Métodos Estadísticos para la mineria de datos",
    "section": "",
    "text": "La filosofía de la minería de datos trata de la conversión de datos en conocimiento para la toma de decisiones, y como tal constituye la fase central del proceso de extracción de conocimiento a partir de bases de datos. La minería de datos es un punto de encuentro de diferentes disciplinas:\nJuntas permiten afrontar muchos problemas actuales en cuanto al tratamiento de la información.\nLa asignatura introduce las técnicas más usuales para la resolución de tres tipos de problemas fundamentales: el análisis de datos binarios (transacciones), el análisis de datos científicos (por ejemplo, de genómica) y el análisis de datos de empresas; los cuales configuran buena parte de los problemas actuales que trata la minería de datos.\nComo objetivo paralelo hay utilizar la R, un potente en torno a programación libre.\nEsta web está creada por Dante Conti y Sergi Ramírez, (c) 2025"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "material/trees_ensambleMethods/DecisionsTree_RandomForest_XGBoost.html",
    "href": "material/trees_ensambleMethods/DecisionsTree_RandomForest_XGBoost.html",
    "title": "Árboles de Decisión, Random Forest y XGBoost",
    "section": "",
    "text": "En este ejemplo se entrena un árbol de regresión para predecir el precio unitario de la vivienda en Madrid. Para ello se utilizan los datos de viviendas a la venta en Madrid publicados en Idealista durante el año 2018. Estos datos están incluidos en el paquete idealista18. Las variables que contienen nuestra base de datos son las siguientes:\n\n“ASSETID” : Identificador único del activo\n“PERIOD” : Fecha AAAAMM, indica el trimestre en el que se extrajo el anuncio, utilizamos AAAA03 para el 1.er trimestre, AAAA06 para el 2.º, AAAA09 para el 3.er y AAAA12 para el 4.º\n“PRICE” : Precio de venta del anuncio en idealista expresado en euros\n“UNITPRICE” : Precio en euros por metro cuadrado\n“CONSTRUCTEDAREA” : Superficie construida de la casa en metros cuadrados\n“ROOMNUMBER” : Número de habitaciones\n“BATHNUMBER” : Número de baños\n“HASTERRACE” : Variable ficticia para terraza (toma 1 si hay una terraza, 0 en caso contrario)\n“HASLIFT” : Variable ficticia para ascensor (toma 1 si hay ascensor en el edificio, 0 en caso contrario)\n“HASAIRCONDITIONING” : Variable ficticia para Aire Acondicionado (toma 1 si hay una Aire Acondicionado, 0 en caso contrario)\n“AMENITYID” : Indica las comodidades incluidas (1 - sin muebles, sin comodidades de cocina, 2 - comodidades de cocina, sin muebles, 3 - comodidades de cocina, muebles)\n“HASPARKINGSPACE” : Variable ficticia para estacionamiento (toma 1 si el estacionamiento está incluido en el anuncio, 0 en caso contrario)\n“ISPARKINGSPACEINCLUDEDINPRICE” : Variable ficticia para estacionamiento (toma 1 si el estacionamiento está incluido en el anuncio, 0 en caso contrario)\n“PARKINGSPACEPRICE” : Precio de plaza de parking en euros\n“HASNORTHORIENTATION” : Variable ficticia para orientación (toma 1 si la orientación es Norte en el anuncio, 0 en caso contrario) - Nota importante: las características de orientación no son características ortogonales, una casa orientada al norte también puede estar orientada al este\n“HASSOUTHORIENTATION” : Variable ficticia para orientación (toma 1 si la orientación es Sur en el anuncio, 0 en caso contrario) - Nota importante: las características de orientación no son características ortogonales, una casa orientada al norte también puede estar orientada al este\n“HASEASTORIENTATION” : Variable ficticia para orientación (toma 1 si la orientación es Este en el anuncio, 0 en caso contrario) - Nota importante: las características de orientación no son características ortogonales, una casa orientada al norte también puede estar orientada al este\n“HASWESTORIENTATION” : Variable ficticia para orientación (toma 1 si la orientación es Oeste en el anuncio, 0 en caso contrario) - Nota importante: las características de orientación no son características ortogonales, una casa orientada al norte también puede estar orientada al este\n“HASBOXROOM” : Variable ficticia para boxroom (toma 1 si boxroom está incluido en el anuncio, 0 en caso contrario)\n“HASWARDROBE” : Variable ficticia para vestuario (toma 1 si el vestuario está incluido en el anuncio, 0 en caso contrario)\n“HASSWIMMINGPOOL” : Variable ficticia para piscina (toma 1 si la piscina está incluida en el anuncio, 0 en caso contrario)\n“HASDOORMAN” : Variable ficticia para portero (toma 1 si hay un portero en el edificio, 0 en caso contrario)\n“HASGARDEN” : Variable ficticia para jardín (toma 1 si hay un jardín en el edificio, 0 en caso contrario)\n“ISDUPLEX” : Variable ficticia para dúplex (toma 1 si es un dúplex, 0 en caso contrario)\n“ISSTUDIO” : Variable ficticia para piso de soltero (estudio en español) (toma 1 si es un piso para una sola persona, 0 en caso contrario)\n“ISINTOPFLOOR” : Variable ficticia que indica si el apartamento está ubicado en el piso superior (toma 1 en el piso superior, 0 en caso contrario)\n“CONSTRUCTIONYEAR” : Año de construcción (fuente: anunciante)\n“FLOORCLEAN” : Indica el número de piso del apartamento comenzando desde el valor 0 para la planta baja (fuente: anunciante)\n“FLATLOCATIONID” : Indica el tipo de vistas que tiene el piso (1 - exterior, 2 - interior)\n“CADCONSTRUCTIONYEAR” : Año de construcción según fuente catastral (fuente: catastro), tenga en cuenta que esta cifra puede diferir de la proporcionada por el anunciante\n“CADMAXBUILDINGFLOOR” : Superficie máxima del edificio (fuente: catastro)\n“CADDWELLINGCOUNT” : Recuento de viviendas en el edificio (fuente: catastro)\n“CADASTRALQUALITYID” : Calidad catastral (fuente: catastro)\n“BUILTTYPEID_1” : Valor ficticio para estado del piso: 1 obra nueva 0 en caso contrario (fuente: anunciante)\n“BUILTTYPEID_2” : Valor ficticio para condición plana: 1 segundero a restaurar 0 en caso contrario (fuente: anunciante)\n“BUILTTYPEID_3” : Valor ficticio para estado plano: 1 de segunda mano en buen estado 0 en caso contrario (fuente: anunciante)\n“DISTANCE_TO_CITY_CENTER” : Distancia al centro de la ciudad en km\n“DISTANCE_TO_METRO” : Distancia istancia a una parada de metro en km.\n“DISTANCE_TO_DIAGONAL” : Distancia a la Avenida Diagonal en km; Diagonal es una calle principal que corta la ciudad en diagonal a la cuadrícula de calles.\n“LONGITUDE” : Longitud del activo\n“LATITUDE” : Latitud del activo\n“geometry” : Geometría de características simples en latitud y longitud.\n\nFuente: Idealista\n\nlibrary(\"idealista18\")\nBCN &lt;- get(data(\"Barcelona_Sale\"))\n\n\n# Filtramos la epoca a Navidad\nBCN &lt;- BCN[which(BCN$PERIOD == \"201812\"), ]\n\npisos_sf_BCN &lt;- st_as_sf(BCN, coords = c(\"LONGITUDE\", \"LATITUDE\"), crs = 4326)\n\n# Leer shapefile de secciones censales\nsecciones &lt;- st_read(\"C:/Users/sergi/Downloads/Shapefile/seccionado_2024/SECC_CE_20240101.shp\")\n\n# Transformar pisos al sistema de referencia de las secciones censales\npisos_sf_BCN &lt;- st_transform(pisos_sf_BCN, crs = st_crs(secciones))\n\n# Hacer el match entre pisos y secciones censales\npisos_con_seccion &lt;- st_join(pisos_sf_BCN, secciones, join = st_within)\n\n# Convertir a dataframe para exportar\nBCN &lt;- as.data.frame(pisos_con_seccion)\n\nrm(Barcelona_Sale, Barcelona_Polygons, Barcelona_POIS, pisos_con_seccion, pisos_sf_BCN, secciones); gc()\n\n\nrentaMedia &lt;- read.csv(\"https://raw.githubusercontent.com/miguel-angel-monjas/spain-datasets/refs/heads/master/data/Renta%20media%20en%20Espa%C3%B1a.csv\")\n# NOs quedamos con los datos que nos interesa de Barcelona\nrentaMedia &lt;- rentaMedia[which(rentaMedia$Provincia == \"Barcelona\" & rentaMedia$Tipo.de.elemento == \"sección\"), ]\nrentaMedia$Código.de.territorio &lt;- paste0(\"0\", rentaMedia$Código.de.territorio)\n\n\ncols &lt;- c(\"Renta.media.por.persona\", \"Renta.media.por.hogar\")\n\nm &lt;- match(BCN$CUSEC, rentaMedia$Código.de.territorio)\nBCN[, cols] &lt;- rentaMedia[m, cols]\nEsta web está creada por Dante Conti y Sergi Ramírez, (c) 2024"
  },
  {
    "objectID": "material/trees_ensambleMethods/DecisionsTree_RandomForest_XGBoost.html#análisi-descriptivo-de-los-datos",
    "href": "material/trees_ensambleMethods/DecisionsTree_RandomForest_XGBoost.html#análisi-descriptivo-de-los-datos",
    "title": "Árboles de Decisión, Random Forest y XGBoost",
    "section": "2.1 Análisi descriptivo de los datos",
    "text": "2.1 Análisi descriptivo de los datos\n\n## Descriptiva de los datos\nlibrary(DataExplorer)\nlibrary(lubridate)\nlibrary(dplyr)\n\n## Data Manipulation\nlibrary(reshape2)\n\n## Plotting\nlibrary(ggplot2)\n\n## Descripción completa\nDataExplorer::introduce(BCN)\n\n   rows columns discrete_columns continuous_columns all_missing_columns\n1 23334      36               24                 12                   0\n  total_missing_values complete_rows total_observations memory_usage\n1                    0         23334             840024      6078888\n\n## Descripción de la bbdd\nplot_intro(BCN)\n\n\n\n\n\n\n\n## Descripción de los missings\nplot_missing(BCN)\n\n\n\n\n\n\n\n## Descripción de las varaibles categoricas\nplot_bar(BCN)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n## Descripción variables numéricas\nplot_histogram(BCN)\n\n\n\n\n\n\n\nplot_density(BCN)\n\n\n\n\n\n\n\nplot_qq(BCN)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nplot_correlation(BCN)"
  },
  {
    "objectID": "material/trees_ensambleMethods/DecisionsTree_RandomForest_XGBoost.html#creación-del-árbol",
    "href": "material/trees_ensambleMethods/DecisionsTree_RandomForest_XGBoost.html#creación-del-árbol",
    "title": "Árboles de Decisión, Random Forest y XGBoost",
    "section": "4.1 Creación del árbol",
    "text": "4.1 Creación del árbol\n\nRPython\n\n\n\nlibrary(rpart)\nlibrary(rpart.plot)\n\nset.seed(1994)\n\narbol &lt;- rpart(RENTA ~ ., data = rtrain)\nsummary(arbol)\n\nCall:\nrpart(formula = RENTA ~ ., data = rtrain)\n  n= 18669 \n\n          CP nsplit rel error    xerror        xstd\n1 0.38211183      0 1.0000000 1.0000000 0.008293935\n2 0.06753946      1 0.6178882 0.6178882 0.007426365\n3 0.04447571      2 0.5503487 0.5503487 0.007149373\n4 0.02214609      4 0.4613973 0.4643338 0.006727872\n5 0.01884253      5 0.4392512 0.4425548 0.006607381\n6 0.01186835      6 0.4204087 0.4267711 0.006516230\n7 0.01162364      8 0.3966720 0.4034014 0.006375036\n8 0.01015539     10 0.3734247 0.3792977 0.006221129\n9 0.01000000     12 0.3531139 0.3676740 0.006143722\n\nVariable importance\n                   CDIS DISTANCE_TO_CITY_CENTER    DISTANCE_TO_DIAGONAL \n                     40                      24                      17 \n    CADCONSTRUCTIONYEAR         CONSTRUCTEDAREA               UNITPRICE \n                      8                       3                       3 \n      DISTANCE_TO_METRO              BATHNUMBER              ROOMNUMBER \n                      2                       1                       1 \n\nNode number 1: 18669 observations,    complexity param=0.3821118\n  predicted class=Media  expected loss=0.4377846  P(node) =1\n    class counts:  1941  6232 10496\n   probabilities: 0.104 0.334 0.562 \n  left son=2 (4541 obs) right son=3 (14128 obs)\n  Primary splits:\n      CDIS                    splits as  LRRRRLRRRR, improve=2615.4350, (0 missing)\n      DISTANCE_TO_DIAGONAL    &lt; 1.697581   to the right, improve=1710.4180, (0 missing)\n      HASLIFT                 splits as  LR, improve= 961.7684, (0 missing)\n      DISTANCE_TO_CITY_CENTER &lt; 1.146692   to the left,  improve= 885.2803, (0 missing)\n      CADCONSTRUCTIONYEAR     &lt; 1900.5     to the left,  improve= 800.8061, (0 missing)\n  Surrogate splits:\n      DISTANCE_TO_CITY_CENTER &lt; 1.146692   to the left,  agree=0.853, adj=0.396, (0 split)\n      CADCONSTRUCTIONYEAR     &lt; 1900.5     to the left,  agree=0.823, adj=0.273, (0 split)\n      DISTANCE_TO_DIAGONAL    &lt; 4.28574    to the right, agree=0.775, adj=0.076, (0 split)\n      CONSTRUCTEDAREA         &lt; 46.5       to the left,  agree=0.769, adj=0.052, (0 split)\n      ROOMNUMBER              &lt; 11         to the right, agree=0.757, adj=0.001, (0 split)\n\nNode number 2: 4541 observations,    complexity param=0.02214609\n  predicted class=Baja   expected loss=0.156133  P(node) =0.2432375\n    class counts:     0  3832   709\n   probabilities: 0.000 0.844 0.156 \n  left son=4 (4294 obs) right son=5 (247 obs)\n  Primary splits:\n      DISTANCE_TO_CITY_CENTER &lt; 0.4629643  to the right, improve=263.54560, (0 missing)\n      DISTANCE_TO_DIAGONAL    &lt; 1.293729   to the right, improve= 85.60347, (0 missing)\n      CDIS                    splits as  L----R----, improve= 66.85504, (0 missing)\n      HASLIFT                 splits as  LR, improve= 65.14028, (0 missing)\n      CADCONSTRUCTIONYEAR     &lt; 1950.5     to the left,  improve= 50.10187, (0 missing)\n\nNode number 3: 14128 observations,    complexity param=0.06753946\n  predicted class=Media  expected loss=0.3072622  P(node) =0.7567625\n    class counts:  1941  2400  9787\n   probabilities: 0.137 0.170 0.693 \n  left son=6 (1990 obs) right son=7 (12138 obs)\n  Primary splits:\n      CDIS                 splits as  -RRRL-RRRL, improve=903.3696, (0 missing)\n      DISTANCE_TO_DIAGONAL &lt; 2.370657   to the right, improve=788.5626, (0 missing)\n      CONSTRUCTEDAREA      &lt; 137.5      to the right, improve=434.3222, (0 missing)\n      UNITPRICE            &lt; 2898.227   to the left,  improve=369.9583, (0 missing)\n      BATHNUMBER           &lt; 2.5        to the right, improve=353.8536, (0 missing)\n  Surrogate splits:\n      CONSTRUCTEDAREA   &lt; 223.5      to the right, agree=0.866, adj=0.052, (0 split)\n      BATHNUMBER        &lt; 3.5        to the right, agree=0.864, adj=0.038, (0 split)\n      DISTANCE_TO_METRO &lt; 1.634495   to the right, agree=0.860, adj=0.006, (0 split)\n      ROOMNUMBER        &lt; 6.5        to the right, agree=0.859, adj=0.003, (0 split)\n      UNITPRICE         &lt; 6992.805   to the right, agree=0.859, adj=0.001, (0 split)\n\nNode number 4: 4294 observations,    complexity param=0.01015539\n  predicted class=Baja   expected loss=0.1152771  P(node) =0.230007\n    class counts:     0  3799   495\n   probabilities: 0.000 0.885 0.115 \n  left son=8 (3278 obs) right son=9 (1016 obs)\n  Primary splits:\n      DISTANCE_TO_CITY_CENTER &lt; 3.316192   to the left,  improve=107.18450, (0 missing)\n      CDIS                    splits as  L----R----, improve=107.18450, (0 missing)\n      DISTANCE_TO_DIAGONAL    &lt; 2.818623   to the left,  improve= 91.99937, (0 missing)\n      CADCONSTRUCTIONYEAR     &lt; 1950.5     to the left,  improve= 66.10102, (0 missing)\n      HASLIFT                 splits as  LR, improve= 49.70642, (0 missing)\n  Surrogate splits:\n      DISTANCE_TO_DIAGONAL &lt; 2.826385   to the left,  agree=0.989, adj=0.955, (0 split)\n      CADCONSTRUCTIONYEAR  &lt; 1950.5     to the left,  agree=0.886, adj=0.517, (0 split)\n      UNITPRICE            &lt; 2840.561   to the right, agree=0.876, adj=0.475, (0 split)\n      CADDWELLINGCOUNT     &lt; 30.5       to the left,  agree=0.789, adj=0.106, (0 split)\n      CADMAXBUILDINGFLOOR  &lt; 7.5        to the left,  agree=0.776, adj=0.053, (0 split)\n\nNode number 5: 247 observations\n  predicted class=Media  expected loss=0.1336032  P(node) =0.01323049\n    class counts:     0    33   214\n   probabilities: 0.000 0.134 0.866 \n\nNode number 6: 1990 observations,    complexity param=0.01162364\n  predicted class=Alta   expected loss=0.3613065  P(node) =0.1065938\n    class counts:  1271     0   719\n   probabilities: 0.639 0.000 0.361 \n  left son=12 (1077 obs) right son=13 (913 obs)\n  Primary splits:\n      CONSTRUCTEDAREA      &lt; 119.5      to the right, improve=121.31850, (0 missing)\n      BATHNUMBER           &lt; 2.5        to the right, improve= 87.03903, (0 missing)\n      DISTANCE_TO_DIAGONAL &lt; 0.5534786  to the left,  improve= 85.36516, (0 missing)\n      CDIS                 splits as  ----R----L, improve= 68.18003, (0 missing)\n      UNITPRICE            &lt; 5287.749   to the right, improve= 60.24694, (0 missing)\n  Surrogate splits:\n      ROOMNUMBER                    &lt; 3.5        to the right, agree=0.790, adj=0.542, (0 split)\n      BATHNUMBER                    &lt; 1.5        to the right, agree=0.747, adj=0.448, (0 split)\n      HASTERRACE                    splits as  RL, agree=0.647, adj=0.230, (0 split)\n      HASPARKINGSPACE               splits as  RL, agree=0.641, adj=0.217, (0 split)\n      ISPARKINGSPACEINCLUDEDINPRICE splits as  RL, agree=0.641, adj=0.217, (0 split)\n\nNode number 7: 12138 observations,    complexity param=0.04447571\n  predicted class=Media  expected loss=0.2529247  P(node) =0.6501687\n    class counts:   670  2400  9068\n   probabilities: 0.055 0.198 0.747 \n  left son=14 (2820 obs) right son=15 (9318 obs)\n  Primary splits:\n      DISTANCE_TO_DIAGONAL    &lt; 2.370657   to the right, improve=832.9688, (0 missing)\n      CDIS                    splits as  -RRL--RLL-, improve=615.9082, (0 missing)\n      UNITPRICE               &lt; 2898.227   to the left,  improve=380.3717, (0 missing)\n      DISTANCE_TO_CITY_CENTER &lt; 3.940257   to the right, improve=338.4368, (0 missing)\n      CONSTRUCTEDAREA         &lt; 80.5       to the left,  improve=178.0100, (0 missing)\n  Surrogate splits:\n      DISTANCE_TO_CITY_CENTER &lt; 4.058752   to the right, agree=0.897, adj=0.558, (0 split)\n      CDIS                    splits as  -RRL--LRR-, agree=0.816, adj=0.206, (0 split)\n      UNITPRICE               &lt; 2753.972   to the left,  agree=0.811, adj=0.184, (0 split)\n      DISTANCE_TO_METRO       &lt; 0.7218963  to the right, agree=0.780, adj=0.051, (0 split)\n      CADMAXBUILDINGFLOOR     &lt; 26.5       to the right, agree=0.769, adj=0.005, (0 split)\n\nNode number 8: 3278 observations\n  predicted class=Baja   expected loss=0.05308115  P(node) =0.1755852\n    class counts:     0  3104   174\n   probabilities: 0.000 0.947 0.053 \n\nNode number 9: 1016 observations,    complexity param=0.01015539\n  predicted class=Baja   expected loss=0.3159449  P(node) =0.05442177\n    class counts:     0   695   321\n   probabilities: 0.000 0.684 0.316 \n  left son=18 (712 obs) right son=19 (304 obs)\n  Primary splits:\n      DISTANCE_TO_DIAGONAL    &lt; 3.420441   to the right, improve=181.26100, (0 missing)\n      DISTANCE_TO_CITY_CENTER &lt; 4.8151     to the right, improve=162.58570, (0 missing)\n      UNITPRICE               &lt; 2586.19    to the left,  improve= 62.53053, (0 missing)\n      HASLIFT                 splits as  LR, improve= 54.20484, (0 missing)\n      CONSTRUCTEDAREA         &lt; 74.5       to the left,  improve= 31.09944, (0 missing)\n  Surrogate splits:\n      DISTANCE_TO_CITY_CENTER &lt; 5.091757   to the right, agree=0.958, adj=0.859, (0 split)\n      UNITPRICE               &lt; 3251.667   to the left,  agree=0.775, adj=0.247, (0 split)\n      CADCONSTRUCTIONYEAR     &lt; 1954.5     to the right, agree=0.708, adj=0.023, (0 split)\n      BUILTTYPEID_1           splits as  LR, agree=0.707, adj=0.020, (0 split)\n      DISTANCE_TO_METRO       &lt; 0.02808259 to the right, agree=0.707, adj=0.020, (0 split)\n\nNode number 12: 1077 observations\n  predicted class=Alta   expected loss=0.2005571  P(node) =0.05768922\n    class counts:   861     0   216\n   probabilities: 0.799 0.000 0.201 \n\nNode number 13: 913 observations,    complexity param=0.01162364\n  predicted class=Media  expected loss=0.449069  P(node) =0.0489046\n    class counts:   410     0   503\n   probabilities: 0.449 0.000 0.551 \n  left son=26 (495 obs) right son=27 (418 obs)\n  Primary splits:\n      CDIS                    splits as  ----R----L, improve=47.94928, (0 missing)\n      DISTANCE_TO_DIAGONAL    &lt; 0.5540544  to the left,  improve=44.18423, (0 missing)\n      UNITPRICE               &lt; 5289.352   to the right, improve=37.28880, (0 missing)\n      DISTANCE_TO_METRO       &lt; 0.2951583  to the right, improve=24.68798, (0 missing)\n      DISTANCE_TO_CITY_CENTER &lt; 2.14678    to the left,  improve=21.64891, (0 missing)\n  Surrogate splits:\n      CADDWELLINGCOUNT        &lt; 30.5       to the left,  agree=0.729, adj=0.409, (0 split)\n      DISTANCE_TO_DIAGONAL    &lt; 1.062684   to the right, agree=0.705, adj=0.356, (0 split)\n      UNITPRICE               &lt; 4821.591   to the right, agree=0.677, adj=0.294, (0 split)\n      CADMAXBUILDINGFLOOR     &lt; 8.5        to the left,  agree=0.662, adj=0.261, (0 split)\n      DISTANCE_TO_CITY_CENTER &lt; 3.575557   to the left,  agree=0.623, adj=0.177, (0 split)\n\nNode number 14: 2820 observations,    complexity param=0.04447571\n  predicted class=Baja   expected loss=0.4553191  P(node) =0.1510525\n    class counts:    95  1536  1189\n   probabilities: 0.034 0.545 0.422 \n  left son=28 (2010 obs) right son=29 (810 obs)\n  Primary splits:\n      CDIS                    splits as  -RRL--RLL-, improve=202.04190, (0 missing)\n      DISTANCE_TO_CITY_CENTER &lt; 2.380396   to the left,  improve=170.89780, (0 missing)\n      DISTANCE_TO_DIAGONAL    &lt; 3.446593   to the left,  improve=103.80040, (0 missing)\n      CADCONSTRUCTIONYEAR     &lt; 1971.5     to the left,  improve= 77.57975, (0 missing)\n      CONSTRUCTEDAREA         &lt; 86.5       to the left,  improve= 62.41583, (0 missing)\n  Surrogate splits:\n      DISTANCE_TO_CITY_CENTER &lt; 5.730574   to the left,  agree=0.819, adj=0.370, (0 split)\n      DISTANCE_TO_DIAGONAL    &lt; 3.716013   to the left,  agree=0.759, adj=0.160, (0 split)\n      CONSTRUCTEDAREA         &lt; 130.5      to the left,  agree=0.716, adj=0.012, (0 split)\n      BATHNUMBER              &lt; 2.5        to the left,  agree=0.715, adj=0.006, (0 split)\n      DISTANCE_TO_METRO       &lt; 0.01826087 to the right, agree=0.714, adj=0.005, (0 split)\n\nNode number 15: 9318 observations,    complexity param=0.01186835\n  predicted class=Media  expected loss=0.1544323  P(node) =0.4991162\n    class counts:   575   864  7879\n   probabilities: 0.062 0.093 0.846 \n  left son=30 (523 obs) right son=31 (8795 obs)\n  Primary splits:\n      DISTANCE_TO_CITY_CENTER &lt; 0.871618   to the left,  improve=157.24670, (0 missing)\n      CDIS                    splits as  -RRL--RLL-, improve=145.36290, (0 missing)\n      CADDWELLINGCOUNT        &lt; 167.5      to the right, improve= 73.55115, (0 missing)\n      CONSTRUCTEDAREA         &lt; 135.5      to the right, improve= 56.59289, (0 missing)\n      DISTANCE_TO_DIAGONAL    &lt; 1.820647   to the right, improve= 55.50302, (0 missing)\n  Surrogate splits:\n      BATHNUMBER &lt; 0.5        to the left,  agree=0.944, adj=0.002, (0 split)\n\nNode number 18: 712 observations\n  predicted class=Baja   expected loss=0.1207865  P(node) =0.03813809\n    class counts:     0   626    86\n   probabilities: 0.000 0.879 0.121 \n\nNode number 19: 304 observations\n  predicted class=Media  expected loss=0.2269737  P(node) =0.01628368\n    class counts:     0    69   235\n   probabilities: 0.000 0.227 0.773 \n\nNode number 26: 495 observations\n  predicted class=Alta   expected loss=0.4020202  P(node) =0.02651454\n    class counts:   296     0   199\n   probabilities: 0.598 0.000 0.402 \n\nNode number 27: 418 observations\n  predicted class=Media  expected loss=0.2727273  P(node) =0.02239006\n    class counts:   114     0   304\n   probabilities: 0.273 0.000 0.727 \n\nNode number 28: 2010 observations,    complexity param=0.01884253\n  predicted class=Baja   expected loss=0.3278607  P(node) =0.1076651\n    class counts:    35  1351   624\n   probabilities: 0.017 0.672 0.310 \n  left son=56 (1748 obs) right son=57 (262 obs)\n  Primary splits:\n      DISTANCE_TO_DIAGONAL    &lt; 3.446593   to the left,  improve=135.93570, (0 missing)\n      DISTANCE_TO_CITY_CENTER &lt; 2.380396   to the left,  improve=112.73140, (0 missing)\n      DISTANCE_TO_METRO       &lt; 0.3243732  to the left,  improve= 83.82446, (0 missing)\n      CADCONSTRUCTIONYEAR     &lt; 1967.5     to the left,  improve= 70.86276, (0 missing)\n      CADMAXBUILDINGFLOOR     &lt; 7.5        to the left,  improve= 37.15447, (0 missing)\n  Surrogate splits:\n      DISTANCE_TO_CITY_CENTER &lt; 5.208402   to the left,  agree=0.949, adj=0.611, (0 split)\n      DISTANCE_TO_METRO       &lt; 0.7342067  to the left,  agree=0.904, adj=0.267, (0 split)\n      PARKINGSPACEPRICE       &lt; 28451      to the left,  agree=0.872, adj=0.015, (0 split)\n      CADMAXBUILDINGFLOOR     &lt; 1.5        to the right, agree=0.872, adj=0.015, (0 split)\n\nNode number 29: 810 observations\n  predicted class=Media  expected loss=0.3024691  P(node) =0.04338743\n    class counts:    60   185   565\n   probabilities: 0.074 0.228 0.698 \n\nNode number 30: 523 observations,    complexity param=0.01186835\n  predicted class=Media  expected loss=0.4760994  P(node) =0.02801436\n    class counts:   249     0   274\n   probabilities: 0.476 0.000 0.524 \n  left son=60 (284 obs) right son=61 (239 obs)\n  Primary splits:\n      DISTANCE_TO_DIAGONAL    &lt; 0.990107   to the left,  improve=165.999900, (0 missing)\n      DISTANCE_TO_METRO       &lt; 0.1705538  to the right, improve= 38.560880, (0 missing)\n      DISTANCE_TO_CITY_CENTER &lt; 0.44858    to the right, improve= 36.859370, (0 missing)\n      UNITPRICE               &lt; 4976.906   to the right, improve= 12.669910, (0 missing)\n      CONSTRUCTEDAREA         &lt; 124.5      to the right, improve=  9.569507, (0 missing)\n  Surrogate splits:\n      DISTANCE_TO_METRO       &lt; 0.1699124  to the right, agree=0.744, adj=0.439, (0 split)\n      DISTANCE_TO_CITY_CENTER &lt; 0.4805939  to the right, agree=0.728, adj=0.406, (0 split)\n      UNITPRICE               &lt; 4976.906   to the right, agree=0.642, adj=0.218, (0 split)\n      CADCONSTRUCTIONYEAR     &lt; 1941.5     to the left,  agree=0.587, adj=0.096, (0 split)\n      CADDWELLINGCOUNT        &lt; 24.5       to the left,  agree=0.583, adj=0.088, (0 split)\n\nNode number 31: 8795 observations\n  predicted class=Media  expected loss=0.1353042  P(node) =0.4711018\n    class counts:   326   864  7605\n   probabilities: 0.037 0.098 0.865 \n\nNode number 56: 1748 observations\n  predicted class=Baja   expected loss=0.2580092  P(node) =0.09363115\n    class counts:    35  1297   416\n   probabilities: 0.020 0.742 0.238 \n\nNode number 57: 262 observations\n  predicted class=Media  expected loss=0.2061069  P(node) =0.01403396\n    class counts:     0    54   208\n   probabilities: 0.000 0.206 0.794 \n\nNode number 60: 284 observations\n  predicted class=Alta   expected loss=0.1584507  P(node) =0.01521238\n    class counts:   239     0    45\n   probabilities: 0.842 0.000 0.158 \n\nNode number 61: 239 observations\n  predicted class=Media  expected loss=0.041841  P(node) =0.01280197\n    class counts:    10     0   229\n   probabilities: 0.042 0.000 0.958 \n\n\n\nrpart.plot(arbol)\n\n\n\n\n\n\n\n\n\n\n\n# Decision Tree Classification\nfrom sklearn.tree import DecisionTreeClassifier\n\nclassifier = DecisionTreeClassifier(criterion = 'entropy', random_state = 1994)\nclf = classifier.fit(pyX_train, pyy_train)\n\n\nfrom sklearn import tree\n\ntree.plot_tree(clf)"
  },
  {
    "objectID": "material/trees_ensambleMethods/DecisionsTree_RandomForest_XGBoost.html#creamos-las-predicciones",
    "href": "material/trees_ensambleMethods/DecisionsTree_RandomForest_XGBoost.html#creamos-las-predicciones",
    "title": "Árboles de Decisión, Random Forest y XGBoost",
    "section": "4.2 Creamos las predicciones",
    "text": "4.2 Creamos las predicciones\n\nRPython\n\n\nAplicamos el modelo a nuestros valores de test.\n\npredict(arbol, rtest[1:10, ])\n\n   Alta      Baja      Media\n1     0 0.9469189 0.05308115\n2     0 0.9469189 0.05308115\n3     0 0.9469189 0.05308115\n4     0 0.9469189 0.05308115\n5     0 0.9469189 0.05308115\n6     0 0.1336032 0.86639676\n7     0 0.9469189 0.05308115\n8     0 0.9469189 0.05308115\n9     0 0.9469189 0.05308115\n10    0 0.1336032 0.86639676\n\n\n\npredicciones &lt;- predict(arbol, rtrain, type = \"class\")\ncaret::confusionMatrix(predicciones, as.factor(rtrain$RENTA))\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction Alta Baja Media\n     Alta  1396    0   460\n     Baja    35 5027   676\n     Media  510 1205  9360\n\nOverall Statistics\n                                          \n               Accuracy : 0.8454          \n                 95% CI : (0.8401, 0.8506)\n    No Information Rate : 0.5622          \n    P-Value [Acc &gt; NIR] : &lt; 2.2e-16       \n                                          \n                  Kappa : 0.7207          \n                                          \n Mcnemar's Test P-Value : &lt; 2.2e-16       \n\nStatistics by Class:\n\n                     Class: Alta Class: Baja Class: Media\nSensitivity              0.71922      0.8066       0.8918\nSpecificity              0.97250      0.9428       0.7902\nPos Pred Value           0.75216      0.8761       0.8451\nNeg Pred Value           0.96758      0.9068       0.8504\nPrevalence               0.10397      0.3338       0.5622\nDetection Rate           0.07478      0.2693       0.5014\nDetection Prevalence     0.09942      0.3074       0.5932\nBalanced Accuracy        0.84586      0.8747       0.8410\n\npredicciones &lt;- predict(arbol, rtest, type = \"class\")\ncaret::confusionMatrix(predicciones, as.factor(rtest$RENTA))\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction Alta Baja Media\n     Alta   311    1   148\n     Baja     5 1250   171\n     Media  151  266  2362\n\nOverall Statistics\n                                          \n               Accuracy : 0.8409          \n                 95% CI : (0.8301, 0.8513)\n    No Information Rate : 0.5747          \n    P-Value [Acc &gt; NIR] : &lt; 2.2e-16       \n                                          \n                  Kappa : 0.7099          \n                                          \n Mcnemar's Test P-Value : 3.415e-05       \n\nStatistics by Class:\n\n                     Class: Alta Class: Baja Class: Media\nSensitivity              0.66595      0.8240       0.8810\nSpecificity              0.96451      0.9441       0.7898\nPos Pred Value           0.67609      0.8766       0.8499\nNeg Pred Value           0.96290      0.9176       0.8309\nPrevalence               0.10011      0.3252       0.5747\nDetection Rate           0.06667      0.2680       0.5063\nDetection Prevalence     0.09861      0.3057       0.5957\nBalanced Accuracy        0.81523      0.8840       0.8354\n\n\n\nCM &lt;- caret::confusionMatrix(predicciones, as.factor(rtest$RENTA)); CM &lt;- data.frame(CM$table)\n\ngrafico &lt;- ggplot(CM, aes(Prediction,Reference, fill= Freq)) +\n        geom_tile() + geom_text(aes(label=Freq)) +\n        scale_fill_gradient(low=\"white\", high=\"#009194\") +\n        labs(x = \"Reference\",y = \"Prediction\")\n\nplot(grafico)\n\n\n\n\n\n\n\n\n\n\n\nimport pandas as pd\n\n# Prediction\ny_pred = classifier.predict(pyX_test)\n\nresults = pd.DataFrame({\n    'Real': pyy_test,  # Valores reales\n    'Predicho': y_pred  # Valores predichos\n})\n\n# Muestra los primeros 5 registros\nprint(results.head())  \n\n       Real  Predicho\n2399      2         2\n18420     0         0\n22515     2         2\n20265     1         1\n10432     1         1\n\n\n\nfrom sklearn.metrics import classification_report\n\nprint(f'Classification Report: \\n{classification_report(pyy_test, y_pred)}')\n\nClassification Report: \n              precision    recall  f1-score   support\n\n           0       0.85      0.82      0.84       488\n           1       0.92      0.91      0.91      1522\n           2       0.92      0.93      0.92      2657\n\n    accuracy                           0.91      4667\n   macro avg       0.89      0.89      0.89      4667\nweighted avg       0.91      0.91      0.91      4667\n\n\n\nfrom sklearn.metrics import confusion_matrix\nimport seaborn as sns\n\n# Confusion matrix\ncf_matrix = confusion_matrix(pyy_test, y_pred)\nsns.heatmap(cf_matrix, annot=True, fmt='d', cmap='Blues', cbar=False)"
  },
  {
    "objectID": "material/trees_ensambleMethods/DecisionsTree_RandomForest_XGBoost.html#modelo-de-classificación-con-cross-evaluación",
    "href": "material/trees_ensambleMethods/DecisionsTree_RandomForest_XGBoost.html#modelo-de-classificación-con-cross-evaluación",
    "title": "Árboles de Decisión, Random Forest y XGBoost",
    "section": "4.3 Modelo de classificación con cross-evaluación",
    "text": "4.3 Modelo de classificación con cross-evaluación\n\nRPython\n\n\n\n## Generamos los parámetros de control\ntrControl &lt;- trainControl(method = \"cv\", number = 10, classProbs = TRUE,\n  summaryFunction = multiClassSummary)\n## En este caso, se realiza una cros-validación de 10 etapas\n\n# se fija una semilla aleatoria\nset.seed(1994)\n\n# se entrena el modelo\nmodel &lt;- train(RENTA ~ .,  # . equivale a incluir todas las variables\n               data = rtrain,\n               method = \"rpart\",\n               metric = \"Accuracy\",\n               trControl = trControl)\n\n# Obtenemos los valores del árbol óptimo\nmodel$finalModel\n\nn= 18669 \n\nnode), split, n, loss, yval, (yprob)\n      * denotes terminal node\n\n 1) root 18669 8173 Media (0.10396915 0.33381542 0.56221544)  \n   2) DISTANCE_TO_DIAGONAL&gt;=1.697581 9078 3793 Baja (0.02599692 0.58217669 0.39182639)  \n     4) DISTANCE_TO_CITY_CENTER&lt; 1.182486 2168  144 Baja (0.00000000 0.93357934 0.06642066) *\n     5) DISTANCE_TO_CITY_CENTER&gt;=1.182486 6910 3497 Media (0.03415340 0.47192475 0.49392185)  \n      10) DISTANCE_TO_DIAGONAL&gt;=2.350599 4436 1689 Baja (0.02705140 0.61925158 0.35369702) *\n      11) DISTANCE_TO_DIAGONAL&lt; 2.350599 2474  630 Media (0.04688763 0.20776071 0.74535166) *\n   3) DISTANCE_TO_DIAGONAL&lt; 1.697581 9591 2652 Media (0.17777083 0.09873840 0.72349077) *\n\n# Generamos el gráfico del árbol\nrpart.plot(model$finalModel)\n\n\n\n\n\n\n\n\n\nlibrary(reshape2)\n\n# A continuación generamos un gráfico que nos permite ver la variabilidad de los estadísticos\n# calculados\nggplot(melt(model$resample[,c(2:5, 7:9, 12:13)]), aes(x = variable, y = value, fill=variable)) +\n  geom_boxplot(show.legend=FALSE) +\n  xlab(NULL) + ylab(NULL)\n\n\n\n\n\n\n\n\n\n\n\nfrom sklearn.tree import DecisionTreeClassifier\n\n# Modelo de árbol de decisión\nmodel = DecisionTreeClassifier(random_state=1994)\n\nfrom sklearn.model_selection import cross_val_score\n\n# Realizar validación cruzada con 5 folds\nscores = cross_val_score(model, pyX_train, pyy_train, cv=10, scoring = 'accuracy')  # Métrica: accuracy\n\n# Mostrar resultados\nprint(f\"Accuracy por fold: {scores}\")\n\nAccuracy por fold: [0.90626674 0.90680236 0.8993037  0.90305303 0.91215854 0.90787359\n 0.89769684 0.90836013 0.914791   0.91425509]\n\nprint(f\"Accuracy promedio: {scores.mean():.4f}\")\n\nAccuracy promedio: 0.9071"
  },
  {
    "objectID": "material/trees_ensambleMethods/DecisionsTree_RandomForest_XGBoost.html#realizando-hiperparámetro-tunning",
    "href": "material/trees_ensambleMethods/DecisionsTree_RandomForest_XGBoost.html#realizando-hiperparámetro-tunning",
    "title": "Árboles de Decisión, Random Forest y XGBoost",
    "section": "4.4 Realizando hiperparámetro tunning",
    "text": "4.4 Realizando hiperparámetro tunning\n\nRPython\n\n\n\n# Detectamos cuales son los parámetros del modelo que podemos realizar hiperparámeter tunning\nmodelLookup(\"rpart\")\n\n  model parameter                label forReg forClass probModel\n1 rpart        cp Complexity Parameter   TRUE     TRUE      TRUE\n\n\n\n# Se especifica un rango de valores típicos para el hiperparámetro\ntuneGrid &lt;- expand.grid(cp = seq(0.01,0.05,0.01))\n\n\n# se entrena el modelo\nset.seed(1994)\n\nmodel &lt;- train(RENTA ~ .,\n               data = rtrain,\n               method = \"rpart\",\n               metric = \"Accuracy\",\n               trControl = trControl,\n               tuneGrid = tuneGrid)\n\n# Obtenemos la información del mejor modelo\nmodel$bestTune\n\n    cp\n1 0.01\n\n# Gráfico del árbol obtenido\nrpart.plot(model$finalModel)\n\n\n\n\n\n\n\n\n\n\n\nfrom sklearn.model_selection import GridSearchCV\n\n# Definir rejilla de hiperparámetros\nparam_grid = {\n    'max_depth': [None, 5, 10],\n    'min_samples_split': [2, 5, 10],\n    'min_samples_leaf': [1, 2, 4]\n}\n\n# Declaramos el modelo\nmodel = DecisionTreeClassifier(random_state=1994)\n\n# Configurar GridSearch con validación cruzada\ngrid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=10, scoring='accuracy', n_jobs=-1)\n\n# Ajustar modelo\ngrid_search.fit(pyX_train, pyy_train)\n\nGridSearchCV(cv=10, estimator=DecisionTreeClassifier(random_state=1994),\n             n_jobs=-1,\n             param_grid={'max_depth': [None, 5, 10],\n                         'min_samples_leaf': [1, 2, 4],\n                         'min_samples_split': [2, 5, 10]},\n             scoring='accuracy')In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  GridSearchCV?Documentation for GridSearchCViFittedGridSearchCV(cv=10, estimator=DecisionTreeClassifier(random_state=1994),\n             n_jobs=-1,\n             param_grid={'max_depth': [None, 5, 10],\n                         'min_samples_leaf': [1, 2, 4],\n                         'min_samples_split': [2, 5, 10]},\n             scoring='accuracy') best_estimator_: DecisionTreeClassifierDecisionTreeClassifier(random_state=1994)  DecisionTreeClassifier?Documentation for DecisionTreeClassifierDecisionTreeClassifier(random_state=1994) \n\n# Mostrar mejores parámetros\nprint(f\"Mejores parámetros: {grid_search.best_params_}\")\n\nMejores parámetros: {'max_depth': None, 'min_samples_leaf': 1, 'min_samples_split': 2}\n\nprint(f\"Mejor accuracy: {grid_search.best_score_:.4f}\")\n\nMejor accuracy: 0.9071\n\n\n\nfrom sklearn import tree\ntree.plot_tree(grid_search.best_estimator_)"
  },
  {
    "objectID": "material/trees_ensambleMethods/DecisionsTree_RandomForest_XGBoost.html#como-realizar-poda-de-nuestro-árbol",
    "href": "material/trees_ensambleMethods/DecisionsTree_RandomForest_XGBoost.html#como-realizar-poda-de-nuestro-árbol",
    "title": "Árboles de Decisión, Random Forest y XGBoost",
    "section": "4.5 Como realizar poda de nuestro árbol",
    "text": "4.5 Como realizar poda de nuestro árbol\n\nRPython\n\n\n\n# Con el objetivo de aumentar la generalidad del árbol y facilitar su interpretación, \n# se procede a reducir su tamaño podándolo. Para ello se establece el criterio de \n# que un nodo terminal tiene que tener, como mínimo, 50 observaciones.\nset.seed(1994)\nprunedtree &lt;- rpart(RENTA ~ ., data = rtrain,\n                    cp= 0.01, control = rpart.control(minbucket = 50))\n\nrpart.plot(prunedtree)\n\n\n\n\n\n\n\n\n\n\nEn Python, la poda de un árbol de decisión se puede realizar ajustando los hiperparámetros del árbol durante su creación. Estos hiperparámetros controlan el crecimiento del árbol y, por lo tanto, actúan como técnicas de poda preventiva o postpoda.\nscikit-learn no implementa poda dinámica directa (como ocurre en algunos otros frameworks), pero puedes limitar el tamaño del árbol y evitar sobreajuste mediante los siguientes métodos.\n\n4.5.1 Poda Preventiva (Pre-pruning)\nPoda preventiva consiste en detener el crecimiento del árbol antes de que se haga demasiado grande. Esto se logra ajustando hiperparámetros como:\n\nmax_depth: Profundidad máxima del árbol\nmin_samples_split: Número mínimo de muestras necesarias para dividir un nodo.\nmin_samples_leaf: Número mínimo de muestras necesarias en una hoja.\nmax_leaf_nodes: Número máximo de nodos hoja en el árbol.\n\n\n# Crear un árbol con poda preventiva\nmodel = DecisionTreeClassifier(\n    max_depth=3,              # Limitar la profundidad\n    min_samples_split=10,     # Mínimo 10 muestras para dividir un nodo\n    min_samples_leaf=5,       # Mínimo 5 muestras por hoja\n    random_state=42\n)\n\n# Entrenar el modelo\nmodel.fit(pyX_train, pyy_train)\n\nDecisionTreeClassifier(max_depth=3, min_samples_leaf=5, min_samples_split=10,\n                       random_state=42)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  DecisionTreeClassifier?Documentation for DecisionTreeClassifieriFittedDecisionTreeClassifier(max_depth=3, min_samples_leaf=5, min_samples_split=10,\n                       random_state=42) \n\n# Evaluar\nprint(f\"Accuracy en entrenamiento: {model.score(pyX_train, pyy_train):.4f}\")\n\nAccuracy en entrenamiento: 0.7919\n\nprint(f\"Accuracy en prueba: {model.score(pyX_test, pyy_test):.4f}\")\n\nAccuracy en prueba: 0.7932\n\n\n# Graficamos el árbol podado\ntree.plot_tree(model)\n\n\n\n\n\n\n\n\n\n\n4.5.2 Poda Posterior (Post-Pruning) con ccp_alpha\nSe puedes realizar poda posterior usando cost complexity pruning. Esto implica ajustar el parámetro ccp_alpha (el parámetro de complejidad de coste).\nEl árbol generará múltiples subárboles podados para diferentes valores de ccp_alpha, y tú puedes elegir el más adecuado evaluando su desempeño.\n\nimport matplotlib.pyplot as plt\n\n# Crear un árbol sin poda\nmodel = DecisionTreeClassifier(random_state=1994)\nmodel.fit(pyX_train, pyy_train)\n\nDecisionTreeClassifier(random_state=1994)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  DecisionTreeClassifier?Documentation for DecisionTreeClassifieriFittedDecisionTreeClassifier(random_state=1994) \n\n# Obtener valores de ccp_alpha\npath = model.cost_complexity_pruning_path(pyX_train, pyy_train)\nccp_alphas = path.ccp_alphas\nimpurities = path.impurities\n\n# Entrenar árboles para cada valor de ccp_alpha\nmodels = []\nfor ccp_alpha in ccp_alphas:\n    clf = DecisionTreeClassifier(random_state=42, ccp_alpha=ccp_alpha)\n    clf.fit(pyX_train, pyy_train)\n    models.append(clf)\n\nDecisionTreeClassifier(ccp_alpha=0.12113075107966081, random_state=42)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  DecisionTreeClassifier?Documentation for DecisionTreeClassifieriFittedDecisionTreeClassifier(ccp_alpha=0.12113075107966081, random_state=42) \n\n# Evaluar desempeño\ntrain_scores = [clf.score(pyX_train, pyy_train) for clf in models]\ntest_scores = [clf.score(pyX_test, pyy_test) for clf in models]\n\n# Graficar resultados\nplt.figure(figsize=(8, 6))\nplt.plot(ccp_alphas, train_scores, marker='o', label=\"Train Accuracy\", drawstyle=\"steps-post\")\nplt.plot(ccp_alphas, test_scores, marker='o', label=\"Test Accuracy\", drawstyle=\"steps-post\")\nplt.xlabel(\"ccp_alpha\")\nplt.ylabel(\"Accuracy\")\nplt.title(\"Accuracy vs ccp_alpha\")\nplt.legend()\nplt.grid()\nplt.show()"
  },
  {
    "objectID": "material/trees_ensambleMethods/DecisionsTree_RandomForest_XGBoost.html#aplicación-del-modelo",
    "href": "material/trees_ensambleMethods/DecisionsTree_RandomForest_XGBoost.html#aplicación-del-modelo",
    "title": "Árboles de Decisión, Random Forest y XGBoost",
    "section": "5.1 Aplicación del modelo",
    "text": "5.1 Aplicación del modelo\n\nRPython\n\n\n\n# Random Forest \nlibrary(randomForest)\n## devtools::install_github('araastat/reprtree') # Se instala 1 vez para poder printar graficos\nlibrary(reprtree)\n\nset.seed(1994)\narbol_rf &lt;- randomForest(as.factor(RENTA) ~ .,  data = rtrain, ntree = 25)\n\n\n# se observa el árbol número 20\ntree20 &lt;- getTree(arbol_rf, 20, labelVar = TRUE)\nhead(tree20)\n\n  left daughter right daughter               split var split point status\n1             2              3         CONSTRUCTEDAREA  74.5000000      1\n2             4              5       DISTANCE_TO_METRO   0.7023313      1\n3             6              7              BATHNUMBER   2.5000000      1\n4             8              9             HASWARDROBE   1.5000000      1\n5            10             11       DISTANCE_TO_METRO   1.6344953      1\n6            12             13 DISTANCE_TO_CITY_CENTER   1.1452018      1\n  prediction\n1       &lt;NA&gt;\n2       &lt;NA&gt;\n3       &lt;NA&gt;\n4       &lt;NA&gt;\n5       &lt;NA&gt;\n6       &lt;NA&gt;\n\n## Sin embargo, el método por el que se representa gráficamente no es muy claro y\n## puede llevar a confusión o dificultar la interpretación del árbol. \n## Si se desea estudiar el árbol, hasta un cierto nivel, se puede incluir el argumento depth.\n## El árbol, ahora con una profundidad de 5 ramas.\nplot.getTree(arbol_rf, k = 20, depth = 5)\n\n\n\n\n\n\n\n\n\nlibrary(vip)\nvip(arbol_rf)\n\n\n\n\n\n\n\n\n\n\n\nfrom sklearn.ensemble import RandomForestClassifier\n\nclf = RandomForestClassifier()\nclf.fit(pyX_train, pyy_train)\n\nRandomForestClassifier()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  RandomForestClassifier?Documentation for RandomForestClassifieriFittedRandomForestClassifier() \n\n\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nresults = pd.DataFrame(clf.feature_importances_, index=pyBCN.columns[:-1]).sort_values(by=0, ascending=False)\n\n# Crear gráfico de barras horizontales\nplt.figure(figsize=(10, 8))\nplt.barh(results.index, results[0], color='skyblue')\n\n# Añadir etiquetas y título\nplt.xlabel('Importancia')\nplt.ylabel('Características')\nplt.title('Importancia de las Características')\nplt.grid(axis='x', linestyle='--', alpha=0.7)\nplt.show()"
  },
  {
    "objectID": "material/trees_ensambleMethods/DecisionsTree_RandomForest_XGBoost.html#hiperparameter-tunning-de-random-forest",
    "href": "material/trees_ensambleMethods/DecisionsTree_RandomForest_XGBoost.html#hiperparameter-tunning-de-random-forest",
    "title": "Árboles de Decisión, Random Forest y XGBoost",
    "section": "5.2 Hiperparameter tunning de Random Forest",
    "text": "5.2 Hiperparameter tunning de Random Forest\n\nRPython\n\n\n\n# Identificamos los parámetros que podemos tunnerar\nmodelLookup(\"rf\")\n\n  model parameter                         label forReg forClass probModel\n1    rf      mtry #Randomly Selected Predictors   TRUE     TRUE      TRUE\n\n\n\n# Se especifica un rango de valores posibles de mtry\ntuneGrid &lt;- expand.grid(mtry = c(1, 2, 5, 10))\ntuneGrid\n\n  mtry\n1    1\n2    2\n3    5\n4   10\n\n\n\n# se fija la semilla aleatoria\nset.seed(1994)\n\n# se entrena el modelo\nmodel &lt;- train(RENTA ~ ., data = rtrain, \n               ntree = 20,\n               method = \"rf\", metric = \"Accuracy\",\n               tuneGrid = tuneGrid,\n               trControl = trainControl(classProbs = TRUE))\n\n# Visualizamos los hiperparámetros obtenidos \nmodel$results\n\n  mtry  Accuracy     Kappa  AccuracySD     KappaSD\n1    1 0.7114013 0.4050249 0.021913133 0.053223045\n2    2 0.8424843 0.7083545 0.005692328 0.011410794\n3    5 0.9015691 0.8223990 0.003130659 0.006087154\n4   10 0.9143875 0.8460688 0.002978894 0.005415628\n\n\n\n\n\n5.2.1 Ajuste de hiperparámetros con GridSearchCV\nEl GridSearchCV realiza una búsqueda exhaustiva sobre un conjunto de parámetros especificados. Probará todas las combinaciones posibles de hiperparámetros.\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n\n# Definir los parámetros para la búsqueda\nparam_dist = {\n    'n_estimators': [150, 200],        # Número de árboles en el bosque\n    'max_depth': [None, 10, 20],            # Profundidad máxima del árbol\n    'min_samples_split': [2, 5, 10],            # Número mínimo de muestras para dividir un nodo\n    'min_samples_leaf': [1, 2, 4],               # Número mínimo de muestras en una hoja\n    'max_features': ['auto'],      # Número de características a considerar para dividir un nodo\n    'bootstrap': [True]                      # Si usar bootstrap para los árboles\n}\n\n# Crear el modelo RandomForest\nrf = RandomForestClassifier(random_state = 1994)\n\n# Usar GridSearchCV para encontrar el mejor conjunto de parámetros\ngrid_search = GridSearchCV(estimator = rf, param_grid = param_grid, cv = 10, n_jobs = -1, verbose = 0)\n\n# Ajustar el modelo con los datos de entrenamiento\ngrid_search.fit(pyX_train, pyy_train)\n\n# Mostrar los mejores parámetros encontrados\nprint(\"Mejores parámetros encontrados:\", grid_search.best_params_)\n\ntree.plot_tree(grid_search.best_estimator_)\n\n\n\n5.2.2 Ajuste de Hiperparámetros con RandomizedSearchCV\nRandomizedSearchCV es una técnica más eficiente que GridSearchCV, ya que no prueba todas las combinaciones posibles, sino un número limitado de combinaciones aleatorias dentro de un rango definido. Esto es útil si el espacio de búsqueda es grande y quieres evitar un tiempo de cómputo muy largo.\n\n# Definir los parámetros para la búsqueda aleatoria\nparam_dist = {\n    'n_estimators': [150, 200],        # Número de árboles en el bosque\n    'max_depth': [None, 10, 20],            # Profundidad máxima del árbol\n    'min_samples_split': [2, 5, 10],            # Número mínimo de muestras para dividir un nodo\n    'min_samples_leaf': [1, 2, 4],               # Número mínimo de muestras en una hoja\n    'max_features': ['auto'],      # Número de características a considerar para dividir un nodo\n    'bootstrap': [True]                      # Si usar bootstrap para los árboles\n}\n\n# Usar RandomizedSearchCV para búsqueda aleatoria\nrandom_search = RandomizedSearchCV(estimator=rf, param_distributions=param_dist, \n                                   n_iter=50, cv=10, n_jobs=-1, random_state=1994)\n\n# Ajustar el modelo con los datos de entrenamiento\nrandom_search.fit(X_train, y_train)\n\n# Mostrar los mejores parámetros encontrados\nprint(\"Mejores parámetros encontrados:\", random_search.best_params_)\n\ntree.plot_tree(random_search.best_estimator_)"
  },
  {
    "objectID": "material/trees_ensambleMethods/DecisionsTree_RandomForest_XGBoost.html#predicciones-del-algoritmo",
    "href": "material/trees_ensambleMethods/DecisionsTree_RandomForest_XGBoost.html#predicciones-del-algoritmo",
    "title": "Árboles de Decisión, Random Forest y XGBoost",
    "section": "5.3 Predicciones del algoritmo",
    "text": "5.3 Predicciones del algoritmo\n\nRPython\n\n\n\nprediccion &lt;- predict(arbol_rf, rtrain, type = \"class\")\ncaret::confusionMatrix(prediccion, as.factor(rtrain$RENTA))\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction  Alta  Baja Media\n     Alta   1931     0     3\n     Baja      0  6211     7\n     Media    10    21 10486\n\nOverall Statistics\n                                         \n               Accuracy : 0.9978         \n                 95% CI : (0.997, 0.9984)\n    No Information Rate : 0.5622         \n    P-Value [Acc &gt; NIR] : &lt; 2.2e-16      \n                                         \n                  Kappa : 0.9961         \n                                         \n Mcnemar's Test P-Value : NA             \n\nStatistics by Class:\n\n                     Class: Alta Class: Baja Class: Media\nSensitivity               0.9948      0.9966       0.9990\nSpecificity               0.9998      0.9994       0.9962\nPos Pred Value            0.9984      0.9989       0.9971\nNeg Pred Value            0.9994      0.9983       0.9988\nPrevalence                0.1040      0.3338       0.5622\nDetection Rate            0.1034      0.3327       0.5617\nDetection Prevalence      0.1036      0.3331       0.5633\nBalanced Accuracy         0.9973      0.9980       0.9976\n\n# Realizamos las predicciones de este ultimo arbol para la predicción de test\n## Si no decimos nada en type (type = prob), nos devolvera la probabilidad de \n## pertenecer a cada clase. \nprediccion &lt;- predict(arbol_rf, rtest, type = \"class\")\n## Para ver la performance, realizaremos la matriz de confusión \ncaret::confusionMatrix(prediccion, as.factor(rtest$RENTA))\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction Alta Baja Media\n     Alta   344    1    52\n     Baja     1 1382    76\n     Media  122  134  2553\n\nOverall Statistics\n                                        \n               Accuracy : 0.9173        \n                 95% CI : (0.909, 0.925)\n    No Information Rate : 0.5747        \n    P-Value [Acc &gt; NIR] : &lt; 2.2e-16     \n                                        \n                  Kappa : 0.8478        \n                                        \n Mcnemar's Test P-Value : 1.382e-09     \n\nStatistics by Class:\n\n                     Class: Alta Class: Baja Class: Media\nSensitivity              0.73662      0.9110       0.9523\nSpecificity              0.98737      0.9755       0.8710\nPos Pred Value           0.86650      0.9472       0.9089\nNeg Pred Value           0.97118      0.9579       0.9310\nPrevalence               0.10011      0.3252       0.5747\nDetection Rate           0.07374      0.2962       0.5473\nDetection Prevalence     0.08510      0.3128       0.6021\nBalanced Accuracy        0.86200      0.9433       0.9116\n\n\n\nCM &lt;- caret::confusionMatrix(prediccion, as.factor(rtest$RENTA)); CM &lt;- data.frame(CM$table)\n\ngrafico &lt;- ggplot(CM, aes(Prediction,Reference, fill= Freq)) +\n        geom_tile() + geom_text(aes(label=Freq)) +\n        scale_fill_gradient(low=\"white\", high=\"#009194\") +\n        labs(x = \"Reference\",y = \"Prediction\")\n\nplot(grafico)\n\n\n\n\n\n\n\n\n\n\n\npreds = clf.predict(pyX_test)\nprint(f'Classification Report: \\n{classification_report(pyy_test, preds)}')\n\nClassification Report: \n              precision    recall  f1-score   support\n\n           0       0.90      0.77      0.83       488\n           1       0.95      0.90      0.93      1522\n           2       0.91      0.96      0.93      2657\n\n    accuracy                           0.92      4667\n   macro avg       0.92      0.88      0.90      4667\nweighted avg       0.92      0.92      0.92      4667\n\n\n\n# Confusion matrix\ncf_matrix = confusion_matrix(pyy_test, preds)\nsns.heatmap(cf_matrix, annot=True, fmt='d', cmap='Blues', cbar=False)"
  },
  {
    "objectID": "material/trees_ensambleMethods/DecisionsTree_RandomForest_XGBoost.html#aplicamos-el-algoritmo-con-cross-validation-e-hiperparameter-tunning",
    "href": "material/trees_ensambleMethods/DecisionsTree_RandomForest_XGBoost.html#aplicamos-el-algoritmo-con-cross-validation-e-hiperparameter-tunning",
    "title": "Árboles de Decisión, Random Forest y XGBoost",
    "section": "6.1 Aplicamos el algoritmo con cross-validation e hiperparameter tunning",
    "text": "6.1 Aplicamos el algoritmo con cross-validation e hiperparameter tunning\n\nRPython\n\n\nPara aplicar los modelos de XGBoost es necesario pasar los datos categoricos en dummies. Una variable dummy (también conocida como cualitativa o binaria) es aquella que toma el valor 1 o 0 para indicar la presencia o ausencia de una cierta característica o condición.\n\n\nSi quisieramos hacerlo en cross validación hariamos lo siguiente:"
  },
  {
    "objectID": "material/trees_ensambleMethods/DecisionsTree_RandomForest_XGBoost.html#predicciones",
    "href": "material/trees_ensambleMethods/DecisionsTree_RandomForest_XGBoost.html#predicciones",
    "title": "Árboles de Decisión, Random Forest y XGBoost",
    "section": "6.2 Predicciones",
    "text": "6.2 Predicciones\n\nRPython"
  },
  {
    "objectID": "index.html#introducción-a-la-mineria-de-datos",
    "href": "index.html#introducción-a-la-mineria-de-datos",
    "title": "Métodos Estadísticos para la mineria de datos",
    "section": "1 Introducción a la mineria de datos",
    "text": "1 Introducción a la mineria de datos\nLa minería de datos es el proceso de extraer patrones, tendencias y conocimientos útiles a partir de grandes volúmenes de datos. Combina estadística, aprendizaje automático y bases de datos para ayudar a resolver problemas en diversas áreas, como negocios, ciencia y tecnología.\nTeoria\nLaboratorio - Software Carpentry\nLaboratorio - Descriptive Analysis\nLaboratorio - Advanced Preprocessing"
  },
  {
    "objectID": "index.html#introducción-a-la-mineria-de-datos-1",
    "href": "index.html#introducción-a-la-mineria-de-datos-1",
    "title": "Métodos Estadísticos para la mineria de datos",
    "section": "2 [Introducción a la mineria de datos] ()",
    "text": "2 [Introducción a la mineria de datos] ()"
  },
  {
    "objectID": "index.html#introducción-a-la-mineria-de-datos-2",
    "href": "index.html#introducción-a-la-mineria-de-datos-2",
    "title": "Métodos Estadísticos para la mineria de datos",
    "section": "3 [Introducción a la mineria de datos] ()",
    "text": "3 [Introducción a la mineria de datos] ()"
  },
  {
    "objectID": "index.html#introducción-a-la-mineria-de-datos-3",
    "href": "index.html#introducción-a-la-mineria-de-datos-3",
    "title": "Métodos Estadísticos para la mineria de datos",
    "section": "4 [Introducción a la mineria de datos] ()",
    "text": "4 [Introducción a la mineria de datos] ()"
  },
  {
    "objectID": "index.html#introducción-a-la-mineria-de-datos-4",
    "href": "index.html#introducción-a-la-mineria-de-datos-4",
    "title": "Métodos Estadísticos para la mineria de datos",
    "section": "5 [Introducción a la mineria de datos] ()",
    "text": "5 [Introducción a la mineria de datos] ()"
  },
  {
    "objectID": "index.html#introducción-a-la-mineria-de-datos-5",
    "href": "index.html#introducción-a-la-mineria-de-datos-5",
    "title": "Métodos Estadísticos para la mineria de datos",
    "section": "6 [Introducción a la mineria de datos] ()",
    "text": "6 [Introducción a la mineria de datos] ()"
  },
  {
    "objectID": "index.html#introducción-a-la-mineria-de-datos-6",
    "href": "index.html#introducción-a-la-mineria-de-datos-6",
    "title": "Métodos Estadísticos para la mineria de datos",
    "section": "7 [Introducción a la mineria de datos] ()",
    "text": "7 [Introducción a la mineria de datos] ()"
  },
  {
    "objectID": "index.html#introducción-a-la-mineria-de-datos-7",
    "href": "index.html#introducción-a-la-mineria-de-datos-7",
    "title": "Métodos Estadísticos para la mineria de datos",
    "section": "8 [Introducción a la mineria de datos] ()",
    "text": "8 [Introducción a la mineria de datos] ()"
  },
  {
    "objectID": "index.html#introducción-a-la-mineria-de-datos-8",
    "href": "index.html#introducción-a-la-mineria-de-datos-8",
    "title": "Métodos Estadísticos para la mineria de datos",
    "section": "9 [Introducción a la mineria de datos] ()",
    "text": "9 [Introducción a la mineria de datos] ()"
  },
  {
    "objectID": "index.html#introducción-a-la-mineria-de-datos-9",
    "href": "index.html#introducción-a-la-mineria-de-datos-9",
    "title": "Métodos Estadísticos para la mineria de datos",
    "section": "10 [Introducción a la mineria de datos] ()",
    "text": "10 [Introducción a la mineria de datos] ()"
  },
  {
    "objectID": "index.html#introducción-a-la-mineria-de-datos-material",
    "href": "index.html#introducción-a-la-mineria-de-datos-material",
    "title": "Métodos Estadísticos para la mineria de datos",
    "section": "11 [Introducción a la mineria de datos] (material/)",
    "text": "11 [Introducción a la mineria de datos] (material/)"
  },
  {
    "objectID": "index.html#árboles-de-decisión-y-métodos-de-ensamblado",
    "href": "index.html#árboles-de-decisión-y-métodos-de-ensamblado",
    "title": "Métodos Estadísticos para la mineria de datos",
    "section": "12 Árboles de decisión y métodos de ensamblado",
    "text": "12 Árboles de decisión y métodos de ensamblado\npráctica"
  },
  {
    "objectID": "index.html#clustering",
    "href": "index.html#clustering",
    "title": "Métodos Estadísticos para la mineria de datos",
    "section": "2 Clustering",
    "text": "2 Clustering\nEl clustering agrupa datos similares en clústeres basados en características compartidas. Es útil para descubrir patrones ocultos y segmentar conjuntos de datos, comúnmente aplicado en marketing, biología y análisis de redes.\nTeoria\nLaboratorio"
  },
  {
    "objectID": "index.html#visualización-de-datos",
    "href": "index.html#visualización-de-datos",
    "title": "Métodos Estadísticos para la mineria de datos",
    "section": "3 Visualización de datos",
    "text": "3 Visualización de datos\nLa visualización de datos convierte información compleja en gráficos y representaciones visuales claras, facilitando la interpretación y comunicación de resultados. Herramientas como gráficos de dispersión, histogramas y mapas de calor son fundamentales.\n\n3.1 Analisis de componentes principales (ACP)\nEl ACP reduce la dimensionalidad de los datos al identificar las combinaciones lineales más relevantes de las variables originales, conservando la mayor parte de la variación. Se usa para simplificar datos y facilitar su interpretación.\nTeoria\nLaboratorio\n\n\n3.2 Analisis de correspondiencias múltiples (ACM)\nEl ACM analiza tablas de datos categóricos para identificar relaciones entre categorías, visualizando patrones en mapas bidimensionales que facilitan la interpretación.\nTeoria\nLaboratorio - ACS\nLaboratorio - ACM"
  },
  {
    "objectID": "index.html#reglas-de-asociación",
    "href": "index.html#reglas-de-asociación",
    "title": "Métodos Estadísticos para la mineria de datos",
    "section": "4 Reglas de asociación",
    "text": "4 Reglas de asociación\nEste método identifica relaciones significativas entre variables en grandes bases de datos. Es clave en aplicaciones como los sistemas de recomendación y análisis de cestas de mercado.\nTeoria\nLaboratorio"
  },
  {
    "objectID": "index.html#reglas-de-clasificación",
    "href": "index.html#reglas-de-clasificación",
    "title": "Métodos Estadísticos para la mineria de datos",
    "section": "5 Reglas de clasificación",
    "text": "5 Reglas de clasificación\nLos modelos de clasificación asignan datos a categorías predefinidas basándose en patrones aprendidos. Es ampliamente usado en diagnóstico médico, detección de fraudes y análisis de texto.\n\n5.1 Lineal Discriminant Analysis (LDA) y Quadratic Discriminant Analysis (QDA)\nAmbos métodos buscan separar categorías utilizando fronteras de decisión basadas en estadísticas. LDA asume varianzas iguales entre clases, mientras que QDA permite varianzas diferentes.\nTeoria\nLaboratorio\n\n\n5.2 Naives Bayes\nUn clasificador basado en probabilidad que asume independencia entre las características. Es eficiente y se aplica en problemas como clasificación de texto y detección de spam.\nTeoria\nLaboratorio"
  },
  {
    "objectID": "index.html#métodos-particionales",
    "href": "index.html#métodos-particionales",
    "title": "Métodos Estadísticos para la mineria de datos",
    "section": "6 Métodos particionales",
    "text": "6 Métodos particionales\nDividen datos en subconjuntos o particiones, a menudo mediante árboles de decisión y técnicas relacionadas.\n\n6.1 Decisions Tree\nModelo gráfico que toma decisiones en base a condiciones secuenciales. Es intuitivo y útil en clasificación y regresión.\n\n\n6.2 Random Forest\nCombina múltiples árboles de decisión para mejorar precisión y reducir sobreajuste. Es robusto y adecuado para tareas de clasificación y regresión.\n\n\n6.3 Bagging & Boosting\nMétodos de ensamblado que mejoran el rendimiento combinando múltiples modelos. Bagging reduce la variabilidad, mientras que Boosting optimiza errores iterativamente.\nTeoria\nLaboratorio"
  },
  {
    "objectID": "index.html#métodos-flexibles-de-discriminación",
    "href": "index.html#métodos-flexibles-de-discriminación",
    "title": "Métodos Estadísticos para la mineria de datos",
    "section": "7 Métodos flexibles de discriminación",
    "text": "7 Métodos flexibles de discriminación\n\n7.1 Support Vectors Machines (SVM)\nSeparan clases usando hiperplanos óptimos en un espacio de alta dimensionalidad. Son efectivas en problemas no lineales y clasificación compleja.\nTeoria\nLaboratorio"
  },
  {
    "objectID": "index.html#deep-learning",
    "href": "index.html#deep-learning",
    "title": "Métodos Estadísticos para la mineria de datos",
    "section": "8 Deep Learning",
    "text": "8 Deep Learning\nEl aprendizaje profundo utiliza redes neuronales para modelar datos complejos. Es ampliamente aplicado en reconocimiento de imágenes, procesamiento de lenguaje natural y más.\n\n8.1 Redes neuronales: Discriminación pel perceptrón multicapa\nLas redes multicapa, basadas en múltiples capas de neuronas interconectadas, resuelven problemas no lineales con alta precisión.\n\n\n8.2 Redes neuronales convolucionales\nEspecializadas en procesar datos con estructura espacial, como imágenes. Extraen automáticamente características relevantes para tareas como clasificación de imágenes y visión por computadora.\nTeoria\nDatos de deportes\nDetección de imagenes deportivas\nDreamBooth (parte 1)\nImportante: Para poder hacer uso de este script es necesario que tengas:\n\nEntre 2 y 3 fotos de cuerpo entero\nEntre 3 y 5 fotos de medio cuerpo\nEntorno a 10 fotos de cara\n\nDreamBooth (parte 2)\nDreamBooth (completo)"
  },
  {
    "objectID": "material/ANN/02_DreamBooth_parte2.html",
    "href": "material/ANN/02_DreamBooth_parte2.html",
    "title": "DREAMBOOTH 🤖",
    "section": "",
    "text": "%load_ext autoreload\n%autoreload 2\n\nThe autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload\nDreambooth es un modelo de generación de aprendizaje profundo, y que fue desarrollado en 2022 por un grupo de investigadores de Google Research y la Universidad de Boston. La misión de esta tecnología es la de poder entrenar a modelos de inteligencia artificial para personalizarlo según tus necesidades.\nEsta web está creada por Dante Conti y Sergi Ramírez, (c) 2024"
  },
  {
    "objectID": "material/ANN/02_DreamBooth_parte2.html#como-funciona",
    "href": "material/ANN/02_DreamBooth_parte2.html#como-funciona",
    "title": "DREAMBOOTH 🤖",
    "section": "¿Como funciona? 🔩",
    "text": "¿Como funciona? 🔩\nEl funcionamiento de esta técnica funciona en tres pasos.\n\nEn primer lugar, necesitas un modelo de difusión preentrenado, que es uno de esos sistemas de inteligencia artificial que pueden crear imágenes a partir de texto. Por ejemplo, se puede usar:\n\n\nStable Diffusion\nDALL-E\nMidjourney\n\nsiempre y cuando funcionen con el proceso de ruido y denoising.\nLo que hace esta técnica es crear una imagen completamente ruidosa, y luego ir quitando ese ruido reconstruyendo en el proceso una imagen totalmente original que se parezca a lo que le has pedido por texto. Pues es en este punto en el que Dreambooth ayudará con un modelo entrenado para que puedas obtener imágenes de sujetos concretos.\n\nel segundo paso, en el que necesitas un conjunto de imágenes del sujeto con el que quieres personalizar la IA. Puede ser un estilo, una cara, o lo que sea. Se recomienda tener un set de unas 8 o 10 imágenes como mínimo para poder entrenar el modelo.\n\nEntonces, lo que hace Dreambooth es utilizar este set de imágenes para entrenar al modelo de difusión, entrenar a la IA para que sepa reconocer lo que hay en ellas. Puede reconocer tu cara para luego poder dibujarla desde cero, así como un estilo o una posición.\n\nUna vez has usado Dreambooth para entrenar a la IA, este sistema usará las imágenes del sujeto como punto de partida para el proceso de crear la imagen aleatoria, permitiendo que la IA tenga más información sobre cómo es el sujeto que quieres dibujar, y que así pueda hacer imágenes que se parezcan a él.\n\n\n\n\nImagen"
  },
  {
    "objectID": "material/ANN/02_DreamBooth_parte2.html#instalació-de-paquets",
    "href": "material/ANN/02_DreamBooth_parte2.html#instalació-de-paquets",
    "title": "DREAMBOOTH 🤖",
    "section": "Instalació de paquets",
    "text": "Instalació de paquets\n\nInstal·leu el paquet de Python Py-Dreambooth tal com es mostra a continuació.\n\n\n!pip install -q py_dreambooth"
  },
  {
    "objectID": "material/ANN/02_DreamBooth_parte2.html#importa-mòduls",
    "href": "material/ANN/02_DreamBooth_parte2.html#importa-mòduls",
    "title": "DREAMBOOTH 🤖",
    "section": "Importa mòduls",
    "text": "Importa mòduls\n\nHi ha diversos tipus de classes de model, però estaràs utilitzant el model més bàsic, el model Stable Diffusion Dreambooth SDDreamboothModel, però no t’has de preocupar per això ara mateix. 🤷‍♂️\n\n\nfrom py_dreambooth.dataset import LocalDataset\nfrom py_dreambooth.model import SdDreamboothModel\nfrom py_dreambooth.predictor import LocalPredictor\nfrom py_dreambooth.trainer import LocalTrainer\nfrom py_dreambooth.utils.image_helpers import display_images\nfrom py_dreambooth.utils.prompt_helpers import make_prompt\n\nMontem la relació entre el google drive i el quadern de jupyter\n\nfrom google.colab import drive\ndrive.mount('/content/drive')\n\nDrive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True)."
  },
  {
    "objectID": "material/ANN/02_DreamBooth_parte2.html#preparem-les-dades",
    "href": "material/ANN/02_DreamBooth_parte2.html#preparem-les-dades",
    "title": "DREAMBOOTH 🤖",
    "section": "Preparem les dades 📸",
    "text": "Preparem les dades 📸\n\nDATA_DIR = \"/content/drive/MyDrive/ANN/DREAMBOOTH/data\"  # el directori amb fotos per a que el model s'entreni\nOUTPUT_DIR = \"/content/drive/MyDrive/ANN/DREAMBOOTH/models\"  # El directori on s'ubicaran els fitxers de model entrenats\n\ndataset = LocalDataset(DATA_DIR)\n\n\nMolt important! En el DATA_DIR definit anteriorment, posar les imatges (jpg o png) del subjecte que es vol entrenar.\nPer a aquesta tasca, necessitareu entre 10 i 20 solos, selfies d’alta qualitat preses amb diferents fons, il·luminació i expressions facials. Crec que es pot trobar un gran exemple al repositori de GitHub de Joe Penna.\n\n\n\n\nSamples\n\n\n\nUtilitzeu el mètode de processament d’imatges següent per retallar les imatges en un quadrat centrat a la cara. Si el subjecte que el model està tractant d’aprendre no és una persona (per exemple, un gos), estableix l’argument detect_face argumentant com a False.\n\n\ndataset = dataset.preprocess_images(detect_face=True)\n\nA total of 8 images were found.\n\n\n 38%|███▊      | 3/8 [00:00&lt;00:00,  6.17it/s]\n\n\nNo faces detected in the image '443008034_395930086752782_7217331932050061307_n.jpg'.\nNo faces detected in the image '440173417_1436258973657135_9081022692963550822_n.jpg'.\n\n\n 75%|███████▌  | 6/8 [00:00&lt;00:00,  6.41it/s]\n\n\nNo faces detected in the image '429164819_452647503759342_2302826312178258320_n.jpg'.\n\n\n100%|██████████| 8/8 [00:01&lt;00:00,  5.50it/s]\n\n\nA total of 5 images were preprocessed and stored in the path '/content/drive/MyDrive/ANN/DREAMBOOTH/data_preproc'."
  },
  {
    "objectID": "material/ANN/02_DreamBooth_parte2.html#carregar-el-model",
    "href": "material/ANN/02_DreamBooth_parte2.html#carregar-el-model",
    "title": "DREAMBOOTH 🤖",
    "section": "Carregar el model 🤖",
    "text": "Carregar el model 🤖\n\nSi reinicieu el nucli del bloc de notes i voleu tornar a carregar els models que ja heu entrenat, podeu fer-ho de la següent manera.\n\n\npredictor = LocalPredictor(model, OUTPUT_DIR)"
  },
  {
    "objectID": "material/ANN/02_DreamBooth_parte2.html#crea-imatges-com-vulgueu",
    "href": "material/ANN/02_DreamBooth_parte2.html#crea-imatges-com-vulgueu",
    "title": "DREAMBOOTH 🤖",
    "section": "Crea imatges com vulgueu! 💃",
    "text": "Crea imatges com vulgueu! 💃\n\nUtilitzeu les indicacions per crear qualsevol imatge que vulgueu. El text de l’indicatiu ha de contenir el nom de l’assumpte i el nom de la classe definits anteriorment.\nTens problemes per arribar amb un bon prompte? No et preocupis. Podeu utilitzar la funció make_prompt per a generar una sol·licitud comissariada a l’atzar. Mira això. 🙆‍♀️\nLa creació de grans imatges pren paciència. Juga amb les indicacions, però si la qualitat de la pròpia generació és problemàtica, és possible que hagis de tornar a entrenar amb millors dades i paràmetres d’entrenament més adequats.\n\n\n%%time\nprompt = f\"A photo of {SUBJECT_NAME} {CLASS_NAME} with Simpsons\"\n# prompt = next(make_prompt(SUBJECT_NAME, CLASS_NAME))\n\nprint(f\"The prompt is as follows:\\n{prompt}\")\n\nimages = predictor.predict(\n    prompt,\n    height = 768,\n    width = 512,\n    num_images_per_prompt = 5,\n)\n\ndisplay_images(images, fig_size = 10)\n\nThe prompt is as follows:\nA photo of mire person with Simpsons\n\n\n\n\n\n\n\n\n\n\n\n\nCPU times: user 45.9 s, sys: 3.72 s, total: 49.6 s\nWall time: 50.3 s"
  },
  {
    "objectID": "material/ANN/02_DreamBooth_parte2.html#bibliografia",
    "href": "material/ANN/02_DreamBooth_parte2.html#bibliografia",
    "title": "DREAMBOOTH 🤖",
    "section": "Bibliografia 💃",
    "text": "Bibliografia 💃\n\nStable Diffusion\nDreamBooth\nPy-Dreambooth"
  },
  {
    "objectID": "material/ANN/01_DreamBooth_parte1.html",
    "href": "material/ANN/01_DreamBooth_parte1.html",
    "title": "DREAMBOOTH 🤖",
    "section": "",
    "text": "%load_ext autoreload\n%autoreload 2\n\nThe autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload\nDreambooth es un modelo de generación de aprendizaje profundo, y que fue desarrollado en 2022 por un grupo de investigadores de Google Research y la Universidad de Boston. La misión de esta tecnología es la de poder entrenar a modelos de inteligencia artificial para personalizarlo según tus necesidades.\nEsta web está creada por Dante Conti y Sergi Ramírez, (c) 2024"
  },
  {
    "objectID": "material/ANN/01_DreamBooth_parte1.html#como-funciona",
    "href": "material/ANN/01_DreamBooth_parte1.html#como-funciona",
    "title": "DREAMBOOTH 🤖",
    "section": "¿Como funciona? 🔩",
    "text": "¿Como funciona? 🔩\nEl funcionamiento de esta técnica funciona en tres pasos.\n\nEn primer lugar, necesitas un modelo de difusión preentrenado, que es uno de esos sistemas de inteligencia artificial que pueden crear imágenes a partir de texto. Por ejemplo, se puede usar:\n\n\nStable Diffusion\nDALL-E\nMidjourney\n\nsiempre y cuando funcionen con el proceso de ruido y denoising.\nLo que hace esta técnica es crear una imagen completamente ruidosa, y luego ir quitando ese ruido reconstruyendo en el proceso una imagen totalmente original que se parezca a lo que le has pedido por texto. Pues es en este punto en el que Dreambooth ayudará con un modelo entrenado para que puedas obtener imágenes de sujetos concretos.\n\nel segundo paso, en el que necesitas un conjunto de imágenes del sujeto con el que quieres personalizar la IA. Puede ser un estilo, una cara, o lo que sea. Se recomienda tener un set de unas 8 o 10 imágenes como mínimo para poder entrenar el modelo.\n\nEntonces, lo que hace Dreambooth es utilizar este set de imágenes para entrenar al modelo de difusión, entrenar a la IA para que sepa reconocer lo que hay en ellas. Puede reconocer tu cara para luego poder dibujarla desde cero, así como un estilo o una posición.\n\nUna vez has usado Dreambooth para entrenar a la IA, este sistema usará las imágenes del sujeto como punto de partida para el proceso de crear la imagen aleatoria, permitiendo que la IA tenga más información sobre cómo es el sujeto que quieres dibujar, y que así pueda hacer imágenes que se parezcan a él.\n\n\n\n\nImagen"
  },
  {
    "objectID": "material/ANN/01_DreamBooth_parte1.html#instalació-de-paquets",
    "href": "material/ANN/01_DreamBooth_parte1.html#instalació-de-paquets",
    "title": "DREAMBOOTH 🤖",
    "section": "Instalació de paquets",
    "text": "Instalació de paquets\n\nInstal·leu el paquet de Python Py-Dreambooth tal com es mostra a continuació.\n\n\n!pip install -q py_dreambooth"
  },
  {
    "objectID": "material/ANN/01_DreamBooth_parte1.html#importa-mòduls",
    "href": "material/ANN/01_DreamBooth_parte1.html#importa-mòduls",
    "title": "DREAMBOOTH 🤖",
    "section": "Importa mòduls",
    "text": "Importa mòduls\n\nHi ha diversos tipus de classes de model, però estaràs utilitzant el model més bàsic, el model Stable Diffusion Dreambooth SDDreamboothModel, però no t’has de preocupar per això ara mateix. 🤷‍♂️\n\n\nfrom py_dreambooth.dataset import LocalDataset\nfrom py_dreambooth.model import SdDreamboothModel\nfrom py_dreambooth.predictor import LocalPredictor\nfrom py_dreambooth.trainer import LocalTrainer\nfrom py_dreambooth.utils.image_helpers import display_images\nfrom py_dreambooth.utils.prompt_helpers import make_prompt\n\nMontem la relació entre el google drive i el quadern de jupyter\n\nfrom google.colab import drive\ndrive.mount('/content/drive')\n\nDrive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True)."
  },
  {
    "objectID": "material/ANN/01_DreamBooth_parte1.html#preparem-les-dades",
    "href": "material/ANN/01_DreamBooth_parte1.html#preparem-les-dades",
    "title": "DREAMBOOTH 🤖",
    "section": "Preparem les dades 📸",
    "text": "Preparem les dades 📸\n\nDATA_DIR = \"/content/drive/MyDrive/ANN/DREAMBOOTH/data\"  # el directori amb fotos per a que el model s'entreni\nOUTPUT_DIR = \"/content/drive/MyDrive/ANN/DREAMBOOTH/models\"  # El directori on s'ubicaran els fitxers de model entrenats\n\ndataset = LocalDataset(DATA_DIR)\n\n\nMolt important! En el DATA_DIR definit anteriorment, posar les imatges (jpg o png) del subjecte que es vol entrenar.\nPer a aquesta tasca, necessitareu entre 10 i 20 solos, selfies d’alta qualitat preses amb diferents fons, il·luminació i expressions facials. Crec que es pot trobar un gran exemple al repositori de GitHub de Joe Penna.\n\n\n\n\nSamples\n\n\n\nUtilitzeu el mètode de processament d’imatges següent per retallar les imatges en un quadrat centrat a la cara. Si el subjecte que el model està tractant d’aprendre no és una persona (per exemple, un gos), estableix l’argument detect_face argumentant com a False.\n\n\ndataset = dataset.preprocess_images(detect_face=True)\n\nA total of 8 images were found.\n\n\n 38%|███▊      | 3/8 [00:00&lt;00:00,  6.17it/s]\n\n\nNo faces detected in the image '443008034_395930086752782_7217331932050061307_n.jpg'.\nNo faces detected in the image '440173417_1436258973657135_9081022692963550822_n.jpg'.\n\n\n 75%|███████▌  | 6/8 [00:00&lt;00:00,  6.41it/s]\n\n\nNo faces detected in the image '429164819_452647503759342_2302826312178258320_n.jpg'.\n\n\n100%|██████████| 8/8 [00:01&lt;00:00,  5.50it/s]\n\n\nA total of 5 images were preprocessed and stored in the path '/content/drive/MyDrive/ANN/DREAMBOOTH/data_preproc'."
  },
  {
    "objectID": "material/ANN/01_DreamBooth_parte1.html#entrena-el-model",
    "href": "material/ANN/01_DreamBooth_parte1.html#entrena-el-model",
    "title": "DREAMBOOTH 🤖",
    "section": "Entrena el model 🤖",
    "text": "Entrena el model 🤖\n\nAra és el moment d’entrenar el model! Digues al model el nom del subjecte al qual vols entrenar (p. ex., Joe) i la classe a la qual pertany.\nEn definir un model, un dels arguments importants és quantes iteracions entrenar, o max.train.steps. S’accepta generalment que 800 a 1200 passos són apropiats per a una persona, i 200 a 400 passos són apropiats per a un animal no humà. El valor per defecte és 100 vegades el nombre de fotos que teniu. No cal que us preocupeu per això ara mateix ,🤷‍♂️, però si no us agraden els resultats de la imatge generada a continuació, aquest és el primer paràmetre a ajustar.\n\n\nSUBJECT_NAME = \"mire\"  # The name of the subject you want to learn\nCLASS_NAME = \"person\"  # The class to which the subject you want to learn belongs\n\nmodel = SdDreamboothModel(\n    subject_name = SUBJECT_NAME,\n    class_name = CLASS_NAME,\n    max_train_steps=400,\n)\n\ntrainer = LocalTrainer(output_dir = OUTPUT_DIR)\n\n\nEl temps d’entrenament del model pot ser tan curt com unes poques desenes de minuts o com diverses hores.\n\n\n%%time\npredictor = trainer.fit(model, dataset)\n\nThe model training has begun.\n'max_train_steps' is set to 400.\n\n\n╭─────────────────────────────── Traceback (most recent call last) ────────────────────────────────╮\n│ /usr/local/lib/python3.10/dist-packages/IPython/core/magics/execution.py:1335 in time            │\n│                                                                                                  │\n│   1332 │   │   else:                                                                             │\n│   1333 │   │   │   st = clock2()                                                                 │\n│   1334 │   │   │   try:                                                                          │\n│ ❱ 1335 │   │   │   │   exec(code, glob, local_ns)                                                │\n│   1336 │   │   │   │   out=None                                                                  │\n│   1337 │   │   │   │   # multi-line %%time case                                                  │\n│   1338 │   │   │   │   if expr_val is not None:                                                  │\n│ in &lt;module&gt;:1                                                                                    │\n│                                                                                                  │\n│ /usr/local/lib/python3.10/dist-packages/py_dreambooth/trainer.py:124 in fit                      │\n│                                                                                                  │\n│   121 │   │   │   f\"The model training has begun.\\n'max_train_steps' is set to {max_train_step   │\n│   122 │   │   │   self.logger,                                                                   │\n│   123 │   │   )                                                                                  │\n│ ❱ 124 │   │   _ = subprocess.run(shlex.split(command), check=True)                               │\n│   125 │   │   log_or_print(\"The model training has ended.\", self.logger)                         │\n│   126 │   │                                                                                      │\n│   127 │   │   predictor = LocalPredictor(model, self.output_dir, self.logger)                    │\n│                                                                                                  │\n│ /usr/lib/python3.10/subprocess.py:526 in run                                                     │\n│                                                                                                  │\n│    523 │   │   │   raise                                                                         │\n│    524 │   │   retcode = process.poll()                                                          │\n│    525 │   │   if check and retcode:                                                             │\n│ ❱  526 │   │   │   raise CalledProcessError(retcode, process.args,                               │\n│    527 │   │   │   │   │   │   │   │   │    output=stdout, stderr=stderr)                        │\n│    528 │   return CompletedProcess(process.args, retcode, stdout, stderr)                        │\n│    529                                                                                           │\n╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\nCalledProcessError: Command '['accelerate', 'launch', \n'/usr/local/lib/python3.10/dist-packages/py_dreambooth/scripts/train/train_sd_dreambooth.py', \n'--pretrained_model_name_or_path', 'stabilityai/stable-diffusion-2-1-base', '--instance_data_dir', \n'/content/drive/MyDrive/ANN/DREAMBOOTH/data_preproc', '--instance_prompt', 'a photo of mire person', \n'--num_class_images', '200', '--output_dir', '/content/drive/MyDrive/ANN/DREAMBOOTH/models', '--resolution', '768',\n'--train_batch_size', '1', '--learning_rate', '2e-06', '--lr_scheduler', 'constant', '--lr_warmup_steps', '0', \n'--validation_prompt', 'a photo of mire person with Eiffel Tower in the background', '--compress_output', 'False', \n'--with_prior_preservation', 'True', '--prior_loss_weight', '1.0', '--class_prompt', 'a photo of person', \n'--train_text_encoder', 'True', '--max_train_steps', '400', '--gradient_accumulation_steps', '1', \n'--gradient_checkpointing', 'True', '--use_8bit_adam', 'True', '--enable_xformers_memory_efficient_attention', \n'True', '--mixed_precision', 'fp16', '--set_grads_to_none', 'True', '--report_to', 'tensorboard']' returned \nnon-zero exit status 1."
  },
  {
    "objectID": "material/ANN/01_DreamBooth_parte1.html#bibliografia",
    "href": "material/ANN/01_DreamBooth_parte1.html#bibliografia",
    "title": "DREAMBOOTH 🤖",
    "section": "Bibliografia 💃",
    "text": "Bibliografia 💃\n\nStable Diffusion\nDreamBooth\nPy-Dreambooth"
  },
  {
    "objectID": "material/ANN/Ejercicio_CNN_Deportes.html",
    "href": "material/ANN/Ejercicio_CNN_Deportes.html",
    "title": "Convolutional Neural Networks",
    "section": "",
    "text": "import numpy as np\nimport os\nimport re\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report\n\n\n!pip install keras\n\nRequirement already satisfied: keras in /usr/local/lib/python3.10/dist-packages (3.5.0)\nRequirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from keras) (1.4.0)\nRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from keras) (1.26.4)\nRequirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras) (13.9.4)\nRequirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras) (0.0.8)\nRequirement already satisfied: h5py in /usr/local/lib/python3.10/dist-packages (from keras) (3.12.1)\nRequirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras) (0.13.1)\nRequirement already satisfied: ml-dtypes in /usr/local/lib/python3.10/dist-packages (from keras) (0.4.1)\nRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from keras) (24.2)\nRequirement already satisfied: typing-extensions&gt;=4.5.0 in /usr/local/lib/python3.10/dist-packages (from optree-&gt;keras) (4.12.2)\nRequirement already satisfied: markdown-it-py&gt;=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich-&gt;keras) (3.0.0)\nRequirement already satisfied: pygments&lt;3.0.0,&gt;=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich-&gt;keras) (2.18.0)\nRequirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py&gt;=2.2.0-&gt;rich-&gt;keras) (0.1.2)\n\n\n\nimport keras\nfrom keras.utils import to_categorical\nfrom keras import Input, Model\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Flatten, BatchNormalization, LeakyReLU\nfrom keras.layers import Conv2D, MaxPooling2D\n\nVamos a emparejar el notebook de python con el google drive\n\nfrom google.colab import drive\ndrive.mount('/content/drive')\n\nMounted at /content/drive\n\n\n\n\nLa red toma como entrada los pixeles de una imagen. Si tenemos una imagen con apenas 28×28 pixeles de alto y ancho, eso equivale a 784 neuronas. Y eso es si sólo tenemos 1 color (escala de grises). Si tuviéramos una imagen a color, necesitaríamos 3 canales (red, green, blue) y entonces usaríamos 28x28x3 = 2352 neuronas de entrada. Esa es nuestra capa de entrada.\n\n\n\nimagen\n\n\n\n\n\ndirname = os.path.join(os.getcwd(), 'drive/MyDrive/DOCENCIA/ANN/sports')\nimgpath = dirname + os.sep\n\n\nimages = []\ndirectories = []\ndircount = []\nprevRoot=''\ncant=0\n\nprint(\"leyendo imagenes de \",imgpath)\n\nfor root, dirnames, filenames in os.walk(imgpath):\n    for filename in filenames:\n        if re.search(\"\\.(jpg|jpeg|png|bmp|tiff)$\", filename):\n            cant=cant+1\n            filepath = os.path.join(root, filename)\n            image = plt.imread(filepath)\n            images.append(image)\n            b = \"Leyendo...\" + str(cant)\n            print (b, end=\"\\r\")\n            if prevRoot !=root:\n                print(root, cant)\n                prevRoot=root\n                directories.append(root)\n                dircount.append(cant)\n                cant=0\ndircount.append(cant)\n\ndircount = dircount[1:]\ndircount[0]=dircount[0]+1\nprint('Directorios leidos:',len(directories))\nprint(\"Imagenes en cada directorio\", dircount)\nprint('suma Total de imagenes en subdirs:',sum(dircount))\n\nleyendo imagenes de  /content/drive/MyDrive/DOCENCIA/ANN/sports/\n/content/drive/MyDrive/DOCENCIA/ANN/sports/ciclismo 1\n/content/drive/MyDrive/DOCENCIA/ANN/sports/f1 1000\n/content/drive/MyDrive/DOCENCIA/ANN/sports/futbol 1000\n/content/drive/MyDrive/DOCENCIA/ANN/sports/basket 1000\n/content/drive/MyDrive/DOCENCIA/ANN/sports/tenis 1000\nDirectorios leidos: 5\nImagenes en cada directorio [1001, 1000, 1000, 1000, 999]\nsuma Total de imagenes en subdirs: 5000\nEsta web está creada por Dante Conti y Sergi Ramírez, (c) 2024"
  },
  {
    "objectID": "material/ANN/Ejercicio_CNN_Deportes.html#imagenes-y-píxeles",
    "href": "material/ANN/Ejercicio_CNN_Deportes.html#imagenes-y-píxeles",
    "title": "Convolutional Neural Networks",
    "section": "",
    "text": "La red toma como entrada los pixeles de una imagen. Si tenemos una imagen con apenas 28×28 pixeles de alto y ancho, eso equivale a 784 neuronas. Y eso es si sólo tenemos 1 color (escala de grises). Si tuviéramos una imagen a color, necesitaríamos 3 canales (red, green, blue) y entonces usaríamos 28x28x3 = 2352 neuronas de entrada. Esa es nuestra capa de entrada.\n\n\n\nimagen\n\n\n\n\n\ndirname = os.path.join(os.getcwd(), 'drive/MyDrive/DOCENCIA/ANN/sports')\nimgpath = dirname + os.sep\n\n\nimages = []\ndirectories = []\ndircount = []\nprevRoot=''\ncant=0\n\nprint(\"leyendo imagenes de \",imgpath)\n\nfor root, dirnames, filenames in os.walk(imgpath):\n    for filename in filenames:\n        if re.search(\"\\.(jpg|jpeg|png|bmp|tiff)$\", filename):\n            cant=cant+1\n            filepath = os.path.join(root, filename)\n            image = plt.imread(filepath)\n            images.append(image)\n            b = \"Leyendo...\" + str(cant)\n            print (b, end=\"\\r\")\n            if prevRoot !=root:\n                print(root, cant)\n                prevRoot=root\n                directories.append(root)\n                dircount.append(cant)\n                cant=0\ndircount.append(cant)\n\ndircount = dircount[1:]\ndircount[0]=dircount[0]+1\nprint('Directorios leidos:',len(directories))\nprint(\"Imagenes en cada directorio\", dircount)\nprint('suma Total de imagenes en subdirs:',sum(dircount))\n\nleyendo imagenes de  /content/drive/MyDrive/DOCENCIA/ANN/sports/\n/content/drive/MyDrive/DOCENCIA/ANN/sports/ciclismo 1\n/content/drive/MyDrive/DOCENCIA/ANN/sports/f1 1000\n/content/drive/MyDrive/DOCENCIA/ANN/sports/futbol 1000\n/content/drive/MyDrive/DOCENCIA/ANN/sports/basket 1000\n/content/drive/MyDrive/DOCENCIA/ANN/sports/tenis 1000\nDirectorios leidos: 5\nImagenes en cada directorio [1001, 1000, 1000, 1000, 999]\nsuma Total de imagenes en subdirs: 5000"
  },
  {
    "objectID": "material/ANN/Ejercicio_CNN_Deportes.html#hacemos-el-one-hot-encoding-para-la-red",
    "href": "material/ANN/Ejercicio_CNN_Deportes.html#hacemos-el-one-hot-encoding-para-la-red",
    "title": "Convolutional Neural Networks",
    "section": "Hacemos el One-hot Encoding para la red",
    "text": "Hacemos el One-hot Encoding para la red\n\n# Change the labels from categorical to one-hot encoding\ntrain_Y_one_hot = to_categorical(train_Y)\ntest_Y_one_hot = to_categorical(test_Y)\n\n# Display the change for category label using one-hot encoding\nprint('Original label:', train_Y[0])\nprint('After conversion to one-hot:', train_Y_one_hot[0])\n\nOriginal label: 1\nAfter conversion to one-hot: [0. 1. 0. 0. 0.]"
  },
  {
    "objectID": "material/ANN/Ejercicio_CNN_Deportes.html#conjunto-de-kernels",
    "href": "material/ANN/Ejercicio_CNN_Deportes.html#conjunto-de-kernels",
    "title": "Convolutional Neural Networks",
    "section": "Conjunto de Kernels",
    "text": "Conjunto de Kernels\nCuando generamos nuestra matriz agregada, en realidad, no aplicaremos 1 sólo kernel, si no que tendremos muchos kernel (en su conjunto se llama filtros). Por ejemplo en esta primer convolución podríamos tener 32 filtros, con lo cual realmente obtendremos 32 matrices de salida (este conjunto se conoce como feature mapping), cada una de 28x28x1 dando un total del 25.088 neuronas para nuestra PRIMER CAPA OCULTA de neuronas.\n\n\nA medida que vamos desplazando el kernel y vamos obteniendo una nueva imagen filtrada por el kernel. En esta primer convolución y siguiendo con el ejemplo anterior, es como si obtuviéramos 32 imágenes filtradas nuevas. Estas imágenes nuevas lo que están “dibujando” son ciertas características de la imagen original. Esto ayudará en el futuro a poder distinguir un objeto de otro.\n\n\n\nimagen"
  },
  {
    "objectID": "material/ANN/Ejercicio_CNN_Deportes.html#max-pooling",
    "href": "material/ANN/Ejercicio_CNN_Deportes.html#max-pooling",
    "title": "Convolutional Neural Networks",
    "section": "Max-Pooling",
    "text": "Max-Pooling\nVamos a intentar explicarlo con un ejemplo: supongamos que haremos Max-pooling de tamaño 2×2. Esto quiere decir que recorreremos cada una de nuestras 32 imágenes de características obtenidas anteriormente de 28x28px de izquierda-derecha, arriba-abajo PERO en vez de tomar de a 1 pixel, tomaremos de “2×2” (2 de alto por 2 de ancho = 4 pixeles) e iremos preservando el valor “más alto” de entre esos 4 pixeles (por eso lo de “Max”). En este caso, usando 2×2, la imagen resultante es reducida “a la mitad”y quedará de 14×14 pixeles. Luego de este proceso de subsamplig nos quedarán 32 imágenes de 14×14, pasando de haber tenido 25.088 neuronas a 6.272, son bastantes menos y -en teoría- siguen almacenando la información más importante para detectar características deseadas.\n\n\n\nImagen\n\n\nMuy bien, pues esa ha sido una primer convolución: consiste de una entrada, un conjunto de filtros, generamos un mapa de características y hacemos un subsampling. Con lo cual, en el ejemplo de imágenes de 1 sólo color tendremos:\n\n\n\nImagen\n\n\nLa primer convolución es capaz de detectar características primitivas como lineas ó curvas. A medida que hagamos más capas con las convoluciones, los mapas de características serán capaces de reconocer formas más complejas, y el conjunto total de capas de convoluciones podrá ver.\nPues ahora deberemos hacer una Segunda convolución que será:\n\n\n\nImagen\n\n\nLa 3er convolución comenzará en tamaño 7×7 pixels y luego del max-pooling quedará en 3×3 con lo cual podríamos hacer sólo 1 convolución más. En este ejemplo empezamos con una imagen de 28x28px e hicimos 3 convoluciones. Si la imagen inicial hubiese sido mayor (de 224x224px) aún hubiéramos podido seguir haciendo convoluciones.\nLlegamos a la última convolución y nos queda el desenlace…\nPara terminar, tomaremos la última capa oculta a la que hicimos subsampling, que se dice que es tridimensional por tomar la forma -en nuestro ejemplo- 3x3x128 (alto,ancho,mapas) y la aplanamos, esto es que deja de ser tridimensional, y pasa a ser una capa de neuronas tradicionales, de las que ya conocíamos. Por ejemplo, podríamos aplanar (y conectar) a una nueva capa oculta de 100 neuronas feedforward.\n\n\n\nImagen\n\n\nEntonces, a esta nueva capa oculta tradicional, le aplicamos una función llamada Softmax que conecta contra la capa de salida final que tendrá la cantidad de neuronas correspondientes con las clases que estamos clasificando. Si clasificamos perros y gatos, serán 2 neuronas. Si es el dataset Mnist numérico serán 10 neuronas de salida. Si clasificamos coches, aviones ó barcos serán 3, etc.\nLas salidas al momento del entrenamiento tendrán el formato conocido como one-hot-encoding en el que para perros y gatos sera: [1,0] y [0,1], para coches, aviones ó barcos será [1,0,0]; [0,1,0];[0,0,1].\nY la función de Softmax se encarga de pasar a probabilidad (entre 0 y 1) a las neuronas de salida. Por ejemplo una salida [0,2 0,8] nos indica 20% probabilidades de que sea perro y 80% de que sea gato."
  },
  {
    "objectID": "material/ANN/Ejercicio_CNN_Deportes.html#backpropagation",
    "href": "material/ANN/Ejercicio_CNN_Deportes.html#backpropagation",
    "title": "Convolutional Neural Networks",
    "section": "Backpropagation",
    "text": "Backpropagation\nEl proceso es similar al de las redes tradicionales en las que tenemos una entrada y una salida esperada (por eso aprendizaje supervisado) y mediante el backpropagation mejoramos el valor de los pesos de las interconexiones entre capas de neuronas y a medida que iteramos esos pesos se ajustan hasta ser óptimos.\nEn el caso de la CNN, deberemos ajustar el valor de los pesos de los distintos kernels. Esto es una gran ventaja al momento del aprendizaje pues como vimos cada kernel es de un tamaño reducido, en nuestro ejemplo en la primer convolución es de tamaño de 3×3, eso son sólo 9 parámetros que debemos ajustar en 32 filtros dan un total de 288 parámetros. En comparación con los pesos entre dos capas de neuronas “tradicionales”: una de 748 y otra de 6272 en donde están TODAS interconectarlas con TODAS y eso equivaldría a tener que entrenar y ajustar más de 4,5 millones de pesos (repito: sólo para 1 capa)."
  },
  {
    "objectID": "material/ANN/Ejercicio_CNN_Deportes.html#arquitectura-básica",
    "href": "material/ANN/Ejercicio_CNN_Deportes.html#arquitectura-básica",
    "title": "Convolutional Neural Networks",
    "section": "Arquitectura básica",
    "text": "Arquitectura básica\nResumiendo: podemos decir que los elementos que usamos para crear CNNs son:\n\nEntrada: Serán los pixeles de la imagen. Serán alto, ancho y profundidad será 1 sólo color o 3 para Red,Green,Blue.\nCapa De Convolución: procesará la salida de neuronas que están conectadas en “regiones locales” de entrada (es decir pixeles cercanos), calculando el producto escalar entre sus pesos (valor de pixel) y una pequeña región a la que están conectados en el volumen de entrada. Aquí usaremos por ejemplo 32 filtros o la cantidad que decidamos y ese será el volumen de salida.\n“CAPA RELU” aplicará la función de activación en los elementos de la matriz.\nPOOL ó SUBSAMPLING: Hará una reducción en las dimensiones alto y ancho, pero se mantiene la profundidad.\nCAPA “TRADICIONAL” red de neuronas feedforward que conectará con la última capa de subsampling y finalizará con la cantidad de neuronas que queremos clasificar."
  },
  {
    "objectID": "material/ANN/Ejercicio_CNN_Deportes.html#declaración-de-parámetros",
    "href": "material/ANN/Ejercicio_CNN_Deportes.html#declaración-de-parámetros",
    "title": "Convolutional Neural Networks",
    "section": "Declaración de parámetros",
    "text": "Declaración de parámetros\n\n#declaramos variables con los parámetros de configuración de la red\nINIT_LR = 1e-3 # Valor inicial de learning rate. El valor 1e-3 corresponde con 0.001\nepochs = 10 # Cantidad de iteraciones completas al conjunto de imagenes de entrenamiento\nbatch_size = 64 # cantidad de imágenes que se toman a la vez en memoria"
  },
  {
    "objectID": "material/ANN/Ejercicio_CNN_Deportes.html#construcción-del-modelo",
    "href": "material/ANN/Ejercicio_CNN_Deportes.html#construcción-del-modelo",
    "title": "Convolutional Neural Networks",
    "section": "Construcción del modelo",
    "text": "Construcción del modelo\n\nsport_model = Sequential()\nsport_model.add(Input(shape = (21, 28, 3)))\nsport_model.add(Conv2D(32, kernel_size = (3, 3), activation = 'linear', padding = 'same'))\nsport_model.add(LeakyReLU(negative_slope = 0.1))\nsport_model.add(MaxPooling2D((2, 2), padding = 'same'))\nsport_model.add(Dropout(0.5))\nsport_model.add(Flatten())\nsport_model.add(Dense(32, activation = 'linear'))\nsport_model.add(LeakyReLU(negative_slope = 0.1))\nsport_model.add(Dropout(0.5))\nsport_model.add(Dense(nClasses, activation = 'softmax'))\n\n\nsport_model.summary()\n\nModel: \"sequential_1\"\n\n\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n┃ Layer (type)                         ┃ Output Shape                ┃         Param # ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n│ conv2d_1 (Conv2D)                    │ (None, 21, 28, 32)          │             896 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ leaky_re_lu_2 (LeakyReLU)            │ (None, 21, 28, 32)          │               0 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ max_pooling2d_1 (MaxPooling2D)       │ (None, 11, 14, 32)          │               0 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dropout_2 (Dropout)                  │ (None, 11, 14, 32)          │               0 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ flatten_1 (Flatten)                  │ (None, 4928)                │               0 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_2 (Dense)                      │ (None, 32)                  │         157,728 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ leaky_re_lu_3 (LeakyReLU)            │ (None, 32)                  │               0 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dropout_3 (Dropout)                  │ (None, 32)                  │               0 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_3 (Dense)                      │ (None, 5)                   │             165 │\n└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n\n\n\n Total params: 158,789 (620.27 KB)\n\n\n\n Trainable params: 158,789 (620.27 KB)\n\n\n\n Non-trainable params: 0 (0.00 B)\n\n\n\n\nfrom keras.utils import plot_model\n\nplot_model(sport_model, to_file='modelo_red_neuronal.png', show_shapes = True, show_layer_names = True)\n\n# Mostrar la imagen generada\nimg = plt.imread('modelo_red_neuronal.png')\nplt.imshow(img)\nplt.axis('off')  # Opcional: desactivar los ejes\nplt.show()\n\n\n\n\n\n\n\n\n\n# from tensorflow.keras.optimizers import Adagrad\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.optimizers.schedules import ExponentialDecay\n\n# Define un programador de tasa de aprendizaje\nlr_schedule = ExponentialDecay(\n    initial_learning_rate = INIT_LR,  # Tasa de aprendizaje inicial\n    decay_steps = 100,                # Número de pasos para aplicar el decaimiento\n    decay_rate = INIT_LR / 100,       # Factor de decaimiento (0.96 es un ejemplo)\n    staircase = False                 # `False` para un decaimiento continuo\n)\n\n\n# Compilamos el modelo\nsport_model.compile(loss = \"categorical_crossentropy\",\n                    optimizer = Adam(learning_rate = lr_schedule),\n                    metrics = ['accuracy'])"
  },
  {
    "objectID": "material/ANN/Ejercicio_CNN_Deportes.html#tensorflow",
    "href": "material/ANN/Ejercicio_CNN_Deportes.html#tensorflow",
    "title": "Convolutional Neural Networks",
    "section": "TENSORFLOW",
    "text": "TENSORFLOW\n\nimport tensorflow as tf\nfrom tensorflow.keras.callbacks import TensorBoard\n\n# Definir el callback de TensorBoard\ntensorboard_callback = TensorBoard(log_dir=\"./logs\", histogram_freq=1)\n\n\n# Compilar el modelo\nsport_model.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n\n# Entrenar el modelo con el callback\nsport_train = sport_model.fit(train_X, train_label, epochs = 10,\n                              callbacks = [tensorboard_callback],\n                              validation_data = (valid_X, valid_label))\n\nEpoch 1/10\n100/100 ━━━━━━━━━━━━━━━━━━━━ 4s 24ms/step - accuracy: 0.9707 - loss: 0.0519 - val_accuracy: 0.9900 - val_loss: 0.0153\nEpoch 2/10\n100/100 ━━━━━━━━━━━━━━━━━━━━ 4s 35ms/step - accuracy: 0.9760 - loss: 0.0437 - val_accuracy: 0.9875 - val_loss: 0.0167\nEpoch 3/10\n100/100 ━━━━━━━━━━━━━━━━━━━━ 3s 31ms/step - accuracy: 0.9758 - loss: 0.0464 - val_accuracy: 0.9912 - val_loss: 0.0142\nEpoch 4/10\n100/100 ━━━━━━━━━━━━━━━━━━━━ 2s 20ms/step - accuracy: 0.9744 - loss: 0.0399 - val_accuracy: 0.9900 - val_loss: 0.0144\nEpoch 5/10\n100/100 ━━━━━━━━━━━━━━━━━━━━ 2s 22ms/step - accuracy: 0.9780 - loss: 0.0379 - val_accuracy: 0.9912 - val_loss: 0.0127\nEpoch 6/10\n100/100 ━━━━━━━━━━━━━━━━━━━━ 2s 20ms/step - accuracy: 0.9818 - loss: 0.0367 - val_accuracy: 0.9937 - val_loss: 0.0125\nEpoch 7/10\n100/100 ━━━━━━━━━━━━━━━━━━━━ 2s 20ms/step - accuracy: 0.9846 - loss: 0.0299 - val_accuracy: 0.9950 - val_loss: 0.0110\nEpoch 8/10\n100/100 ━━━━━━━━━━━━━━━━━━━━ 4s 31ms/step - accuracy: 0.9848 - loss: 0.0332 - val_accuracy: 0.9912 - val_loss: 0.0138\nEpoch 9/10\n100/100 ━━━━━━━━━━━━━━━━━━━━ 4s 20ms/step - accuracy: 0.9890 - loss: 0.0269 - val_accuracy: 0.9925 - val_loss: 0.0112\nEpoch 10/10\n100/100 ━━━━━━━━━━━━━━━━━━━━ 2s 21ms/step - accuracy: 0.9879 - loss: 0.0291 - val_accuracy: 0.9937 - val_loss: 0.0117\n\n\n\n# Crear un callback de TensorBoard\nimport datetime\nlog_dir = \"/content/logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\ntensorboard_callback = TensorBoard(log_dir=log_dir, histogram_freq=1)\n\n\n# Iniciar TensorBoard en Colab\n%load_ext tensorboard\n%tensorboard --logdir /content/logs/fit\n\nThe tensorboard extension is already loaded. To reload it, use:\n  %reload_ext tensorboard\n\n\nReusing TensorBoard on port 6006 (pid 19935), started 0:01:09 ago. (Use '!kill 19935' to kill it.)"
  },
  {
    "objectID": "docs/material/ANN/01_DreamBooth_parte1.html",
    "href": "docs/material/ANN/01_DreamBooth_parte1.html",
    "title": "DREAMBOOTH 🤖",
    "section": "",
    "text": "%load_ext autoreload\n%autoreload 2\n\nThe autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload\nDreambooth es un modelo de generación de aprendizaje profundo, y que fue desarrollado en 2022 por un grupo de investigadores de Google Research y la Universidad de Boston. La misión de esta tecnología es la de poder entrenar a modelos de inteligencia artificial para personalizarlo según tus necesidades.\nEsta web está creada por Dante Conti y Sergi Ramírez, (c) 2024"
  },
  {
    "objectID": "docs/material/ANN/01_DreamBooth_parte1.html#como-funciona",
    "href": "docs/material/ANN/01_DreamBooth_parte1.html#como-funciona",
    "title": "DREAMBOOTH 🤖",
    "section": "¿Como funciona? 🔩",
    "text": "¿Como funciona? 🔩\nEl funcionamiento de esta técnica funciona en tres pasos.\n\nEn primer lugar, necesitas un modelo de difusión preentrenado, que es uno de esos sistemas de inteligencia artificial que pueden crear imágenes a partir de texto. Por ejemplo, se puede usar:\n\n\nStable Diffusion\nDALL-E\nMidjourney\n\nsiempre y cuando funcionen con el proceso de ruido y denoising.\nLo que hace esta técnica es crear una imagen completamente ruidosa, y luego ir quitando ese ruido reconstruyendo en el proceso una imagen totalmente original que se parezca a lo que le has pedido por texto. Pues es en este punto en el que Dreambooth ayudará con un modelo entrenado para que puedas obtener imágenes de sujetos concretos.\n\nel segundo paso, en el que necesitas un conjunto de imágenes del sujeto con el que quieres personalizar la IA. Puede ser un estilo, una cara, o lo que sea. Se recomienda tener un set de unas 8 o 10 imágenes como mínimo para poder entrenar el modelo.\n\nEntonces, lo que hace Dreambooth es utilizar este set de imágenes para entrenar al modelo de difusión, entrenar a la IA para que sepa reconocer lo que hay en ellas. Puede reconocer tu cara para luego poder dibujarla desde cero, así como un estilo o una posición.\n\nUna vez has usado Dreambooth para entrenar a la IA, este sistema usará las imágenes del sujeto como punto de partida para el proceso de crear la imagen aleatoria, permitiendo que la IA tenga más información sobre cómo es el sujeto que quieres dibujar, y que así pueda hacer imágenes que se parezcan a él.\n\n\n\n\nImagen"
  },
  {
    "objectID": "docs/material/ANN/01_DreamBooth_parte1.html#instalació-de-paquets",
    "href": "docs/material/ANN/01_DreamBooth_parte1.html#instalació-de-paquets",
    "title": "DREAMBOOTH 🤖",
    "section": "Instalació de paquets",
    "text": "Instalació de paquets\n\nInstal·leu el paquet de Python Py-Dreambooth tal com es mostra a continuació.\n\n\n!pip install -q py_dreambooth"
  },
  {
    "objectID": "docs/material/ANN/01_DreamBooth_parte1.html#importa-mòduls",
    "href": "docs/material/ANN/01_DreamBooth_parte1.html#importa-mòduls",
    "title": "DREAMBOOTH 🤖",
    "section": "Importa mòduls",
    "text": "Importa mòduls\n\nHi ha diversos tipus de classes de model, però estaràs utilitzant el model més bàsic, el model Stable Diffusion Dreambooth SDDreamboothModel, però no t’has de preocupar per això ara mateix. 🤷‍♂️\n\n\nfrom py_dreambooth.dataset import LocalDataset\nfrom py_dreambooth.model import SdDreamboothModel\nfrom py_dreambooth.predictor import LocalPredictor\nfrom py_dreambooth.trainer import LocalTrainer\nfrom py_dreambooth.utils.image_helpers import display_images\nfrom py_dreambooth.utils.prompt_helpers import make_prompt\n\nMontem la relació entre el google drive i el quadern de jupyter\n\nfrom google.colab import drive\ndrive.mount('/content/drive')\n\nDrive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True)."
  },
  {
    "objectID": "docs/material/ANN/01_DreamBooth_parte1.html#preparem-les-dades",
    "href": "docs/material/ANN/01_DreamBooth_parte1.html#preparem-les-dades",
    "title": "DREAMBOOTH 🤖",
    "section": "Preparem les dades 📸",
    "text": "Preparem les dades 📸\n\nDATA_DIR = \"/content/drive/MyDrive/ANN/DREAMBOOTH/data\"  # el directori amb fotos per a que el model s'entreni\nOUTPUT_DIR = \"/content/drive/MyDrive/ANN/DREAMBOOTH/models\"  # El directori on s'ubicaran els fitxers de model entrenats\n\ndataset = LocalDataset(DATA_DIR)\n\n\nMolt important! En el DATA_DIR definit anteriorment, posar les imatges (jpg o png) del subjecte que es vol entrenar.\nPer a aquesta tasca, necessitareu entre 10 i 20 solos, selfies d’alta qualitat preses amb diferents fons, il·luminació i expressions facials. Crec que es pot trobar un gran exemple al repositori de GitHub de Joe Penna.\n\n\n\n\nSamples\n\n\n\nUtilitzeu el mètode de processament d’imatges següent per retallar les imatges en un quadrat centrat a la cara. Si el subjecte que el model està tractant d’aprendre no és una persona (per exemple, un gos), estableix l’argument detect_face argumentant com a False.\n\n\ndataset = dataset.preprocess_images(detect_face=True)\n\nA total of 8 images were found.\n\n\n 38%|███▊      | 3/8 [00:00&lt;00:00,  6.17it/s]\n\n\nNo faces detected in the image '443008034_395930086752782_7217331932050061307_n.jpg'.\nNo faces detected in the image '440173417_1436258973657135_9081022692963550822_n.jpg'.\n\n\n 75%|███████▌  | 6/8 [00:00&lt;00:00,  6.41it/s]\n\n\nNo faces detected in the image '429164819_452647503759342_2302826312178258320_n.jpg'.\n\n\n100%|██████████| 8/8 [00:01&lt;00:00,  5.50it/s]\n\n\nA total of 5 images were preprocessed and stored in the path '/content/drive/MyDrive/ANN/DREAMBOOTH/data_preproc'."
  },
  {
    "objectID": "docs/material/ANN/01_DreamBooth_parte1.html#entrena-el-model",
    "href": "docs/material/ANN/01_DreamBooth_parte1.html#entrena-el-model",
    "title": "DREAMBOOTH 🤖",
    "section": "Entrena el model 🤖",
    "text": "Entrena el model 🤖\n\nAra és el moment d’entrenar el model! Digues al model el nom del subjecte al qual vols entrenar (p. ex., Joe) i la classe a la qual pertany.\nEn definir un model, un dels arguments importants és quantes iteracions entrenar, o max.train.steps. S’accepta generalment que 800 a 1200 passos són apropiats per a una persona, i 200 a 400 passos són apropiats per a un animal no humà. El valor per defecte és 100 vegades el nombre de fotos que teniu. No cal que us preocupeu per això ara mateix ,🤷‍♂️, però si no us agraden els resultats de la imatge generada a continuació, aquest és el primer paràmetre a ajustar.\n\n\nSUBJECT_NAME = \"mire\"  # The name of the subject you want to learn\nCLASS_NAME = \"person\"  # The class to which the subject you want to learn belongs\n\nmodel = SdDreamboothModel(\n    subject_name = SUBJECT_NAME,\n    class_name = CLASS_NAME,\n    max_train_steps=400,\n)\n\ntrainer = LocalTrainer(output_dir = OUTPUT_DIR)\n\n\nEl temps d’entrenament del model pot ser tan curt com unes poques desenes de minuts o com diverses hores.\n\n\n%%time\npredictor = trainer.fit(model, dataset)\n\nThe model training has begun.\n'max_train_steps' is set to 400.\n\n\n╭─────────────────────────────── Traceback (most recent call last) ────────────────────────────────╮\n│ /usr/local/lib/python3.10/dist-packages/IPython/core/magics/execution.py:1335 in time            │\n│                                                                                                  │\n│   1332 │   │   else:                                                                             │\n│   1333 │   │   │   st = clock2()                                                                 │\n│   1334 │   │   │   try:                                                                          │\n│ ❱ 1335 │   │   │   │   exec(code, glob, local_ns)                                                │\n│   1336 │   │   │   │   out=None                                                                  │\n│   1337 │   │   │   │   # multi-line %%time case                                                  │\n│   1338 │   │   │   │   if expr_val is not None:                                                  │\n│ in &lt;module&gt;:1                                                                                    │\n│                                                                                                  │\n│ /usr/local/lib/python3.10/dist-packages/py_dreambooth/trainer.py:124 in fit                      │\n│                                                                                                  │\n│   121 │   │   │   f\"The model training has begun.\\n'max_train_steps' is set to {max_train_step   │\n│   122 │   │   │   self.logger,                                                                   │\n│   123 │   │   )                                                                                  │\n│ ❱ 124 │   │   _ = subprocess.run(shlex.split(command), check=True)                               │\n│   125 │   │   log_or_print(\"The model training has ended.\", self.logger)                         │\n│   126 │   │                                                                                      │\n│   127 │   │   predictor = LocalPredictor(model, self.output_dir, self.logger)                    │\n│                                                                                                  │\n│ /usr/lib/python3.10/subprocess.py:526 in run                                                     │\n│                                                                                                  │\n│    523 │   │   │   raise                                                                         │\n│    524 │   │   retcode = process.poll()                                                          │\n│    525 │   │   if check and retcode:                                                             │\n│ ❱  526 │   │   │   raise CalledProcessError(retcode, process.args,                               │\n│    527 │   │   │   │   │   │   │   │   │    output=stdout, stderr=stderr)                        │\n│    528 │   return CompletedProcess(process.args, retcode, stdout, stderr)                        │\n│    529                                                                                           │\n╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\nCalledProcessError: Command '['accelerate', 'launch', \n'/usr/local/lib/python3.10/dist-packages/py_dreambooth/scripts/train/train_sd_dreambooth.py', \n'--pretrained_model_name_or_path', 'stabilityai/stable-diffusion-2-1-base', '--instance_data_dir', \n'/content/drive/MyDrive/ANN/DREAMBOOTH/data_preproc', '--instance_prompt', 'a photo of mire person', \n'--num_class_images', '200', '--output_dir', '/content/drive/MyDrive/ANN/DREAMBOOTH/models', '--resolution', '768',\n'--train_batch_size', '1', '--learning_rate', '2e-06', '--lr_scheduler', 'constant', '--lr_warmup_steps', '0', \n'--validation_prompt', 'a photo of mire person with Eiffel Tower in the background', '--compress_output', 'False', \n'--with_prior_preservation', 'True', '--prior_loss_weight', '1.0', '--class_prompt', 'a photo of person', \n'--train_text_encoder', 'True', '--max_train_steps', '400', '--gradient_accumulation_steps', '1', \n'--gradient_checkpointing', 'True', '--use_8bit_adam', 'True', '--enable_xformers_memory_efficient_attention', \n'True', '--mixed_precision', 'fp16', '--set_grads_to_none', 'True', '--report_to', 'tensorboard']' returned \nnon-zero exit status 1."
  },
  {
    "objectID": "docs/material/ANN/01_DreamBooth_parte1.html#bibliografia",
    "href": "docs/material/ANN/01_DreamBooth_parte1.html#bibliografia",
    "title": "DREAMBOOTH 🤖",
    "section": "Bibliografia 💃",
    "text": "Bibliografia 💃\n\nStable Diffusion\nDreamBooth\nPy-Dreambooth"
  },
  {
    "objectID": "material/ANN/Script_dreambooth.html",
    "href": "material/ANN/Script_dreambooth.html",
    "title": "Machine Learning para Data Scientist",
    "section": "",
    "text": "Instalamos las dependencias. Es posible que nos pida reiniciar el el propio colab! Saldrá un botón de reiniciar. Una vez hecho, volveis a cargar la linia y funcionará!\n\n!pip install py-dreambooth\n\nRequirement already satisfied: py-dreambooth in /usr/local/lib/python3.10/dist-packages (0.2.8)\nRequirement already satisfied: accelerate&gt;=0.23.0 in /usr/local/lib/python3.10/dist-packages (from py-dreambooth) (1.1.1)\nRequirement already satisfied: autocrop&gt;=1.3.0 in /usr/local/lib/python3.10/dist-packages (from py-dreambooth) (1.3.0)\nRequirement already satisfied: awscli&gt;=1.29.41 in /usr/local/lib/python3.10/dist-packages (from py-dreambooth) (1.36.20)\nRequirement already satisfied: bitsandbytes&gt;=0.41.0 in /usr/local/lib/python3.10/dist-packages (from py-dreambooth) (0.45.0)\nRequirement already satisfied: diffusers&gt;=0.24.0 in /usr/local/lib/python3.10/dist-packages (from py-dreambooth) (0.31.0)\nRequirement already satisfied: matplotlib&gt;=3.7.2 in /usr/local/lib/python3.10/dist-packages (from py-dreambooth) (3.8.0)\nRequirement already satisfied: peft&gt;=0.7.1 in /usr/local/lib/python3.10/dist-packages (from py-dreambooth) (0.13.2)\nRequirement already satisfied: pillow&gt;=9.4.0 in /usr/local/lib/python3.10/dist-packages (from py-dreambooth) (11.0.0)\nRequirement already satisfied: torch==2.0.1 in /usr/local/lib/python3.10/dist-packages (from py-dreambooth) (2.0.1)\nRequirement already satisfied: torchvision&gt;=0.15.2 in /usr/local/lib/python3.10/dist-packages (from py-dreambooth) (0.15.2)\nRequirement already satisfied: sagemaker&gt;=2.183.0 in /usr/local/lib/python3.10/dist-packages (from py-dreambooth) (2.237.0)\nRequirement already satisfied: tensorboard&gt;=2.13.0 in /usr/local/lib/python3.10/dist-packages (from py-dreambooth) (2.17.1)\nRequirement already satisfied: tqdm&gt;=4.65.0 in /usr/local/lib/python3.10/dist-packages (from py-dreambooth) (4.66.6)\nRequirement already satisfied: transformers&gt;=4.33.2 in /usr/local/lib/python3.10/dist-packages (from py-dreambooth) (4.46.3)\nRequirement already satisfied: wandb&gt;=0.15.11 in /usr/local/lib/python3.10/dist-packages (from py-dreambooth) (0.18.7)\nRequirement already satisfied: xformers&gt;=0.0.20 in /usr/local/lib/python3.10/dist-packages (from py-dreambooth) (0.0.22)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1-&gt;py-dreambooth) (3.16.1)\nRequirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1-&gt;py-dreambooth) (4.12.2)\nRequirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1-&gt;py-dreambooth) (1.13.1)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1-&gt;py-dreambooth) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1-&gt;py-dreambooth) (3.1.4)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1-&gt;py-dreambooth) (11.7.99)\nRequirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1-&gt;py-dreambooth) (11.7.99)\nRequirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1-&gt;py-dreambooth) (11.7.101)\nRequirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1-&gt;py-dreambooth) (8.5.0.96)\nRequirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1-&gt;py-dreambooth) (11.10.3.66)\nRequirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1-&gt;py-dreambooth) (10.9.0.58)\nRequirement already satisfied: nvidia-curand-cu11==10.2.10.91 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1-&gt;py-dreambooth) (10.2.10.91)\nRequirement already satisfied: nvidia-cusolver-cu11==11.4.0.1 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1-&gt;py-dreambooth) (11.4.0.1)\nRequirement already satisfied: nvidia-cusparse-cu11==11.7.4.91 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1-&gt;py-dreambooth) (11.7.4.91)\nRequirement already satisfied: nvidia-nccl-cu11==2.14.3 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1-&gt;py-dreambooth) (2.14.3)\nRequirement already satisfied: nvidia-nvtx-cu11==11.7.91 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1-&gt;py-dreambooth) (11.7.91)\nRequirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1-&gt;py-dreambooth) (2.0.0)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66-&gt;torch==2.0.1-&gt;py-dreambooth) (75.1.0)\nRequirement already satisfied: wheel in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66-&gt;torch==2.0.1-&gt;py-dreambooth) (0.45.1)\nRequirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0-&gt;torch==2.0.1-&gt;py-dreambooth) (3.30.5)\nRequirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0-&gt;torch==2.0.1-&gt;py-dreambooth) (18.1.8)\nRequirement already satisfied: huggingface-hub&gt;=0.21.0 in /usr/local/lib/python3.10/dist-packages (from accelerate&gt;=0.23.0-&gt;py-dreambooth) (0.26.3)\nRequirement already satisfied: numpy&lt;3.0.0,&gt;=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate&gt;=0.23.0-&gt;py-dreambooth) (1.26.4)\nRequirement already satisfied: packaging&gt;=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate&gt;=0.23.0-&gt;py-dreambooth) (24.2)\nRequirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate&gt;=0.23.0-&gt;py-dreambooth) (5.9.5)\nRequirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate&gt;=0.23.0-&gt;py-dreambooth) (6.0.2)\nRequirement already satisfied: safetensors&gt;=0.4.3 in /usr/local/lib/python3.10/dist-packages (from accelerate&gt;=0.23.0-&gt;py-dreambooth) (0.4.5)\nRequirement already satisfied: opencv-python-headless&lt;5,&gt;=3 in /usr/local/lib/python3.10/dist-packages (from autocrop&gt;=1.3.0-&gt;py-dreambooth) (4.10.0.84)\nRequirement already satisfied: botocore==1.35.79 in /usr/local/lib/python3.10/dist-packages (from awscli&gt;=1.29.41-&gt;py-dreambooth) (1.35.79)\nRequirement already satisfied: docutils&lt;0.17,&gt;=0.10 in /usr/local/lib/python3.10/dist-packages (from awscli&gt;=1.29.41-&gt;py-dreambooth) (0.16)\nRequirement already satisfied: s3transfer&lt;0.11.0,&gt;=0.10.0 in /usr/local/lib/python3.10/dist-packages (from awscli&gt;=1.29.41-&gt;py-dreambooth) (0.10.4)\nRequirement already satisfied: colorama&lt;0.4.7,&gt;=0.2.5 in /usr/local/lib/python3.10/dist-packages (from awscli&gt;=1.29.41-&gt;py-dreambooth) (0.4.6)\nRequirement already satisfied: rsa&lt;4.8,&gt;=3.1.2 in /usr/local/lib/python3.10/dist-packages (from awscli&gt;=1.29.41-&gt;py-dreambooth) (4.7.2)\nRequirement already satisfied: jmespath&lt;2.0.0,&gt;=0.7.1 in /usr/local/lib/python3.10/dist-packages (from botocore==1.35.79-&gt;awscli&gt;=1.29.41-&gt;py-dreambooth) (1.0.1)\nRequirement already satisfied: python-dateutil&lt;3.0.0,&gt;=2.1 in /usr/local/lib/python3.10/dist-packages (from botocore==1.35.79-&gt;awscli&gt;=1.29.41-&gt;py-dreambooth) (2.8.2)\nRequirement already satisfied: urllib3!=2.2.0,&lt;3,&gt;=1.25.4 in /usr/local/lib/python3.10/dist-packages (from botocore==1.35.79-&gt;awscli&gt;=1.29.41-&gt;py-dreambooth) (2.2.3)\nRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.10/dist-packages (from diffusers&gt;=0.24.0-&gt;py-dreambooth) (6.11.0)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from diffusers&gt;=0.24.0-&gt;py-dreambooth) (2024.9.11)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from diffusers&gt;=0.24.0-&gt;py-dreambooth) (2.32.3)\nRequirement already satisfied: contourpy&gt;=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib&gt;=3.7.2-&gt;py-dreambooth) (1.3.1)\nRequirement already satisfied: cycler&gt;=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib&gt;=3.7.2-&gt;py-dreambooth) (0.12.1)\nRequirement already satisfied: fonttools&gt;=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib&gt;=3.7.2-&gt;py-dreambooth) (4.55.1)\nRequirement already satisfied: kiwisolver&gt;=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib&gt;=3.7.2-&gt;py-dreambooth) (1.4.7)\nRequirement already satisfied: pyparsing&gt;=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib&gt;=3.7.2-&gt;py-dreambooth) (3.2.0)\nRequirement already satisfied: attrs&lt;24,&gt;=23.1.0 in /usr/local/lib/python3.10/dist-packages (from sagemaker&gt;=2.183.0-&gt;py-dreambooth) (23.2.0)\nRequirement already satisfied: boto3&lt;2.0,&gt;=1.35.75 in /usr/local/lib/python3.10/dist-packages (from sagemaker&gt;=2.183.0-&gt;py-dreambooth) (1.35.79)\nRequirement already satisfied: cloudpickle==2.2.1 in /usr/local/lib/python3.10/dist-packages (from sagemaker&gt;=2.183.0-&gt;py-dreambooth) (2.2.1)\nRequirement already satisfied: docker in /usr/local/lib/python3.10/dist-packages (from sagemaker&gt;=2.183.0-&gt;py-dreambooth) (7.1.0)\nRequirement already satisfied: fastapi in /usr/local/lib/python3.10/dist-packages (from sagemaker&gt;=2.183.0-&gt;py-dreambooth) (0.115.6)\nRequirement already satisfied: google-pasta in /usr/local/lib/python3.10/dist-packages (from sagemaker&gt;=2.183.0-&gt;py-dreambooth) (0.2.0)\nRequirement already satisfied: jsonschema in /usr/local/lib/python3.10/dist-packages (from sagemaker&gt;=2.183.0-&gt;py-dreambooth) (4.23.0)\nRequirement already satisfied: omegaconf&lt;2.3,&gt;=2.2 in /usr/local/lib/python3.10/dist-packages (from sagemaker&gt;=2.183.0-&gt;py-dreambooth) (2.2.3)\nRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from sagemaker&gt;=2.183.0-&gt;py-dreambooth) (2.2.2)\nRequirement already satisfied: pathos in /usr/local/lib/python3.10/dist-packages (from sagemaker&gt;=2.183.0-&gt;py-dreambooth) (0.3.3)\nRequirement already satisfied: platformdirs in /usr/local/lib/python3.10/dist-packages (from sagemaker&gt;=2.183.0-&gt;py-dreambooth) (4.3.6)\nRequirement already satisfied: protobuf&lt;5.0,&gt;=3.12 in /usr/local/lib/python3.10/dist-packages (from sagemaker&gt;=2.183.0-&gt;py-dreambooth) (4.25.5)\nRequirement already satisfied: sagemaker-core&lt;2.0.0,&gt;=1.0.17 in /usr/local/lib/python3.10/dist-packages (from sagemaker&gt;=2.183.0-&gt;py-dreambooth) (1.0.17)\nRequirement already satisfied: schema in /usr/local/lib/python3.10/dist-packages (from sagemaker&gt;=2.183.0-&gt;py-dreambooth) (0.7.7)\nRequirement already satisfied: smdebug-rulesconfig==1.0.1 in /usr/local/lib/python3.10/dist-packages (from sagemaker&gt;=2.183.0-&gt;py-dreambooth) (1.0.1)\nRequirement already satisfied: tblib&lt;4,&gt;=1.7.0 in /usr/local/lib/python3.10/dist-packages (from sagemaker&gt;=2.183.0-&gt;py-dreambooth) (3.0.0)\nRequirement already satisfied: uvicorn in /usr/local/lib/python3.10/dist-packages (from sagemaker&gt;=2.183.0-&gt;py-dreambooth) (0.32.1)\nRequirement already satisfied: absl-py&gt;=0.4 in /usr/local/lib/python3.10/dist-packages (from tensorboard&gt;=2.13.0-&gt;py-dreambooth) (1.4.0)\nRequirement already satisfied: grpcio&gt;=1.48.2 in /usr/local/lib/python3.10/dist-packages (from tensorboard&gt;=2.13.0-&gt;py-dreambooth) (1.68.1)\nRequirement already satisfied: markdown&gt;=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard&gt;=2.13.0-&gt;py-dreambooth) (3.7)\nRequirement already satisfied: six&gt;1.9 in /usr/local/lib/python3.10/dist-packages (from tensorboard&gt;=2.13.0-&gt;py-dreambooth) (1.16.0)\nRequirement already satisfied: tensorboard-data-server&lt;0.8.0,&gt;=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard&gt;=2.13.0-&gt;py-dreambooth) (0.7.2)\nRequirement already satisfied: werkzeug&gt;=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard&gt;=2.13.0-&gt;py-dreambooth) (3.1.3)\nRequirement already satisfied: tokenizers&lt;0.21,&gt;=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers&gt;=4.33.2-&gt;py-dreambooth) (0.20.3)\nRequirement already satisfied: click!=8.0.0,&gt;=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb&gt;=0.15.11-&gt;py-dreambooth) (8.1.7)\nRequirement already satisfied: docker-pycreds&gt;=0.4.0 in /usr/local/lib/python3.10/dist-packages (from wandb&gt;=0.15.11-&gt;py-dreambooth) (0.4.0)\nRequirement already satisfied: gitpython!=3.1.29,&gt;=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb&gt;=0.15.11-&gt;py-dreambooth) (3.1.43)\nRequirement already satisfied: sentry-sdk&gt;=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb&gt;=0.15.11-&gt;py-dreambooth) (2.19.0)\nRequirement already satisfied: setproctitle in /usr/local/lib/python3.10/dist-packages (from wandb&gt;=0.15.11-&gt;py-dreambooth) (1.3.4)\nRequirement already satisfied: gitdb&lt;5,&gt;=4.0.1 in /usr/local/lib/python3.10/dist-packages (from gitpython!=3.1.29,&gt;=1.0.0-&gt;wandb&gt;=0.15.11-&gt;py-dreambooth) (4.0.11)\nRequirement already satisfied: fsspec&gt;=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub&gt;=0.21.0-&gt;accelerate&gt;=0.23.0-&gt;py-dreambooth) (2024.10.0)\nRequirement already satisfied: zipp&gt;=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata-&gt;diffusers&gt;=0.24.0-&gt;py-dreambooth) (3.21.0)\nRequirement already satisfied: antlr4-python3-runtime==4.9.* in /usr/local/lib/python3.10/dist-packages (from omegaconf&lt;2.3,&gt;=2.2-&gt;sagemaker&gt;=2.183.0-&gt;py-dreambooth) (4.9.3)\nRequirement already satisfied: charset-normalizer&lt;4,&gt;=2 in /usr/local/lib/python3.10/dist-packages (from requests-&gt;diffusers&gt;=0.24.0-&gt;py-dreambooth) (3.4.0)\nRequirement already satisfied: idna&lt;4,&gt;=2.5 in /usr/local/lib/python3.10/dist-packages (from requests-&gt;diffusers&gt;=0.24.0-&gt;py-dreambooth) (3.10)\nRequirement already satisfied: certifi&gt;=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests-&gt;diffusers&gt;=0.24.0-&gt;py-dreambooth) (2024.8.30)\nRequirement already satisfied: pyasn1&gt;=0.1.3 in /usr/local/lib/python3.10/dist-packages (from rsa&lt;4.8,&gt;=3.1.2-&gt;awscli&gt;=1.29.41-&gt;py-dreambooth) (0.6.1)\nRequirement already satisfied: pydantic&lt;3.0.0,&gt;=2.0.0 in /usr/local/lib/python3.10/dist-packages (from sagemaker-core&lt;2.0.0,&gt;=1.0.17-&gt;sagemaker&gt;=2.183.0-&gt;py-dreambooth) (2.10.3)\nRequirement already satisfied: rich&lt;14.0.0,&gt;=13.0.0 in /usr/local/lib/python3.10/dist-packages (from sagemaker-core&lt;2.0.0,&gt;=1.0.17-&gt;sagemaker&gt;=2.183.0-&gt;py-dreambooth) (13.9.4)\nRequirement already satisfied: mock&lt;5.0,&gt;4.0 in /usr/local/lib/python3.10/dist-packages (from sagemaker-core&lt;2.0.0,&gt;=1.0.17-&gt;sagemaker&gt;=2.183.0-&gt;py-dreambooth) (4.0.3)\nRequirement already satisfied: jsonschema-specifications&gt;=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema-&gt;sagemaker&gt;=2.183.0-&gt;py-dreambooth) (2024.10.1)\nRequirement already satisfied: referencing&gt;=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema-&gt;sagemaker&gt;=2.183.0-&gt;py-dreambooth) (0.35.1)\nRequirement already satisfied: rpds-py&gt;=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema-&gt;sagemaker&gt;=2.183.0-&gt;py-dreambooth) (0.22.3)\nRequirement already satisfied: MarkupSafe&gt;=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug&gt;=1.0.1-&gt;tensorboard&gt;=2.13.0-&gt;py-dreambooth) (3.0.2)\nRequirement already satisfied: starlette&lt;0.42.0,&gt;=0.40.0 in /usr/local/lib/python3.10/dist-packages (from fastapi-&gt;sagemaker&gt;=2.183.0-&gt;py-dreambooth) (0.41.3)\nRequirement already satisfied: pytz&gt;=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas-&gt;sagemaker&gt;=2.183.0-&gt;py-dreambooth) (2024.2)\nRequirement already satisfied: tzdata&gt;=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas-&gt;sagemaker&gt;=2.183.0-&gt;py-dreambooth) (2024.2)\nRequirement already satisfied: ppft&gt;=1.7.6.9 in /usr/local/lib/python3.10/dist-packages (from pathos-&gt;sagemaker&gt;=2.183.0-&gt;py-dreambooth) (1.7.6.9)\nRequirement already satisfied: dill&gt;=0.3.9 in /usr/local/lib/python3.10/dist-packages (from pathos-&gt;sagemaker&gt;=2.183.0-&gt;py-dreambooth) (0.3.9)\nRequirement already satisfied: pox&gt;=0.3.5 in /usr/local/lib/python3.10/dist-packages (from pathos-&gt;sagemaker&gt;=2.183.0-&gt;py-dreambooth) (0.3.5)\nRequirement already satisfied: multiprocess&gt;=0.70.17 in /usr/local/lib/python3.10/dist-packages (from pathos-&gt;sagemaker&gt;=2.183.0-&gt;py-dreambooth) (0.70.17)\nRequirement already satisfied: mpmath&lt;1.4,&gt;=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy-&gt;torch==2.0.1-&gt;py-dreambooth) (1.3.0)\nRequirement already satisfied: h11&gt;=0.8 in /usr/local/lib/python3.10/dist-packages (from uvicorn-&gt;sagemaker&gt;=2.183.0-&gt;py-dreambooth) (0.14.0)\nRequirement already satisfied: smmap&lt;6,&gt;=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb&lt;5,&gt;=4.0.1-&gt;gitpython!=3.1.29,&gt;=1.0.0-&gt;wandb&gt;=0.15.11-&gt;py-dreambooth) (5.0.1)\nRequirement already satisfied: annotated-types&gt;=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic&lt;3.0.0,&gt;=2.0.0-&gt;sagemaker-core&lt;2.0.0,&gt;=1.0.17-&gt;sagemaker&gt;=2.183.0-&gt;py-dreambooth) (0.7.0)\nRequirement already satisfied: pydantic-core==2.27.1 in /usr/local/lib/python3.10/dist-packages (from pydantic&lt;3.0.0,&gt;=2.0.0-&gt;sagemaker-core&lt;2.0.0,&gt;=1.0.17-&gt;sagemaker&gt;=2.183.0-&gt;py-dreambooth) (2.27.1)\nRequirement already satisfied: markdown-it-py&gt;=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich&lt;14.0.0,&gt;=13.0.0-&gt;sagemaker-core&lt;2.0.0,&gt;=1.0.17-&gt;sagemaker&gt;=2.183.0-&gt;py-dreambooth) (3.0.0)\nRequirement already satisfied: pygments&lt;3.0.0,&gt;=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich&lt;14.0.0,&gt;=13.0.0-&gt;sagemaker-core&lt;2.0.0,&gt;=1.0.17-&gt;sagemaker&gt;=2.183.0-&gt;py-dreambooth) (2.18.0)\nRequirement already satisfied: anyio&lt;5,&gt;=3.4.0 in /usr/local/lib/python3.10/dist-packages (from starlette&lt;0.42.0,&gt;=0.40.0-&gt;fastapi-&gt;sagemaker&gt;=2.183.0-&gt;py-dreambooth) (3.7.1)\nRequirement already satisfied: sniffio&gt;=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio&lt;5,&gt;=3.4.0-&gt;starlette&lt;0.42.0,&gt;=0.40.0-&gt;fastapi-&gt;sagemaker&gt;=2.183.0-&gt;py-dreambooth) (1.3.1)\nRequirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio&lt;5,&gt;=3.4.0-&gt;starlette&lt;0.42.0,&gt;=0.40.0-&gt;fastapi-&gt;sagemaker&gt;=2.183.0-&gt;py-dreambooth) (1.2.2)\nRequirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py&gt;=2.2.0-&gt;rich&lt;14.0.0,&gt;=13.0.0-&gt;sagemaker-core&lt;2.0.0,&gt;=1.0.17-&gt;sagemaker&gt;=2.183.0-&gt;py-dreambooth) (0.1.2)\n\n\nIndicamos el directorio de nuestros datos y donde queremos que se guarde nuestro modelo. Necesario crear la carpeta de las imágenes si no la tenemos creada.\n\nfrom py_dreambooth.dataset import LocalDataset\nfrom py_dreambooth.model import SdDreamboothModel\nfrom py_dreambooth.trainer import LocalTrainer\nfrom py_dreambooth.utils.image_helpers import display_images\nfrom py_dreambooth.utils.prompt_helpers import make_prompt\n\nDATA_DIR = \"data\"  # The directory where you put your prepared photos\nOUTPUT_DIR = \"models\"\n\n'dataset = LocalDataset(DATA_DIR)\\ndataset = dataset.preprocess_images(detect_face=True)\\n\\nSUBJECT_NAME = \"&lt;YOUR-NAME&gt;\"  \\nCLASS_NAME = \"person\"\\n\\nmodel = SdDreamboothModel(subject_name=SUBJECT_NAME, class_name=CLASS_NAME)\\ntrainer = LocalTrainer(output_dir=OUTPUT_DIR)\\n\\npredictor = trainer.fit(model, dataset)\\n\\n# Use the prompt helper to create an awesome AI avatar!\\nprompt = next(make_prompt(SUBJECT_NAME, CLASS_NAME))\\n\\nimages = predictor.predict(\\n    prompt, height=768, width=512, num_images_per_prompt=2,\\n)\\n\\ndisplay_images(images, fig_size=10)'\n\n\nCreamos el dataset e indicamos nuestro nombre\n\ndataset = LocalDataset(DATA_DIR)\ndataset = dataset.preprocess_images(detect_face=True)\n\nSUBJECT_NAME = \"&lt;YOUR-NAME&gt;\"  #Incluir tu nombre sin los &lt;&gt;!\nCLASS_NAME = \"person\"\n\nA total of 8 images were found.\n\n\n100%|██████████| 8/8 [00:00&lt;00:00, 10.87it/s]\n\n\nA total of 8 images were preprocessed and stored in the path 'data_preproc'.\n\n\n\n\n\nCreamos el modelo y lo entrenamos con nuestros datos\nSi se aumenta el parámetro de max_train_steps tardará más el entrenamiento (50 son 12 min aprox)\n\nmodel = SdDreamboothModel(subject_name=SUBJECT_NAME, class_name=CLASS_NAME, max_train_steps=20)\ntrainer = LocalTrainer(output_dir=OUTPUT_DIR)\n\npredictor = trainer.fit(model, dataset)\n\nThe model training has begun.\n'max_train_steps' is set to 50.\nThe model training has ended.\n\n\n\n\n\nThe model has loaded from the directory, 'models'.\n\n\nCreamos el prompt i creamos las imágenes con nuestro modelo ya entrenado\n\n#prompt = next(make_prompt(SUBJECT_NAME, CLASS_NAME)) #Es para crear un prompt random!!\n\nprompt = f\"A hyper-realistic and stunning depiction of {SUBJECT_NAME} {CLASS_NAME}, capturing the person's charisma and charm, trending on Behance, intricate textures, vivid color palette, reminiscent of Alex Ross and Norman Rockwell\"\n\nimages = predictor.predict(\n    prompt, height=768, width=512, num_images_per_prompt=2,\n)\n\ndisplay_images(images, fig_size=10)\n\nWarning: the subject and class names are not included in the prompt.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEsta web está creada por Dante Conti y Sergi Ramírez, (c) 2024"
  },
  {
    "objectID": "material/FactorialMethods/ACP.html",
    "href": "material/FactorialMethods/ACP.html",
    "title": "Anàlisis de Components Principals (ACP)",
    "section": "",
    "text": "El Análisis de Componentes Principales (ACP) es una técnica de reducción de dimensionalidad que transforma un conjunto de variables posiblemente correlacionadas en un conjunto más pequeño de variables no correlacionadas llamadas componentes principales (CPs).\nObjetivos típicos:\n\nCapturar la mayor varianza con el menor número de componentes.\n\nDescorrelacionar variables y simplificar la estructura.\n\nFacilitar visualización, preprocesamiento para modelos y compresión.\n\n\nIntuición: el ACP encuentra direcciones (vectores) en el espacio de los datos que maximizan la varianza. Esas direcciones son los autovectores de la matriz de covarianzas (o correlaciones), y la varianza capturada por cada componente viene dada por su autovalor.\nEsta web está creada por Dante Conti y Sergi Ramírez, (c) 2025"
  },
  {
    "objectID": "material/FactorialMethods/ACP.html#introducció",
    "href": "material/FactorialMethods/ACP.html#introducció",
    "title": "Anàlisis de Components Principals (ACP)",
    "section": "",
    "text": "L’anàlisi de correspondencies simples (ACS) s’utilitza per a descriure taules de contingència 1 (TC) mitjançant la representació geomètrica de les taules de condicionals fila i columna (perfils) derivades d’aquelles.\nL’objectiu de l’ACS és descriure les associacions entre les variables fila i columna, a través dels seus perfils:\n\nComparar els perfilis fila.\nComparar els perfilis columna.\nEstudiar les correspondències entre perfils fila i columna\n\nLa metodologia la va desenvolupar Benzecri, a principis dels anys 60 del segle XX en la Universitat de Renner (França). En essència. és un tipus especial d’anàlisi de components principals però realitzat sobre una taula de contingència i usant una distància euclidiana ponderada anomenada chi-quadrat (\\(\\chi^{2}\\))\n\n\nLa prova de chi-quadrat (\\(\\chi^{2}\\)) és un mètode estadístic que s’utilitza per a determinar si existeix una associació significativa entre variables categòriques comparant les freqüències observades i esperades en una taula de contingència.\nH0 : No hi ha associació significativa entre les variables.\nH1 : Hi ha una associació significativa entre les variables.\nPer a realitzar la prova chi-quadrat (\\(\\chi^{2}\\)):\n1- Crear una taula de contingència amb les freqüències observades per a cada categoria.\n2- Calcular les freqüències esperades assumint la independència entre les variables.\n3- Calcular l’estadístic chi-quadrat (\\(\\chi^{2}\\)).\nComparar l’estadístic calculat amb el valor crític de la distribució chi-quadrat (\\(\\chi^{2}\\)) per a determinar si es rebutja o no la hipòtesi nul·la.\n\\[\n  \\chi^{2} = \\sum{\\frac{(O_{ij}-E_{ij})^{2}}{E_{ij}}}\n\\] Ón:\n\n\\(chi-quadrat (\\)^{2}\\()\\): El estadístic de prova \\(\\chi^{2}\\), medeix la discrepancia entre els valors observats i els esperats\n\\(\\sum\\) (sigma): Suma els valors de cada cela de la taula de contingència.\n\\(O_{ij}\\): La freqüència observada en cada cel·la de la taula de contingència.\n\\(E_{ij}\\): La freqüència esperada en cada cel·la de la taula de contingència.\n\nCasos d’ús d’exemple\n\nAvaluar la relació entre les variables demogràfiques (per exemple, edat, gènere, ingressos) i les preferències del consumidor o el comportament de compra.\nExaminar l’associació entre els factors de risc i els resultats de les malalties, com el tabaquisme i la incidència el càncer de pulmó.\nExplorar la relació entre variables categòriques com el nivell educatiu i la situació laboral o l’afiliació política i el comportament electoral.\nDeterminar si els patrons d’herència observats són consistents amb les proporcions mendelianes esperades, o si uns certs marcadors genètics estan associats amb trets o malalties específiques.\nAvaluar la relació entre les variables de control de qualitat, com el tipus de defecte, i la línia de producció.\n\nSuposicions\n\nIndependència: L’ocurrència d’una observació no ha d’influir ni ser influenciada per una altra observació.\nCategòric: Totes dues variables són per a dades categòriques.\nMútuament excloents: Les observacions només poden pertànyer a una cel·la de la taula de contingència.\nGrandària de la mostra: Ha d’haver-hi almenys cinc observacions en cada cel·la de la taula de contingència.\n\nProves alternatives\nProva exacta de Fisher: adequada quan la grandària de la mostra és petit i les freqüències de cel·la esperades en la taula de contingència són inferiors a 5. Sovint s’utilitza com a alternativa a la prova de chi-quadrat en taules de contingència de 2x2.\nProva de McNemar: s’utilitza en analitzar dades categòriques aparellades, generalment en una taula de contingència de 2x2, on les observacions són dependents o estan relacionades. S’utilitza comunament en estudis d’abans i després o en estudis de casos i controls aparellats.\nProva de Cochran-Estovalles-Haenszel: s’utilitza en analitzar dades categòriques en estudis estratificats o aparellats. Permet la comparació de múltiples taules de contingència 2x2 mentre controla variables de confusió o factors d’estratificació.\n\nlibrary(\"factoextra\")\nlibrary(\"FactoMineR\")\nlibrary(\"gplots\")\nlibrary(\"dplyr\")"
  },
  {
    "objectID": "material/FactorialMethods/ACP.html#definició-del-problema",
    "href": "material/FactorialMethods/ACP.html#definició-del-problema",
    "title": "Anàlisis de Components Principals (ACP)",
    "section": "2 Definició del problema",
    "text": "2 Definició del problema\n\ndata(\"housetasks\")\nhead(housetasks)\n\n           Wife Alternating Husband Jointly\nLaundry     156          14       2       4\nMain_meal   124          20       5       4\nDinner       77          11       7      13\nBreakfeast   82          36      15       7\nTidying      53          11       1      57\nDishes       32          24       4      53\n\ncolnames(housetasks) &lt;- c(\"Dona\", \"Alternant\", \"Marit\", \"Conjuntament\")\nrownames(housetasks) &lt;- c(\"Bugaderia\", \"Dinar\", \"Sopar\", \"Esmorçar\", \"Ordenar\", \"Netejar_Plats\", \"Compres\", \"Oficial\", \"Conduir\", \"Finances\", \"Assegurança\", \"Reparacions\", \"Vacances\")\n\ndf &lt;- as.table(as.matrix(housetasks))\ndf\n\n              Dona Alternant Marit Conjuntament\nBugaderia      156        14     2            4\nDinar          124        20     5            4\nSopar           77        11     7           13\nEsmorçar        82        36    15            7\nOrdenar         53        11     1           57\nNetejar_Plats   32        24     4           53\nCompres         33        23     9           55\nOficial         12        46    23           15\nConduir         10        51    75            3\nFinances        13        13    21           66\nAssegurança      8         1    53           77\nReparacions      0         3   160            2\nVacances         0         1     6          153\n\n\n\nballoonplot(t(df), label=F, main=\"Tareas del hogar\")"
  },
  {
    "objectID": "material/FactorialMethods/ACP.html#prova-de-la-chi2",
    "href": "material/FactorialMethods/ACP.html#prova-de-la-chi2",
    "title": "Anàlisis de Components Principals (ACP)",
    "section": "3 Prova de la \\(\\chi^{2}\\)",
    "text": "3 Prova de la \\(\\chi^{2}\\)\n\\(H_{0}\\): Variables independents (hipòtesi nula) \\(H_{1}\\): Variables dependents (hipòtesi alternativa)\n\nchisq.test(housetasks)\n\n\n    Pearson's Chi-squared test\n\ndata:  housetasks\nX-squared = 1944.5, df = 36, p-value &lt; 2.2e-16\n\n\nEs refusa l’hipòtesi nula en favor de la alternativa, les parelles s’organitzen per fer les tasques de la llar."
  },
  {
    "objectID": "material/FactorialMethods/ACP.html#anàlisi-de-correspondència-simple-acs",
    "href": "material/FactorialMethods/ACP.html#anàlisi-de-correspondència-simple-acs",
    "title": "Anàlisis de Components Principals (ACP)",
    "section": "4 Anàlisi de Correspondència Simple (ACS)",
    "text": "4 Anàlisi de Correspondència Simple (ACS)\n\nhousetasks_CA &lt;- CA(housetasks, graph = F)\nprint(housetasks_CA)\n\n**Results of the Correspondence Analysis (CA)**\nThe row variable has  13  categories; the column variable has 4 categories\nThe chi square of independence between the two variables is equal to 1944.456 (p-value =  0 ).\n*The results are available in the following objects:\n\n   name              description                   \n1  \"$eig\"            \"eigenvalues\"                 \n2  \"$col\"            \"results for the columns\"     \n3  \"$col$coord\"      \"coord. for the columns\"      \n4  \"$col$cos2\"       \"cos2 for the columns\"        \n5  \"$col$contrib\"    \"contributions of the columns\"\n6  \"$row\"            \"results for the rows\"        \n7  \"$row$coord\"      \"coord. for the rows\"         \n8  \"$row$cos2\"       \"cos2 for the rows\"           \n9  \"$row$contrib\"    \"contributions of the rows\"   \n10 \"$call\"           \"summary called parameters\"   \n11 \"$call$marge.col\" \"weights of the columns\"      \n12 \"$call$marge.row\" \"weights of the rows\"         \n\n\n\nhousetasks_CA$col\n\n$coord\n                   Dim 1      Dim 2       Dim 3\nDona         -0.83762154  0.3652207 -0.19991139\nAlternant    -0.06218462  0.2915938  0.84858939\nMarit         1.16091847  0.6019199 -0.18885924\nConjuntament  0.14942609 -1.0265791 -0.04644302\n\n$contrib\n                 Dim 1     Dim 2      Dim 3\nDona         44.462018 10.312237 10.8220753\nAlternant     0.103739  2.782794 82.5492464\nMarit        54.233879 17.786612  6.1331792\nConjuntament  1.200364 69.118357  0.4954991\n\n$cos2\n                   Dim 1     Dim 2       Dim 3\nDona         0.801875947 0.1524482 0.045675847\nAlternant    0.004779897 0.1051016 0.890118521\nMarit        0.772026244 0.2075420 0.020431728\nConjuntament 0.020705858 0.9772939 0.002000236\n\n$inertia\n[1] 0.3010185 0.1178242 0.3813729 0.3147248\n\n\n\nhousetasks_CA$row\n\n$coord\n                   Dim 1      Dim 2       Dim 3\nBugaderia     -0.9918368  0.4953220 -0.31672897\nDinar         -0.8755855  0.4901092 -0.16406487\nSopar         -0.6925740  0.3081043 -0.20741377\nEsmorçar      -0.5086002  0.4528038  0.22040453\nOrdenar       -0.3938084 -0.4343444 -0.09421375\nNetejar_Plats -0.1889641 -0.4419662  0.26694926\nCompres       -0.1176813 -0.4033171  0.20261512\nOficial        0.2266324  0.2536132  0.92336416\nConduir        0.7417696  0.6534143  0.54445849\nFinances       0.2707669 -0.6178684  0.03479681\nAssegurança    0.6470759 -0.4737832 -0.28936051\nReparacions    1.5287787  0.8642647 -0.47208778\nVacances       0.2524863 -1.4350066 -0.12958665\n\n$contrib\n                   Dim 1      Dim 2       Dim 3\nBugaderia     18.2867003  5.5638913  7.96842443\nDinar         12.3888433  4.7355230  1.85868941\nSopar          5.4713982  1.3210221  2.09692603\nEsmorçar       3.8249284  3.6986131  3.06939857\nOrdenar        1.9983518  2.9656441  0.48873403\nNetejar_Plats  0.4261663  2.8441170  3.63429434\nCompres        0.1755248  2.5151584  2.22335679\nOficial        0.5207837  0.7956201 36.94038942\nConduir        8.0778371  7.6468564 18.59638635\nFinances       0.8750075  5.5585460  0.06175066\nAssegurança    6.1470616  4.0203590  5.25263863\nReparacions   40.7300940 15.8806509 16.59639139\nVacances       1.0773030 42.4539986  1.21261994\n\n$cos2\n                   Dim 1      Dim 2       Dim 3\nBugaderia     0.73998741 0.18455213 0.075460467\nDinar         0.74160285 0.23235928 0.026037873\nSopar         0.77664011 0.15370323 0.069656660\nEsmorçar      0.50494329 0.40023001 0.094826699\nOrdenar       0.43981243 0.53501508 0.025172490\nNetejar_Plats 0.11811778 0.64615253 0.235729693\nCompres       0.06365362 0.74765514 0.188691242\nOficial       0.05304464 0.06642648 0.880528877\nConduir       0.43201860 0.33522911 0.232752289\nFinances      0.16067678 0.83666958 0.002653634\nAssegurança   0.57601197 0.30880208 0.115185951\nReparacions   0.70673575 0.22587147 0.067392778\nVacances      0.02979239 0.96235977 0.007847841\n\n$inertia\n [1] 0.13415976 0.09069235 0.03824633 0.04112368 0.02466697 0.01958732\n [7] 0.01497017 0.05330000 0.10150885 0.02956446 0.05793584 0.31287411\n[13] 0.19631064\n\n\n\nfviz_screeplot(housetasks_CA, addlabel=T)\n\n\n\n\n\n\n\n\nEl 89% de la variança de les variables están explicades per les dimensiones 1 i 2.\n\nfviz_ca_biplot(housetasks_CA,repel = T)\n\n\n\n\n\n\n\n\nLa descripció del gràfic és el següent:\n\nBlau: Corresponen a les files\nVermell: Corresponen a les columnes\n\nD’aqui podem extreure les següents conclusions:\n1- Les tasques de dinar, sopar, estendre i esmorçar son realitzades amb més freqüéncia per les dones.\n2- Les tasques de conduir i fer reparacions es realitzen amb més freqüéncia pels marits.\n3- Les tasques de vacances, finances i seguretat ho fan en conjunt.\nPer tal de poder descriure les dimensions, podem realitzar un gràfic de correlacions.\n\nlibrary(corrplot)\ncorrplot(housetasks_CA$col$cos2)\n\n\n\n\n\n\n\n\nUtilitzem la distància \\(cos^{2}\\) per la variable de tasques.\n\ncorrplot(housetasks_CA$row$cos2 )\n\n\n\n\n\n\n\n\nD’aquí podem extreure que:\n\nLa 1a component fa referència a tasques realitzades de manera individual\nLa 2a component fa referència a tasques realitzades de manera col·lectiva.\n\nA continuació anem a veure la contribució de cada columna a cada dimensió:\n\nfviz_contrib(housetasks_CA, choice = \"col\" ,axes = 1)\n\n\n\n\n\n\n\n\n\nfviz_contrib(housetasks_CA, choice = \"col\" ,axes = 2)\n\n\n\n\n\n\n\n\n\nfviz_contrib(housetasks_CA, choice = \"col\" ,axes = 1:2)\n\n\n\n\n\n\n\n\n\nfviz_contrib(housetasks_CA, choice = \"row\" ,axes = 1)\n\n\n\n\n\n\n\n\n\nfviz_contrib(housetasks_CA, choice = \"row\" ,axes = 2)\n\n\n\n\n\n\n\n\n\nfviz_contrib(housetasks_CA, choice = \"row\" ,axes = 1:2)\n\n\n\n\n\n\n\n\n\nfviz_ca_biplot(housetasks_CA,repel = T, arrow = c(F,T), col.col = \"cos2\", \n               gradient.cols = c(\"red\", \"yellow\", \"green\"),\n               alpha.col = \"contrib\")"
  },
  {
    "objectID": "material/FactorialMethods/ACP.html#footnotes",
    "href": "material/FactorialMethods/ACP.html#footnotes",
    "title": "Anàlisis de Components Principals (ACP)",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nEn estadística les taules de contingència s’empren per a registrar i analitzar l’associació entre dues o més variables, habitualment de naturalesa qualitativa (nominals o ordinals).↩︎"
  },
  {
    "objectID": "docs/material/ANN/Ejercicio_CNN_Deportes.html",
    "href": "docs/material/ANN/Ejercicio_CNN_Deportes.html",
    "title": "Convolutional Neural Networks",
    "section": "",
    "text": "import numpy as np\nimport os\nimport re\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report\n\n\n!pip install keras\n\nRequirement already satisfied: keras in /usr/local/lib/python3.10/dist-packages (3.5.0)\nRequirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from keras) (1.4.0)\nRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from keras) (1.26.4)\nRequirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras) (13.9.4)\nRequirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras) (0.0.8)\nRequirement already satisfied: h5py in /usr/local/lib/python3.10/dist-packages (from keras) (3.12.1)\nRequirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras) (0.13.1)\nRequirement already satisfied: ml-dtypes in /usr/local/lib/python3.10/dist-packages (from keras) (0.4.1)\nRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from keras) (24.2)\nRequirement already satisfied: typing-extensions&gt;=4.5.0 in /usr/local/lib/python3.10/dist-packages (from optree-&gt;keras) (4.12.2)\nRequirement already satisfied: markdown-it-py&gt;=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich-&gt;keras) (3.0.0)\nRequirement already satisfied: pygments&lt;3.0.0,&gt;=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich-&gt;keras) (2.18.0)\nRequirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py&gt;=2.2.0-&gt;rich-&gt;keras) (0.1.2)\n\n\n\nimport keras\nfrom keras.utils import to_categorical\nfrom keras import Input, Model\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Flatten, BatchNormalization, LeakyReLU\nfrom keras.layers import Conv2D, MaxPooling2D\n\nVamos a emparejar el notebook de python con el google drive\n\nfrom google.colab import drive\ndrive.mount('/content/drive')\n\nMounted at /content/drive\n\n\n\n\nLa red toma como entrada los pixeles de una imagen. Si tenemos una imagen con apenas 28×28 pixeles de alto y ancho, eso equivale a 784 neuronas. Y eso es si sólo tenemos 1 color (escala de grises). Si tuviéramos una imagen a color, necesitaríamos 3 canales (red, green, blue) y entonces usaríamos 28x28x3 = 2352 neuronas de entrada. Esa es nuestra capa de entrada.\n\n\n\nimagen\n\n\n\n\n\ndirname = os.path.join(os.getcwd(), 'drive/MyDrive/DOCENCIA/ANN/sports')\nimgpath = dirname + os.sep\n\n\nimages = []\ndirectories = []\ndircount = []\nprevRoot=''\ncant=0\n\nprint(\"leyendo imagenes de \",imgpath)\n\nfor root, dirnames, filenames in os.walk(imgpath):\n    for filename in filenames:\n        if re.search(\"\\.(jpg|jpeg|png|bmp|tiff)$\", filename):\n            cant=cant+1\n            filepath = os.path.join(root, filename)\n            image = plt.imread(filepath)\n            images.append(image)\n            b = \"Leyendo...\" + str(cant)\n            print (b, end=\"\\r\")\n            if prevRoot !=root:\n                print(root, cant)\n                prevRoot=root\n                directories.append(root)\n                dircount.append(cant)\n                cant=0\ndircount.append(cant)\n\ndircount = dircount[1:]\ndircount[0]=dircount[0]+1\nprint('Directorios leidos:',len(directories))\nprint(\"Imagenes en cada directorio\", dircount)\nprint('suma Total de imagenes en subdirs:',sum(dircount))\n\nleyendo imagenes de  /content/drive/MyDrive/DOCENCIA/ANN/sports/\n/content/drive/MyDrive/DOCENCIA/ANN/sports/ciclismo 1\n/content/drive/MyDrive/DOCENCIA/ANN/sports/f1 1000\n/content/drive/MyDrive/DOCENCIA/ANN/sports/futbol 1000\n/content/drive/MyDrive/DOCENCIA/ANN/sports/basket 1000\n/content/drive/MyDrive/DOCENCIA/ANN/sports/tenis 1000\nDirectorios leidos: 5\nImagenes en cada directorio [1001, 1000, 1000, 1000, 999]\nsuma Total de imagenes en subdirs: 5000\nEsta web está creada por Dante Conti y Sergi Ramírez, (c) 2024"
  },
  {
    "objectID": "docs/material/ANN/Ejercicio_CNN_Deportes.html#imagenes-y-píxeles",
    "href": "docs/material/ANN/Ejercicio_CNN_Deportes.html#imagenes-y-píxeles",
    "title": "Convolutional Neural Networks",
    "section": "",
    "text": "La red toma como entrada los pixeles de una imagen. Si tenemos una imagen con apenas 28×28 pixeles de alto y ancho, eso equivale a 784 neuronas. Y eso es si sólo tenemos 1 color (escala de grises). Si tuviéramos una imagen a color, necesitaríamos 3 canales (red, green, blue) y entonces usaríamos 28x28x3 = 2352 neuronas de entrada. Esa es nuestra capa de entrada.\n\n\n\nimagen\n\n\n\n\n\ndirname = os.path.join(os.getcwd(), 'drive/MyDrive/DOCENCIA/ANN/sports')\nimgpath = dirname + os.sep\n\n\nimages = []\ndirectories = []\ndircount = []\nprevRoot=''\ncant=0\n\nprint(\"leyendo imagenes de \",imgpath)\n\nfor root, dirnames, filenames in os.walk(imgpath):\n    for filename in filenames:\n        if re.search(\"\\.(jpg|jpeg|png|bmp|tiff)$\", filename):\n            cant=cant+1\n            filepath = os.path.join(root, filename)\n            image = plt.imread(filepath)\n            images.append(image)\n            b = \"Leyendo...\" + str(cant)\n            print (b, end=\"\\r\")\n            if prevRoot !=root:\n                print(root, cant)\n                prevRoot=root\n                directories.append(root)\n                dircount.append(cant)\n                cant=0\ndircount.append(cant)\n\ndircount = dircount[1:]\ndircount[0]=dircount[0]+1\nprint('Directorios leidos:',len(directories))\nprint(\"Imagenes en cada directorio\", dircount)\nprint('suma Total de imagenes en subdirs:',sum(dircount))\n\nleyendo imagenes de  /content/drive/MyDrive/DOCENCIA/ANN/sports/\n/content/drive/MyDrive/DOCENCIA/ANN/sports/ciclismo 1\n/content/drive/MyDrive/DOCENCIA/ANN/sports/f1 1000\n/content/drive/MyDrive/DOCENCIA/ANN/sports/futbol 1000\n/content/drive/MyDrive/DOCENCIA/ANN/sports/basket 1000\n/content/drive/MyDrive/DOCENCIA/ANN/sports/tenis 1000\nDirectorios leidos: 5\nImagenes en cada directorio [1001, 1000, 1000, 1000, 999]\nsuma Total de imagenes en subdirs: 5000"
  },
  {
    "objectID": "docs/material/ANN/Ejercicio_CNN_Deportes.html#hacemos-el-one-hot-encoding-para-la-red",
    "href": "docs/material/ANN/Ejercicio_CNN_Deportes.html#hacemos-el-one-hot-encoding-para-la-red",
    "title": "Convolutional Neural Networks",
    "section": "Hacemos el One-hot Encoding para la red",
    "text": "Hacemos el One-hot Encoding para la red\n\n# Change the labels from categorical to one-hot encoding\ntrain_Y_one_hot = to_categorical(train_Y)\ntest_Y_one_hot = to_categorical(test_Y)\n\n# Display the change for category label using one-hot encoding\nprint('Original label:', train_Y[0])\nprint('After conversion to one-hot:', train_Y_one_hot[0])\n\nOriginal label: 1\nAfter conversion to one-hot: [0. 1. 0. 0. 0.]"
  },
  {
    "objectID": "docs/material/ANN/Ejercicio_CNN_Deportes.html#conjunto-de-kernels",
    "href": "docs/material/ANN/Ejercicio_CNN_Deportes.html#conjunto-de-kernels",
    "title": "Convolutional Neural Networks",
    "section": "Conjunto de Kernels",
    "text": "Conjunto de Kernels\nCuando generamos nuestra matriz agregada, en realidad, no aplicaremos 1 sólo kernel, si no que tendremos muchos kernel (en su conjunto se llama filtros). Por ejemplo en esta primer convolución podríamos tener 32 filtros, con lo cual realmente obtendremos 32 matrices de salida (este conjunto se conoce como feature mapping), cada una de 28x28x1 dando un total del 25.088 neuronas para nuestra PRIMER CAPA OCULTA de neuronas.\n\n\nA medida que vamos desplazando el kernel y vamos obteniendo una nueva imagen filtrada por el kernel. En esta primer convolución y siguiendo con el ejemplo anterior, es como si obtuviéramos 32 imágenes filtradas nuevas. Estas imágenes nuevas lo que están “dibujando” son ciertas características de la imagen original. Esto ayudará en el futuro a poder distinguir un objeto de otro.\n\n\n\nimagen"
  },
  {
    "objectID": "docs/material/ANN/Ejercicio_CNN_Deportes.html#max-pooling",
    "href": "docs/material/ANN/Ejercicio_CNN_Deportes.html#max-pooling",
    "title": "Convolutional Neural Networks",
    "section": "Max-Pooling",
    "text": "Max-Pooling\nVamos a intentar explicarlo con un ejemplo: supongamos que haremos Max-pooling de tamaño 2×2. Esto quiere decir que recorreremos cada una de nuestras 32 imágenes de características obtenidas anteriormente de 28x28px de izquierda-derecha, arriba-abajo PERO en vez de tomar de a 1 pixel, tomaremos de “2×2” (2 de alto por 2 de ancho = 4 pixeles) e iremos preservando el valor “más alto” de entre esos 4 pixeles (por eso lo de “Max”). En este caso, usando 2×2, la imagen resultante es reducida “a la mitad”y quedará de 14×14 pixeles. Luego de este proceso de subsamplig nos quedarán 32 imágenes de 14×14, pasando de haber tenido 25.088 neuronas a 6.272, son bastantes menos y -en teoría- siguen almacenando la información más importante para detectar características deseadas.\n\n\n\nImagen\n\n\nMuy bien, pues esa ha sido una primer convolución: consiste de una entrada, un conjunto de filtros, generamos un mapa de características y hacemos un subsampling. Con lo cual, en el ejemplo de imágenes de 1 sólo color tendremos:\n\n\n\nImagen\n\n\nLa primer convolución es capaz de detectar características primitivas como lineas ó curvas. A medida que hagamos más capas con las convoluciones, los mapas de características serán capaces de reconocer formas más complejas, y el conjunto total de capas de convoluciones podrá ver.\nPues ahora deberemos hacer una Segunda convolución que será:\n\n\n\nImagen\n\n\nLa 3er convolución comenzará en tamaño 7×7 pixels y luego del max-pooling quedará en 3×3 con lo cual podríamos hacer sólo 1 convolución más. En este ejemplo empezamos con una imagen de 28x28px e hicimos 3 convoluciones. Si la imagen inicial hubiese sido mayor (de 224x224px) aún hubiéramos podido seguir haciendo convoluciones.\nLlegamos a la última convolución y nos queda el desenlace…\nPara terminar, tomaremos la última capa oculta a la que hicimos subsampling, que se dice que es tridimensional por tomar la forma -en nuestro ejemplo- 3x3x128 (alto,ancho,mapas) y la aplanamos, esto es que deja de ser tridimensional, y pasa a ser una capa de neuronas tradicionales, de las que ya conocíamos. Por ejemplo, podríamos aplanar (y conectar) a una nueva capa oculta de 100 neuronas feedforward.\n\n\n\nImagen\n\n\nEntonces, a esta nueva capa oculta tradicional, le aplicamos una función llamada Softmax que conecta contra la capa de salida final que tendrá la cantidad de neuronas correspondientes con las clases que estamos clasificando. Si clasificamos perros y gatos, serán 2 neuronas. Si es el dataset Mnist numérico serán 10 neuronas de salida. Si clasificamos coches, aviones ó barcos serán 3, etc.\nLas salidas al momento del entrenamiento tendrán el formato conocido como one-hot-encoding en el que para perros y gatos sera: [1,0] y [0,1], para coches, aviones ó barcos será [1,0,0]; [0,1,0];[0,0,1].\nY la función de Softmax se encarga de pasar a probabilidad (entre 0 y 1) a las neuronas de salida. Por ejemplo una salida [0,2 0,8] nos indica 20% probabilidades de que sea perro y 80% de que sea gato."
  },
  {
    "objectID": "docs/material/ANN/Ejercicio_CNN_Deportes.html#backpropagation",
    "href": "docs/material/ANN/Ejercicio_CNN_Deportes.html#backpropagation",
    "title": "Convolutional Neural Networks",
    "section": "Backpropagation",
    "text": "Backpropagation\nEl proceso es similar al de las redes tradicionales en las que tenemos una entrada y una salida esperada (por eso aprendizaje supervisado) y mediante el backpropagation mejoramos el valor de los pesos de las interconexiones entre capas de neuronas y a medida que iteramos esos pesos se ajustan hasta ser óptimos.\nEn el caso de la CNN, deberemos ajustar el valor de los pesos de los distintos kernels. Esto es una gran ventaja al momento del aprendizaje pues como vimos cada kernel es de un tamaño reducido, en nuestro ejemplo en la primer convolución es de tamaño de 3×3, eso son sólo 9 parámetros que debemos ajustar en 32 filtros dan un total de 288 parámetros. En comparación con los pesos entre dos capas de neuronas “tradicionales”: una de 748 y otra de 6272 en donde están TODAS interconectarlas con TODAS y eso equivaldría a tener que entrenar y ajustar más de 4,5 millones de pesos (repito: sólo para 1 capa)."
  },
  {
    "objectID": "docs/material/ANN/Ejercicio_CNN_Deportes.html#arquitectura-básica",
    "href": "docs/material/ANN/Ejercicio_CNN_Deportes.html#arquitectura-básica",
    "title": "Convolutional Neural Networks",
    "section": "Arquitectura básica",
    "text": "Arquitectura básica\nResumiendo: podemos decir que los elementos que usamos para crear CNNs son:\n\nEntrada: Serán los pixeles de la imagen. Serán alto, ancho y profundidad será 1 sólo color o 3 para Red,Green,Blue.\nCapa De Convolución: procesará la salida de neuronas que están conectadas en “regiones locales” de entrada (es decir pixeles cercanos), calculando el producto escalar entre sus pesos (valor de pixel) y una pequeña región a la que están conectados en el volumen de entrada. Aquí usaremos por ejemplo 32 filtros o la cantidad que decidamos y ese será el volumen de salida.\n“CAPA RELU” aplicará la función de activación en los elementos de la matriz.\nPOOL ó SUBSAMPLING: Hará una reducción en las dimensiones alto y ancho, pero se mantiene la profundidad.\nCAPA “TRADICIONAL” red de neuronas feedforward que conectará con la última capa de subsampling y finalizará con la cantidad de neuronas que queremos clasificar."
  },
  {
    "objectID": "docs/material/ANN/Ejercicio_CNN_Deportes.html#declaración-de-parámetros",
    "href": "docs/material/ANN/Ejercicio_CNN_Deportes.html#declaración-de-parámetros",
    "title": "Convolutional Neural Networks",
    "section": "Declaración de parámetros",
    "text": "Declaración de parámetros\n\n#declaramos variables con los parámetros de configuración de la red\nINIT_LR = 1e-3 # Valor inicial de learning rate. El valor 1e-3 corresponde con 0.001\nepochs = 10 # Cantidad de iteraciones completas al conjunto de imagenes de entrenamiento\nbatch_size = 64 # cantidad de imágenes que se toman a la vez en memoria"
  },
  {
    "objectID": "docs/material/ANN/Ejercicio_CNN_Deportes.html#construcción-del-modelo",
    "href": "docs/material/ANN/Ejercicio_CNN_Deportes.html#construcción-del-modelo",
    "title": "Convolutional Neural Networks",
    "section": "Construcción del modelo",
    "text": "Construcción del modelo\n\nsport_model = Sequential()\nsport_model.add(Input(shape = (21, 28, 3)))\nsport_model.add(Conv2D(32, kernel_size = (3, 3), activation = 'linear', padding = 'same'))\nsport_model.add(LeakyReLU(negative_slope = 0.1))\nsport_model.add(MaxPooling2D((2, 2), padding = 'same'))\nsport_model.add(Dropout(0.5))\nsport_model.add(Flatten())\nsport_model.add(Dense(32, activation = 'linear'))\nsport_model.add(LeakyReLU(negative_slope = 0.1))\nsport_model.add(Dropout(0.5))\nsport_model.add(Dense(nClasses, activation = 'softmax'))\n\n\nsport_model.summary()\n\nModel: \"sequential_1\"\n\n\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n┃ Layer (type)                         ┃ Output Shape                ┃         Param # ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n│ conv2d_1 (Conv2D)                    │ (None, 21, 28, 32)          │             896 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ leaky_re_lu_2 (LeakyReLU)            │ (None, 21, 28, 32)          │               0 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ max_pooling2d_1 (MaxPooling2D)       │ (None, 11, 14, 32)          │               0 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dropout_2 (Dropout)                  │ (None, 11, 14, 32)          │               0 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ flatten_1 (Flatten)                  │ (None, 4928)                │               0 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_2 (Dense)                      │ (None, 32)                  │         157,728 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ leaky_re_lu_3 (LeakyReLU)            │ (None, 32)                  │               0 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dropout_3 (Dropout)                  │ (None, 32)                  │               0 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_3 (Dense)                      │ (None, 5)                   │             165 │\n└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n\n\n\n Total params: 158,789 (620.27 KB)\n\n\n\n Trainable params: 158,789 (620.27 KB)\n\n\n\n Non-trainable params: 0 (0.00 B)\n\n\n\n\nfrom keras.utils import plot_model\n\nplot_model(sport_model, to_file='modelo_red_neuronal.png', show_shapes = True, show_layer_names = True)\n\n# Mostrar la imagen generada\nimg = plt.imread('modelo_red_neuronal.png')\nplt.imshow(img)\nplt.axis('off')  # Opcional: desactivar los ejes\nplt.show()\n\n\n\n\n\n\n\n\n\n# from tensorflow.keras.optimizers import Adagrad\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.optimizers.schedules import ExponentialDecay\n\n# Define un programador de tasa de aprendizaje\nlr_schedule = ExponentialDecay(\n    initial_learning_rate = INIT_LR,  # Tasa de aprendizaje inicial\n    decay_steps = 100,                # Número de pasos para aplicar el decaimiento\n    decay_rate = INIT_LR / 100,       # Factor de decaimiento (0.96 es un ejemplo)\n    staircase = False                 # `False` para un decaimiento continuo\n)\n\n\n# Compilamos el modelo\nsport_model.compile(loss = \"categorical_crossentropy\",\n                    optimizer = Adam(learning_rate = lr_schedule),\n                    metrics = ['accuracy'])"
  },
  {
    "objectID": "docs/material/ANN/Ejercicio_CNN_Deportes.html#tensorflow",
    "href": "docs/material/ANN/Ejercicio_CNN_Deportes.html#tensorflow",
    "title": "Convolutional Neural Networks",
    "section": "TENSORFLOW",
    "text": "TENSORFLOW\n\nimport tensorflow as tf\nfrom tensorflow.keras.callbacks import TensorBoard\n\n# Definir el callback de TensorBoard\ntensorboard_callback = TensorBoard(log_dir=\"./logs\", histogram_freq=1)\n\n\n# Compilar el modelo\nsport_model.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n\n# Entrenar el modelo con el callback\nsport_train = sport_model.fit(train_X, train_label, epochs = 10,\n                              callbacks = [tensorboard_callback],\n                              validation_data = (valid_X, valid_label))\n\nEpoch 1/10\n100/100 ━━━━━━━━━━━━━━━━━━━━ 4s 24ms/step - accuracy: 0.9707 - loss: 0.0519 - val_accuracy: 0.9900 - val_loss: 0.0153\nEpoch 2/10\n100/100 ━━━━━━━━━━━━━━━━━━━━ 4s 35ms/step - accuracy: 0.9760 - loss: 0.0437 - val_accuracy: 0.9875 - val_loss: 0.0167\nEpoch 3/10\n100/100 ━━━━━━━━━━━━━━━━━━━━ 3s 31ms/step - accuracy: 0.9758 - loss: 0.0464 - val_accuracy: 0.9912 - val_loss: 0.0142\nEpoch 4/10\n100/100 ━━━━━━━━━━━━━━━━━━━━ 2s 20ms/step - accuracy: 0.9744 - loss: 0.0399 - val_accuracy: 0.9900 - val_loss: 0.0144\nEpoch 5/10\n100/100 ━━━━━━━━━━━━━━━━━━━━ 2s 22ms/step - accuracy: 0.9780 - loss: 0.0379 - val_accuracy: 0.9912 - val_loss: 0.0127\nEpoch 6/10\n100/100 ━━━━━━━━━━━━━━━━━━━━ 2s 20ms/step - accuracy: 0.9818 - loss: 0.0367 - val_accuracy: 0.9937 - val_loss: 0.0125\nEpoch 7/10\n100/100 ━━━━━━━━━━━━━━━━━━━━ 2s 20ms/step - accuracy: 0.9846 - loss: 0.0299 - val_accuracy: 0.9950 - val_loss: 0.0110\nEpoch 8/10\n100/100 ━━━━━━━━━━━━━━━━━━━━ 4s 31ms/step - accuracy: 0.9848 - loss: 0.0332 - val_accuracy: 0.9912 - val_loss: 0.0138\nEpoch 9/10\n100/100 ━━━━━━━━━━━━━━━━━━━━ 4s 20ms/step - accuracy: 0.9890 - loss: 0.0269 - val_accuracy: 0.9925 - val_loss: 0.0112\nEpoch 10/10\n100/100 ━━━━━━━━━━━━━━━━━━━━ 2s 21ms/step - accuracy: 0.9879 - loss: 0.0291 - val_accuracy: 0.9937 - val_loss: 0.0117\n\n\n\n# Crear un callback de TensorBoard\nimport datetime\nlog_dir = \"/content/logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\ntensorboard_callback = TensorBoard(log_dir=log_dir, histogram_freq=1)\n\n\n# Iniciar TensorBoard en Colab\n%load_ext tensorboard\n%tensorboard --logdir /content/logs/fit\n\nThe tensorboard extension is already loaded. To reload it, use:\n  %reload_ext tensorboard\n\n\nReusing TensorBoard on port 6006 (pid 19935), started 0:01:09 ago. (Use '!kill 19935' to kill it.)"
  },
  {
    "objectID": "docs/material/ANN/02_DreamBooth_parte2.html",
    "href": "docs/material/ANN/02_DreamBooth_parte2.html",
    "title": "DREAMBOOTH 🤖",
    "section": "",
    "text": "%load_ext autoreload\n%autoreload 2\n\nThe autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload\nDreambooth es un modelo de generación de aprendizaje profundo, y que fue desarrollado en 2022 por un grupo de investigadores de Google Research y la Universidad de Boston. La misión de esta tecnología es la de poder entrenar a modelos de inteligencia artificial para personalizarlo según tus necesidades.\nEsta web está creada por Dante Conti y Sergi Ramírez, (c) 2024"
  },
  {
    "objectID": "docs/material/ANN/02_DreamBooth_parte2.html#como-funciona",
    "href": "docs/material/ANN/02_DreamBooth_parte2.html#como-funciona",
    "title": "DREAMBOOTH 🤖",
    "section": "¿Como funciona? 🔩",
    "text": "¿Como funciona? 🔩\nEl funcionamiento de esta técnica funciona en tres pasos.\n\nEn primer lugar, necesitas un modelo de difusión preentrenado, que es uno de esos sistemas de inteligencia artificial que pueden crear imágenes a partir de texto. Por ejemplo, se puede usar:\n\n\nStable Diffusion\nDALL-E\nMidjourney\n\nsiempre y cuando funcionen con el proceso de ruido y denoising.\nLo que hace esta técnica es crear una imagen completamente ruidosa, y luego ir quitando ese ruido reconstruyendo en el proceso una imagen totalmente original que se parezca a lo que le has pedido por texto. Pues es en este punto en el que Dreambooth ayudará con un modelo entrenado para que puedas obtener imágenes de sujetos concretos.\n\nel segundo paso, en el que necesitas un conjunto de imágenes del sujeto con el que quieres personalizar la IA. Puede ser un estilo, una cara, o lo que sea. Se recomienda tener un set de unas 8 o 10 imágenes como mínimo para poder entrenar el modelo.\n\nEntonces, lo que hace Dreambooth es utilizar este set de imágenes para entrenar al modelo de difusión, entrenar a la IA para que sepa reconocer lo que hay en ellas. Puede reconocer tu cara para luego poder dibujarla desde cero, así como un estilo o una posición.\n\nUna vez has usado Dreambooth para entrenar a la IA, este sistema usará las imágenes del sujeto como punto de partida para el proceso de crear la imagen aleatoria, permitiendo que la IA tenga más información sobre cómo es el sujeto que quieres dibujar, y que así pueda hacer imágenes que se parezcan a él.\n\n\n\n\nImagen"
  },
  {
    "objectID": "docs/material/ANN/02_DreamBooth_parte2.html#instalació-de-paquets",
    "href": "docs/material/ANN/02_DreamBooth_parte2.html#instalació-de-paquets",
    "title": "DREAMBOOTH 🤖",
    "section": "Instalació de paquets",
    "text": "Instalació de paquets\n\nInstal·leu el paquet de Python Py-Dreambooth tal com es mostra a continuació.\n\n\n!pip install -q py_dreambooth"
  },
  {
    "objectID": "docs/material/ANN/02_DreamBooth_parte2.html#importa-mòduls",
    "href": "docs/material/ANN/02_DreamBooth_parte2.html#importa-mòduls",
    "title": "DREAMBOOTH 🤖",
    "section": "Importa mòduls",
    "text": "Importa mòduls\n\nHi ha diversos tipus de classes de model, però estaràs utilitzant el model més bàsic, el model Stable Diffusion Dreambooth SDDreamboothModel, però no t’has de preocupar per això ara mateix. 🤷‍♂️\n\n\nfrom py_dreambooth.dataset import LocalDataset\nfrom py_dreambooth.model import SdDreamboothModel\nfrom py_dreambooth.predictor import LocalPredictor\nfrom py_dreambooth.trainer import LocalTrainer\nfrom py_dreambooth.utils.image_helpers import display_images\nfrom py_dreambooth.utils.prompt_helpers import make_prompt\n\nMontem la relació entre el google drive i el quadern de jupyter\n\nfrom google.colab import drive\ndrive.mount('/content/drive')\n\nDrive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True)."
  },
  {
    "objectID": "docs/material/ANN/02_DreamBooth_parte2.html#preparem-les-dades",
    "href": "docs/material/ANN/02_DreamBooth_parte2.html#preparem-les-dades",
    "title": "DREAMBOOTH 🤖",
    "section": "Preparem les dades 📸",
    "text": "Preparem les dades 📸\n\nDATA_DIR = \"/content/drive/MyDrive/ANN/DREAMBOOTH/data\"  # el directori amb fotos per a que el model s'entreni\nOUTPUT_DIR = \"/content/drive/MyDrive/ANN/DREAMBOOTH/models\"  # El directori on s'ubicaran els fitxers de model entrenats\n\ndataset = LocalDataset(DATA_DIR)\n\n\nMolt important! En el DATA_DIR definit anteriorment, posar les imatges (jpg o png) del subjecte que es vol entrenar.\nPer a aquesta tasca, necessitareu entre 10 i 20 solos, selfies d’alta qualitat preses amb diferents fons, il·luminació i expressions facials. Crec que es pot trobar un gran exemple al repositori de GitHub de Joe Penna.\n\n\n\n\nSamples\n\n\n\nUtilitzeu el mètode de processament d’imatges següent per retallar les imatges en un quadrat centrat a la cara. Si el subjecte que el model està tractant d’aprendre no és una persona (per exemple, un gos), estableix l’argument detect_face argumentant com a False.\n\n\ndataset = dataset.preprocess_images(detect_face=True)\n\nA total of 8 images were found.\n\n\n 38%|███▊      | 3/8 [00:00&lt;00:00,  6.17it/s]\n\n\nNo faces detected in the image '443008034_395930086752782_7217331932050061307_n.jpg'.\nNo faces detected in the image '440173417_1436258973657135_9081022692963550822_n.jpg'.\n\n\n 75%|███████▌  | 6/8 [00:00&lt;00:00,  6.41it/s]\n\n\nNo faces detected in the image '429164819_452647503759342_2302826312178258320_n.jpg'.\n\n\n100%|██████████| 8/8 [00:01&lt;00:00,  5.50it/s]\n\n\nA total of 5 images were preprocessed and stored in the path '/content/drive/MyDrive/ANN/DREAMBOOTH/data_preproc'."
  },
  {
    "objectID": "docs/material/ANN/02_DreamBooth_parte2.html#carregar-el-model",
    "href": "docs/material/ANN/02_DreamBooth_parte2.html#carregar-el-model",
    "title": "DREAMBOOTH 🤖",
    "section": "Carregar el model 🤖",
    "text": "Carregar el model 🤖\n\nSi reinicieu el nucli del bloc de notes i voleu tornar a carregar els models que ja heu entrenat, podeu fer-ho de la següent manera.\n\n\npredictor = LocalPredictor(model, OUTPUT_DIR)"
  },
  {
    "objectID": "docs/material/ANN/02_DreamBooth_parte2.html#crea-imatges-com-vulgueu",
    "href": "docs/material/ANN/02_DreamBooth_parte2.html#crea-imatges-com-vulgueu",
    "title": "DREAMBOOTH 🤖",
    "section": "Crea imatges com vulgueu! 💃",
    "text": "Crea imatges com vulgueu! 💃\n\nUtilitzeu les indicacions per crear qualsevol imatge que vulgueu. El text de l’indicatiu ha de contenir el nom de l’assumpte i el nom de la classe definits anteriorment.\nTens problemes per arribar amb un bon prompte? No et preocupis. Podeu utilitzar la funció make_prompt per a generar una sol·licitud comissariada a l’atzar. Mira això. 🙆‍♀️\nLa creació de grans imatges pren paciència. Juga amb les indicacions, però si la qualitat de la pròpia generació és problemàtica, és possible que hagis de tornar a entrenar amb millors dades i paràmetres d’entrenament més adequats.\n\n\n%%time\nprompt = f\"A photo of {SUBJECT_NAME} {CLASS_NAME} with Simpsons\"\n# prompt = next(make_prompt(SUBJECT_NAME, CLASS_NAME))\n\nprint(f\"The prompt is as follows:\\n{prompt}\")\n\nimages = predictor.predict(\n    prompt,\n    height = 768,\n    width = 512,\n    num_images_per_prompt = 5,\n)\n\ndisplay_images(images, fig_size = 10)\n\nThe prompt is as follows:\nA photo of mire person with Simpsons\n\n\n\n\n\n\n\n\n\n\n\n\nCPU times: user 45.9 s, sys: 3.72 s, total: 49.6 s\nWall time: 50.3 s"
  },
  {
    "objectID": "docs/material/ANN/02_DreamBooth_parte2.html#bibliografia",
    "href": "docs/material/ANN/02_DreamBooth_parte2.html#bibliografia",
    "title": "DREAMBOOTH 🤖",
    "section": "Bibliografia 💃",
    "text": "Bibliografia 💃\n\nStable Diffusion\nDreamBooth\nPy-Dreambooth"
  },
  {
    "objectID": "material/SoftwareCarpentry/SoftwareCarpentry.html",
    "href": "material/SoftwareCarpentry/SoftwareCarpentry.html",
    "title": "Software Carpentry",
    "section": "",
    "text": "1 Objetivo\nEste notebook muestra cómo organizar un proyecto de R y controlar operaciones de sistema:\n\nEstructura recomendada de carpetas: syntax/, input/, output/, data/, temp/, logs/.\nManejo de rutas relativas con {here}.\nMensajes y nombres de archivo dinámicos con {glue}.\nCrear y buscar ficheros (dir.create(), file.path(), list.files(), fs::dir_create()…).\nRedirigir salida con sink().\nGuardar gráficos con pdf() y png().\nMini pipeline de ejemplo (leer → procesar → guardar).\n\n\nConsejo: evita setwd() y usa rutas relativas con {here} para que tu proyecto sea 100% reproducible.\n\n\n\n2 Paquetes y opciones\n\n\nCode\n# Paquetes base y útiles:\npacks &lt;- c(\"here\", \"glue\", \"fs\", \"readr\", \"dplyr\", \"ggplot2\")\nto_install &lt;- setdiff(packs, rownames(installed.packages()))\nif (length(to_install)) install.packages(to_install, quiet = TRUE)\n\nlibrary(here)    # Rutas relativas desde la raíz del proyecto\nlibrary(glue)    # Strings con llaves {var}\nlibrary(fs)      # Operaciones de sistema \"friendly\"\nlibrary(readr)   # Lectura/escritura rápida\nlibrary(dplyr)   # Manipulación de datos\nlibrary(ggplot2) # Gráficos\n\n# Opciones útiles\noptions(\n  scipen = 999,   # menos notación científica\n  digits = 4\n)\n\n# Mostrar dónde cree {here} que está la raíz del proyecto\nhere()\n\n\n¿Cómo define {here} la raíz?\n\nBuscar archivo(s) “ancla” (.Rproj, .here, DESCRIPTION, git/, etc).\nSi no encuentra, puede crear un archivo vacío llamado .here en la carpeta raíz del proyecto.\n\n\n\nCode\n# Crea un archivo marcador para que {here} sepa que esta carpeta es la raíz:\nfile_create(\".here\")\n\n\n\n\n3 Estructura del proyecto\nLa estructura propuesta es la siguiente:\n\nproject/\n├─ syntax/        # scripts R (funciones, notebooks, etc.)\n├─ input/         # insumos externos (CSV, XLSX, etc.) SOLO LECTURA\n├─ data/          # datos intermedios limpios/parquet/rds\n├─ output/        # resultados finales (tablas/figuras/listados)\n├─ temp/          # temporales desechables\n├─ logs/          # logs de ejecución\n├─ README.md\n└─ .here          # marca la raíz del proyecto p/ {here}\nPara crearlo, podemos hacerlo de la siguiente forma:\n\n\nCode\n# Crear la estructura de carpetas si no existe:\ndirs &lt;- c(\"syntax\", \"input\", \"data\", \"output\", \"temp\", \"logs\")\ndir_create(path = here(dirs))\ndir_ls(here(), type = \"directory\")\n\n\n\n\n4 Estructura del proyecto\nUsa here(\"carpeta\", \"sub\", \"archivo.ext\") para rutas portables:\n\n\nCode\n# Construir rutas de forma segura:\nruta_input  &lt;- here(\"input\", \"ventas_2025.csv\")\nruta_data   &lt;- here(\"data\",  \"ventas_limpio.rds\")\nruta_salida &lt;- here(\"output\",\"resumen_ventas.csv\")\n\nruta_input\nruta_data\nruta_salida\n\n# Con base R: file.path() también es portable\nfile.path(\"input\", \"ventas_2025.csv\")\n\n\nCon {glue} puedes crear nombres dinámicos:\n\n\nCode\nanio &lt;- 2025; mes &lt;- 9\nnombre_csv &lt;- glue(\"ventas_{anio}-{sprintf('%02d', mes)}.csv\")\nhere(\"input\", nombre_csv)\n\n\n\n\n5 Crear y escribir archivos\n\n\nCode\n# Datos de ejemplo:\ndf &lt;- tibble::tibble(\n  id = 1:5,\n  fecha = as.Date(\"2025-09-01\") + 0:4,\n  ventas = c(100, 80, 95, 120, 110)\n)\n\n# Guardar como CSV en output/\nwrite_csv(df, here(\"output\", \"tabla_ejemplo.csv\"))\n\n# Guardar como RDS en data/\nsaveRDS(df, here(\"data\", \"tabla_ejemplo.rds\"))\n\n\n\n\n6 Búsqueda de ficheros\nlist.files() (base) y fs::dir_ls() (recursivo, con globbing):\n\n\nCode\n# Listado simple\nlist.files(here(\"output\"))\n\n# Listado recursivo con patrón:\ndir_ls(here(), recurse = TRUE, glob = \"output/*.csv\")\n\n# Buscar por ext. en múltiples carpetas:\ndir_ls(here(c(\"input\",\"data\",\"output\")), recurse = TRUE, \n       regexp = \"\\\\.(csv|rds)$\")\n\n\n\n\n7 Redirección de salida con sink()\n\n\nCode\nlog_path &lt;- here(\"logs\", glue(\"log_{format(Sys.time(), '%Y%m%d_%H%M%S')}.txt\"))\n\nsink(log_path, split = TRUE)      # split=TRUE =&gt; también muestra en consola\ncat(\"=== INICIO ===\\n\")\nprint(sessionInfo())\ncat(\"Una línea cualquiera\\n\")\nsink()  # IMPORTANTÍSIMO: cerrar el sink\n\n# Revisa el contenido del log:\nreadLines(log_path, n = 8)\n\n\n⚠️ Cierra siempre el sink() con sink() (sin argumentos) o usa on.exit(sink()) dentro de una función para no “bloquear” la consola.\n\n\n8 Dispositivos gráficos: pdf() y png()\nPuedes abrir un dispositivo gráfico, dibujar y cerrado con dev.off().\n\n\nCode\npdf(here(\"output\", \"grafico_demo.pdf\"), width = 7, height = 5)\nplot(cars, main = \"Gráfico base R - cars\")\ndev.off()\n\n# PNG con resolución\npng(here(\"output\", \"grafico_demo.png\"), width = 1200, height = 900, res = 150)\nplot(pressure, main = \"Gráfico base R - pressure\")\ndev.off()\n\n\nCon ggplot2:\n\n\nCode\np &lt;- ggplot(mtcars, aes(disp, mpg)) + geom_point() +\n  labs(title = \"Relación cilindrada vs. mpg\")\n\n# Guardar directamente\nggsave(filename = here(\"output\", \"mtcars_disp_mpg.png\"), plot = p,\n       width = 7, height = 5, dpi = 150)\n\n# También PDF\nggsave(filename = here(\"output\", \"mtcars_disp_mpg.pdf\"), plot = p,\n       width = 7, height = 5)\n\n\n\n\n9 Buenas prácticas con {here}\n\nColoca un archivo .here o un .Rproj en la raíz\nNunca uses setwd() dentro de scripts reutilizables.\nEscribe funciones que reciban rutas como argumento o que construyan rutas con here().\n\n\n\nCode\n# Función ejemplo usando here()\nlee_input &lt;- function(nombre) {\n  readr::read_csv(here(\"input\", nombre), show_col_types = FALSE)\n}\n\n# Uso:\n# df &lt;- lee_input(\"ventas_2025.csv\")\n\n\n\n\n10 Mensajes y nombres con {glue}\n\n\nCode\n# Glue para strings explicativos\narchivo &lt;- \"ventas_2025.csv\"\nmensaje &lt;- glue(\"Leyendo el archivo '{archivo}' desde {here('input')}\")\nmensaje\n\n\nglue() evalúa expresiones dentro de {}:\n\n\nCode\nclientes &lt;- 1250\nglue(\"Este mes se han registrado {clientes} clientes (Δ = {clientes - 1200}).\")\n\n\n\n\n11 Mini pipeline: leer → procesar → guardar\nEjemplo autocontenido que crea un CSV de entrada, lo procesa y guarda resultados.\n\n\nCode\n# 1) Crear un CSV de ejemplo en input/\ndir_create(here(\"input\"))\ntoy &lt;- tibble::tibble(\n  id = 1:10,\n  fecha = as.Date(\"2025-09-01\") + 0:9,\n  ventas = sample(80:150, 10, replace = TRUE)\n)\nwrite_csv(toy, here(\"input\", \"toy_ventas.csv\"))\n\n# 2) Leer, procesar y registrar\nlog_path &lt;- here(\"logs\", \"mini_pipeline.log\")\nsink(log_path, split = TRUE)\ncat(\"== MINI PIPELINE ==\\n\")\n\nraw &lt;- read_csv(here(\"input\", \"toy_ventas.csv\"), show_col_types = FALSE)\ncat(glue(\"Leídas {nrow(raw)} filas.\\n\"))\n\nproc &lt;- raw |&gt;\n  mutate(\n    semana = format(fecha, \"%Y-%W\"),\n    ventas_norm = scale(ventas)[,1]\n  ) |&gt;\n  group_by(semana) |&gt;\n  summarise(ventas_media = mean(ventas), .groups = \"drop\")\n\ncat(glue(\"Semanas agregadas: {nrow(proc)}\\n\"))\n\n# 3) Guardar resultados\nwrite_csv(proc, here(\"output\", \"resumen_semanal.csv\"))\nsaveRDS(proc, here(\"data\", \"resumen_semanal.rds\"))\ncat(\"Archivos guardados en output/ y data/\\n\")\nsink()\n\n# 4) Graficar y guardar\np &lt;- ggplot(raw, aes(fecha, ventas)) + geom_line() +\n  labs(title = \"Ventas diarias (toy)\", x = \"Fecha\", y = \"Ventas\")\nggsave(here(\"output\", \"ventas_toy.png\"), plot = p, width = 7, \n       height = 5, dpi = 150)\n\n\n\n\n12 Utilidades (helpers) para tus scripts de syntax/\n\n\nCode\n# Guardar en syntax/helpers.R y luego source(\"syntax/helpers.R\") si quieres\n\ninit_log &lt;- function(prefix = \"run\") {\n  dir_create(here(\"logs\"))\n  path &lt;- here(\"logs\", \n               glue(\"{prefix}_{format(Sys.time(), '%Y%m%d_%H%M%S')}.log\"))\n  sink(path, split = TRUE)\n  cat(glue(\"[{Sys.time()}] INICIO\\n\"))\n  return(path)\n}\n\nclose_log &lt;- function() {\n  cat(glue(\"[{Sys.time()}] FIN\\n\"))\n  sink()\n}\n\nsafe_dir &lt;- function(...) {\n  # Crea una ruta y la carpeta si no existe\n  path &lt;- here(...)\n  dir_create(dirname(path))\n  return(path)\n}\n\nsave_table &lt;- function(df, ..., name, ext = \"csv\") {\n  # Guarda tabla df en output/ con nombre dinámico\n  base &lt;- glue(\"{name}.{ext}\")\n  path &lt;- safe_dir(\"output\", base)\n  if (ext == \"csv\") readr::write_csv(df, path)\n  if (ext == \"rds\") saveRDS(df, sub(\"\\\\.csv$\", \".rds\", path))\n  invisible(path)\n}\n\n\n\n\n13 Pautas de versión y limpieza\n\nTodo lo que no sea fuente, mételo bajo control (ej: borrar /temp/ al finalizar).\nUsa git para versionar scripts y notebooks.\nSepara lectura (input/) de resultados (output/) y datos de trabajo (data/).\n\n\n\nCode\n# Limpieza de temporales\nif (dir_exists(here(\"temp\"))) {\n  file_delete(dir_ls(here(\"temp\"), recurse = TRUE, type = \"file\"))\n}\n\n\n\n\n14 Apéndice: alternativas útiles\n\nfs::file_copy(), fs::file_move(), fs::file_delete() para copiar/mover/borrar.\nSys.getenv(\"VAR\") para leer variables de entorno.\nwithr::with_dir() para ejecutar código en otra dir sin cambiar tu wd global.\n\n\n\nCode\n# Copiar un archivo de ejemplo\nfs::file_copy(here(\"output\", \"tabla_ejemplo.csv\"),\n              here(\"temp\", \"copia_tabla.csv\"),\n              overwrite = TRUE)\n\n# Variables de entorno\nSys.getenv(\"HOME\")\n\n\n\n\n15 Session info\n\n\nCode\nsessionInfo()\n\n\n\n\n\n\n\n\nEsta web está creada por Dante Conti y Sergi Ramírez, (c) 2024"
  },
  {
    "objectID": "material/Preprocessing/Preprocessing.html",
    "href": "material/Preprocessing/Preprocessing.html",
    "title": "Preparación de los datos para el modelado",
    "section": "",
    "text": "Este conjunto de datos contiene registros de transacciones de cafeterías, incluyendo detalles sobre ventas, tipo de pago, hora de compra y preferencias del cliente.\nCon atributos que abarcan la hora del día, los días de la semana, los meses, los tipos de café y los ingresos, este conjunto de datos proporciona una base sólida para analizar el comportamiento del cliente, los patrones de ventas y las tendencias de rendimiento empresarial.\nEstructura del conjunto de datos:\n\nhour_of_day: Hora de compra (0–23)\ncash_type: Forma de pago (efectivo/tarjeta)\nmoney: Importe de la transacción (en moneda local)\ncoffee_name: Tipo de café comprado (p. ej., Latte, Americano, Chocolate caliente)\nTime_of_Day: Hora de compra (mañana, tarde, noche)\nWeekday: Día de la semana (p. ej., lun., mar., etc.)\nMonth_name: Mes de compra (p. ej., ene., feb., mar.)\nWeekdaysort: Representación numérica para ordenar por día de la semana (1 = lun., 7 = dom.)\nMonthsort: Representación numérica para ordenar por mes (1 = ene., 12 = dic.)\nDate: Fecha de la transacción (AAAA-MM-DD)\nTime: Hora exacta de la transacción (HH:MM:SS)\n\n\n\n  hour_of_day cash_type money         coffee_name Time_of_Day Weekday\n1          10      card  38.7               Latte     Morning     Fri\n2          12      card  38.7       Hot Chocolate   Afternoon     Fri\n3          12      card  38.7       Hot Chocolate   Afternoon     Fri\n4          13      card  28.9           Americano   Afternoon     Fri\n5          13      card  38.7               Latte   Afternoon     Fri\n6          15      card  33.8 Americano with Milk   Afternoon     Fri\n  Month_name Weekdaysort Monthsort       Date            Time\n1        Mar           5         3 2024-03-01 10:15:50.520000\n2        Mar           5         3 2024-03-01 12:19:22.539000\n3        Mar           5         3 2024-03-01 12:20:18.089000\n4        Mar           5         3 2024-03-01 13:46:33.006000\n5        Mar           5         3 2024-03-01 13:48:14.626000\n6        Mar           5         3 2024-03-01 15:39:47.726000\n\n\nA continuación vamos a detectar de que clase es cada una de las variables\n\nclases &lt;- sapply(datos, class)\nvarNum &lt;- names(clases)[which(clases %in% c(\"numeric\", \"integer\"))]\nvarCat &lt;- names(clases)[which(clases %in% c(\"character\", \"factor\"))]\n\nPara poder realizar una descriptiva correcta, descartaremos las variables Time y Date.\nEsta web está creada por Dante Conti y Sergi Ramírez, (c) 2025"
  },
  {
    "objectID": "material/Preprocessing/Preprocessing.html#univariant-analysis",
    "href": "material/Preprocessing/Preprocessing.html#univariant-analysis",
    "title": "Preparación de los datos para el modelado",
    "section": "2.1 Univariant analysis",
    "text": "2.1 Univariant analysis\n\n2.1.1 Numerical\n\n2.1.1.1 Description\n\nlibrary(psych)\npsych::describe(datos[, varNum])\n\n            vars    n  mean   sd median trimmed  mad   min  max range  skew\nhour_of_day    1 3547 14.19 4.23  14.00   14.11 5.93  6.00 22.0 16.00  0.12\nmoney          2 3547 31.65 4.88  32.82   31.98 4.36 18.12 38.7 20.58 -0.54\nWeekdaysort    3 3547  3.85 1.97   4.00    3.81 2.97  1.00  7.0  6.00  0.08\nMonthsort      4 3547  6.45 3.50   7.00    6.42 4.45  1.00 12.0 11.00  0.00\n            kurtosis   se\nhour_of_day    -1.13 0.07\nmoney          -0.67 0.08\nWeekdaysort    -1.23 0.03\nMonthsort      -1.38 0.06\n\n\n\n\n2.1.1.2 Graphic\n\nbaseggplot2\n\n\n\nfor (var in varNum) {\n  hist(datos[, var], main = paste0(\"Histograma variable \", var))\n  boxplot(datos[, var], main = paste0(\"Boxplot variable \", var))\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nlibrary(ggplot2)\n\n\nAdjuntando el paquete: 'ggplot2'\n\n\nThe following objects are masked from 'package:psych':\n\n    %+%, alpha\n\nfor (var in varNum) {\n  \n  # Histograma\n  grafico &lt;- ggplot(datos, aes(x = get(var))) + \n    geom_histogram(aes(y = ..density..), colour = \"black\", fill = \"white\")+\n    geom_density(alpha=.2, fill=\"#FF6666\")  + \n    geom_vline(aes(xintercept = mean(get(var))),\n            color=\"blue\", linetype = \"dashed\", linewidth = 1)\n  print(grafico)\n  \n  # Boxplot\n  \n  grafico2 &lt;- ggplot(datos, aes(x=get(var))) + \n    geom_boxplot(outlier.colour=\"red\", outlier.shape=8,\n                outlier.size=4)\n  print(grafico2)\n  \n}\n\nWarning: The dot-dot notation (`..density..`) was deprecated in ggplot2 3.4.0.\nℹ Please use `after_stat(density)` instead.\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2.1.2 Categorical\n\n2.1.2.1 Description\n\nfor (var in varCat) {\n  tablaAbs &lt;- data.frame(table(datos[, var]))\n  tablaFreq &lt;- data.frame(table(datos[, var])/sum(table(datos[, var])))\n  m &lt;- match(tablaAbs$Var1, tablaFreq$Var1)\n  tablaAbs[, \"FreqRel\"] &lt;- tablaFreq[m, \"Freq\"]\n  colnames(tablaAbs) &lt;- c(\"Categoria\", \"FreqAbs\", \"FreqRel\")\n  \n  cat(\"===============\", var, \"===================================\\n\")\n  print(tablaAbs)\n  cat(\"==================================================\\n\")\n}\n\n=============== cash_type ===================================\n  Categoria FreqAbs FreqRel\n1      card    3547       1\n==================================================\n=============== coffee_name ===================================\n            Categoria FreqAbs    FreqRel\n1           Americano     564 0.15900761\n2 Americano with Milk     809 0.22808007\n3          Cappuccino     486 0.13701720\n4               Cocoa     239 0.06738089\n5             Cortado     287 0.08091345\n6            Espresso     129 0.03636876\n7       Hot Chocolate     276 0.07781224\n8               Latte     757 0.21341979\n==================================================\n=============== Time_of_Day ===================================\n  Categoria FreqAbs   FreqRel\n1 Afternoon    1205 0.3397237\n2   Morning    1181 0.3329574\n3     Night    1161 0.3273189\n==================================================\n=============== Weekday ===================================\n  Categoria FreqAbs   FreqRel\n1       Fri     532 0.1499859\n2       Mon     544 0.1533690\n3       Sat     470 0.1325063\n4       Sun     419 0.1181280\n5       Thu     510 0.1437835\n6       Tue     572 0.1612630\n7       Wed     500 0.1409642\n==================================================\n=============== Month_name ===================================\n   Categoria FreqAbs    FreqRel\n1        Apr     168 0.04736397\n2        Aug     272 0.07668452\n3        Dec     259 0.07301945\n4        Feb     423 0.11925571\n5        Jan     201 0.05666761\n6        Jul     237 0.06681703\n7        Jun     223 0.06287003\n8        Mar     494 0.13927262\n9        May     241 0.06794474\n10       Nov     259 0.07301945\n11       Oct     426 0.12010149\n12       Sep     344 0.09698337\n==================================================\n\n\n\n\n2.1.2.2 Graphic\n\nbaseggplot2\n\n\n\nfor (var in varCat) {\n  barplot(table(datos[, var]))\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfor (var in varCat) {\n  \n  tabla &lt;- data.frame(table(datos[, var])/sum(table(datos[, var])))\n  p &lt;- ggplot(data = tabla, aes(x = Var1, y = Freq)) +\n        geom_bar(stat = \"identity\", fill = \"steelblue\")+\n        geom_text(aes(label=paste0(round(Freq, 2)*100, \"%\")), vjust=1.6, color=\"white\", size=3.5)+\n        theme_minimal()\n  \n  print(p)\n}"
  },
  {
    "objectID": "material/Preprocessing/Preprocessing.html#bivariant-analysis",
    "href": "material/Preprocessing/Preprocessing.html#bivariant-analysis",
    "title": "Preparación de los datos para el modelado",
    "section": "2.2 Bivariant analysis",
    "text": "2.2 Bivariant analysis\n\n2.2.1 Numerical vs. numerical\n\n2.2.1.1 Description\n\ncor(datos[, varNum])\n\n             hour_of_day       money  Weekdaysort    Monthsort\nhour_of_day  1.000000000  0.20274794 -0.002613959  0.008292999\nmoney        0.202747935  1.00000000 -0.017264091 -0.050043191\nWeekdaysort -0.002613959 -0.01726409  1.000000000  0.044140930\nMonthsort    0.008292999 -0.05004319  0.044140930  1.000000000\n\n\n\n\n2.2.1.2 Graphic\n\nbaseggplot2\n\n\n\nlibrary(PerformanceAnalytics)\n\nCargando paquete requerido: xts\n\n\nCargando paquete requerido: zoo\n\n\n\nAdjuntando el paquete: 'zoo'\n\n\nThe following objects are masked from 'package:base':\n\n    as.Date, as.Date.numeric\n\n\n\nAdjuntando el paquete: 'PerformanceAnalytics'\n\n\nThe following object is masked from 'package:graphics':\n\n    legend\n\nchart.Correlation(as.matrix(datos[, varNum]),histogram = TRUE,pch=12)\n\n\n\n\n\n\n\n\n\n\n\nlibrary(ggcorrplot)\n\nWarning: package 'ggcorrplot' was built under R version 4.4.3\n\ncorr &lt;- round(cor(datos[, varNum]), 1)\nggcorrplot(corr, lab = T)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2.2.2 Numerical vs. categorical\n\n2.2.2.1 Description\n\nfor (varN in varNum) {\n  for (varC in varCat) {\n   print(psych::describeBy(datos[, varN], group = datos[, varC])) \n  }\n}\n\n\n Descriptive statistics by group \ngroup: card\n   vars    n  mean   sd median trimmed  mad min max range skew kurtosis   se\nX1    1 3547 14.19 4.23     14   14.11 5.93   6  22    16 0.12    -1.13 0.07\n\n Descriptive statistics by group \ngroup: Americano\n   vars   n  mean  sd median trimmed  mad min max range skew kurtosis   se\nX1    1 564 13.19 3.7     13      13 4.45   6  22    16 0.42    -0.53 0.16\n------------------------------------------------------------ \ngroup: Americano with Milk\n   vars   n  mean   sd median trimmed  mad min max range skew kurtosis   se\nX1    1 809 13.62 4.31     13   13.39 4.45   6  22    16 0.37    -1.09 0.15\n------------------------------------------------------------ \ngroup: Cappuccino\n   vars   n  mean   sd median trimmed  mad min max range  skew kurtosis   se\nX1    1 486 14.92 4.23     15   15.05 5.93   6  22    16 -0.22    -1.05 0.19\n------------------------------------------------------------ \ngroup: Cocoa\n   vars   n  mean   sd median trimmed  mad min max range  skew kurtosis   se\nX1    1 239 15.26 4.26     16    15.4 5.93   7  22    15 -0.28    -1.14 0.28\n------------------------------------------------------------ \ngroup: Cortado\n   vars   n  mean   sd median trimmed  mad min max range skew kurtosis   se\nX1    1 287 12.65 3.99     12   12.26 4.45   7  22    15 0.69     -0.6 0.24\n------------------------------------------------------------ \ngroup: Espresso\n   vars   n mean   sd median trimmed  mad min max range skew kurtosis   se\nX1    1 129 13.6 3.57     14   13.49 4.45   7  22    15 0.24    -0.78 0.31\n------------------------------------------------------------ \ngroup: Hot Chocolate\n   vars   n  mean   sd median trimmed  mad min max range  skew kurtosis   se\nX1    1 276 16.32 3.89     17   16.51 4.45   8  22    14 -0.41    -0.89 0.23\n------------------------------------------------------------ \ngroup: Latte\n   vars   n  mean   sd median trimmed  mad min max range  skew kurtosis   se\nX1    1 757 14.63 4.31     15   14.67 5.93   7  22    15 -0.08    -1.11 0.16\n\n Descriptive statistics by group \ngroup: Afternoon\n   vars    n  mean   sd median trimmed  mad min max range  skew kurtosis   se\nX1    1 1205 14.07 1.45     14   14.09 1.48  12  16     4 -0.06    -1.36 0.04\n------------------------------------------------------------ \ngroup: Morning\n   vars    n mean   sd median trimmed  mad min max range  skew kurtosis   se\nX1    1 1181  9.4 1.27     10    9.47 1.48   6  11     5 -0.34    -0.93 0.04\n------------------------------------------------------------ \ngroup: Night\n   vars    n  mean   sd median trimmed  mad min max range skew kurtosis   se\nX1    1 1161 19.18 1.63     19    19.1 1.48  17  22     5  0.2    -1.17 0.05\n\n Descriptive statistics by group \ngroup: Fri\n   vars   n  mean   sd median trimmed  mad min max range skew kurtosis   se\nX1    1 532 13.83 4.46     14   13.64 5.93   6  22    16 0.23    -1.09 0.19\n------------------------------------------------------------ \ngroup: Mon\n   vars   n  mean   sd median trimmed  mad min max range skew kurtosis   se\nX1    1 544 14.01 4.29     14   13.94 5.93   6  22    16 0.06    -1.17 0.18\n------------------------------------------------------------ \ngroup: Sat\n   vars   n  mean   sd median trimmed  mad min max range skew kurtosis   se\nX1    1 470 13.93 3.89   13.5   13.71 3.71   7  22    15  0.4    -0.79 0.18\n------------------------------------------------------------ \ngroup: Sun\n   vars   n  mean   sd median trimmed  mad min max range skew kurtosis   se\nX1    1 419 14.33 3.96     14   14.21 4.45   7  22    15 0.23    -1.02 0.19\n------------------------------------------------------------ \ngroup: Thu\n   vars   n  mean   sd median trimmed  mad min max range  skew kurtosis   se\nX1    1 510 14.74 4.27     15   14.76 5.93   7  22    15 -0.05     -1.2 0.19\n------------------------------------------------------------ \ngroup: Tue\n   vars   n  mean   sd median trimmed  mad min max range skew kurtosis   se\nX1    1 572 14.27 4.39     14   14.24 5.93   7  22    15 0.04    -1.31 0.18\n------------------------------------------------------------ \ngroup: Wed\n   vars   n  mean  sd median trimmed  mad min max range skew kurtosis   se\nX1    1 500 14.23 4.2     14    14.2 5.93   7  22    15 0.08    -1.16 0.19\n\n Descriptive statistics by group \ngroup: Apr\n   vars   n mean   sd median trimmed  mad min max range skew kurtosis   se\nX1    1 168 14.4 3.13     14   14.38 4.45  10  20    10  0.1    -1.37 0.24\n------------------------------------------------------------ \ngroup: Aug\n   vars   n  mean   sd median trimmed  mad min max range skew kurtosis   se\nX1    1 272 13.51 4.51     12   13.26 4.45   7  22    15  0.4    -1.18 0.27\n------------------------------------------------------------ \ngroup: Dec\n   vars   n  mean   sd median trimmed  mad min max range skew kurtosis   se\nX1    1 259 14.33 4.39     15   14.25 5.93   7  22    15  0.1    -1.23 0.27\n------------------------------------------------------------ \ngroup: Feb\n   vars   n  mean   sd median trimmed  mad min max range  skew kurtosis   se\nX1    1 423 14.06 3.66     15   14.17 4.45   6  21    15 -0.23    -0.93 0.18\n------------------------------------------------------------ \ngroup: Jan\n   vars   n  mean   sd median trimmed  mad min max range skew kurtosis   se\nX1    1 201 14.34 4.36     14   14.25 5.93   7  22    15 0.09     -1.1 0.31\n------------------------------------------------------------ \ngroup: Jul\n   vars   n  mean  sd median trimmed  mad min max range skew kurtosis   se\nX1    1 237 14.18 4.8     13   14.02 5.93   7  22    15  0.3    -1.35 0.31\n------------------------------------------------------------ \ngroup: Jun\n   vars   n  mean   sd median trimmed  mad min max range  skew kurtosis   se\nX1    1 223 15.23 4.59     16   15.38 5.93   7  22    15 -0.15    -1.47 0.31\n------------------------------------------------------------ \ngroup: Mar\n   vars   n  mean   sd median trimmed  mad min max range  skew kurtosis   se\nX1    1 494 13.51 3.37     14   13.53 4.45   6  21    15 -0.03    -1.02 0.15\n------------------------------------------------------------ \ngroup: May\n   vars   n mean  sd median trimmed  mad min max range  skew kurtosis   se\nX1    1 241 15.2 4.1     15    15.3 5.93   7  22    15 -0.17    -1.12 0.26\n------------------------------------------------------------ \ngroup: Nov\n   vars   n mean   sd median trimmed  mad min max range skew kurtosis   se\nX1    1 259 14.1 4.27     14   14.06 4.45   7  22    15 0.08    -1.03 0.27\n------------------------------------------------------------ \ngroup: Oct\n   vars   n  mean   sd median trimmed  mad min max range skew kurtosis   se\nX1    1 426 14.04 4.59     14   13.91 5.93   7  22    15  0.2    -1.21 0.22\n------------------------------------------------------------ \ngroup: Sep\n   vars   n mean   sd median trimmed  mad min max range skew kurtosis   se\nX1    1 344 14.4 4.73   13.5   14.34 6.67   7  22    15 0.14    -1.43 0.26\n\n Descriptive statistics by group \ngroup: card\n   vars    n  mean   sd median trimmed  mad   min  max range  skew kurtosis\nX1    1 3547 31.65 4.88  32.82   31.98 4.36 18.12 38.7 20.58 -0.54    -0.67\n     se\nX1 0.08\n\n Descriptive statistics by group \ngroup: Americano\n   vars   n  mean   sd median trimmed mad   min  max range  skew kurtosis   se\nX1    1 564 25.98 1.68  25.96   25.99   0 23.02 28.9  5.88 -0.25    -0.22 0.07\n------------------------------------------------------------ \ngroup: Americano with Milk\n   vars   n  mean   sd median trimmed  mad   min  max range  skew kurtosis   se\nX1    1 809 30.59 1.88  30.86   30.57 2.91 27.92 33.8  5.88 -0.17       -1 0.07\n------------------------------------------------------------ \ngroup: Cappuccino\n   vars   n  mean   sd median trimmed  mad   min  max range skew kurtosis   se\nX1    1 486 35.88 1.82  35.76   35.94 2.91 32.82 38.7  5.88 -0.4     -0.7 0.08\n------------------------------------------------------------ \ngroup: Cocoa\n   vars   n  mean   sd median trimmed mad   min  max range  skew kurtosis   se\nX1    1 239 35.65 1.23  35.76    35.7   0 32.82 38.7  5.88 -0.53        2 0.08\n------------------------------------------------------------ \ngroup: Cortado\n   vars   n  mean   sd median trimmed  mad   min  max range  skew kurtosis   se\nX1    1 287 25.73 2.09  25.96   25.68 2.91 23.02 28.9  5.88 -0.03    -1.23 0.12\n------------------------------------------------------------ \ngroup: Espresso\n   vars   n  mean   sd median trimmed  mad   min max range  skew kurtosis   se\nX1    1 129 20.85 1.97  21.06   20.81 2.91 18.12  24  5.88 -0.12    -1.09 0.17\n------------------------------------------------------------ \ngroup: Hot Chocolate\n   vars   n  mean   sd median trimmed mad   min  max range  skew kurtosis   se\nX1    1 276 35.99 1.44  35.76   36.03   0 32.82 38.7  5.88 -0.13     0.77 0.09\n------------------------------------------------------------ \ngroup: Latte\n   vars   n mean   sd median trimmed mad   min  max range  skew kurtosis   se\nX1    1 757 35.5 1.82  35.76   35.48   0 32.82 38.7  5.88 -0.18    -0.82 0.07\n\n Descriptive statistics by group \ngroup: Afternoon\n   vars    n  mean   sd median trimmed  mad   min  max range  skew kurtosis\nX1    1 1205 31.64 4.92  32.82   31.95 4.36 18.12 38.7 20.58 -0.53    -0.75\n     se\nX1 0.14\n------------------------------------------------------------ \ngroup: Morning\n   vars    n  mean   sd median trimmed  mad   min  max range  skew kurtosis\nX1    1 1181 30.42 4.94  30.86    30.6 7.26 18.12 38.7 20.58 -0.25     -0.8\n     se\nX1 0.14\n------------------------------------------------------------ \ngroup: Night\n   vars    n  mean   sd median trimmed  mad   min  max range  skew kurtosis\nX1    1 1161 32.89 4.43   33.8   33.38 2.91 18.12 38.7 20.58 -0.91    -0.02\n     se\nX1 0.13\n\n Descriptive statistics by group \ngroup: Fri\n   vars   n  mean   sd median trimmed  mad   min  max range  skew kurtosis   se\nX1    1 532 31.58 4.91  32.82   31.87 4.36 18.12 38.7 20.58 -0.51    -0.77 0.21\n------------------------------------------------------------ \ngroup: Mon\n   vars   n  mean   sd median trimmed  mad   min  max range  skew kurtosis   se\nX1    1 544 31.92 4.51  32.82   32.17 4.36 18.12 38.7 20.58 -0.53    -0.72 0.19\n------------------------------------------------------------ \ngroup: Sat\n   vars   n  mean sd median trimmed  mad   min  max range  skew kurtosis   se\nX1    1 470 31.35  5  32.82   31.66 4.36 18.12 38.7 20.58 -0.52    -0.58 0.23\n------------------------------------------------------------ \ngroup: Sun\n   vars   n  mean   sd median trimmed  mad   min  max range  skew kurtosis   se\nX1    1 419 31.83 4.88  32.82   32.22 4.36 18.12 38.7 20.58 -0.59    -0.64 0.24\n------------------------------------------------------------ \ngroup: Thu\n   vars   n  mean   sd median trimmed  mad   min  max range  skew kurtosis   se\nX1    1 510 31.55 5.19  32.82   31.95 4.36 18.12 38.7 20.58 -0.57    -0.76 0.23\n------------------------------------------------------------ \ngroup: Tue\n   vars   n  mean   sd median trimmed  mad   min  max range  skew kurtosis  se\nX1    1 572 31.76 4.74  32.82   32.06 4.36 18.12 38.7 20.58 -0.53    -0.62 0.2\n------------------------------------------------------------ \ngroup: Wed\n   vars   n mean   sd median trimmed  mad   min  max range  skew kurtosis   se\nX1    1 500 31.5 4.94  32.82   31.85 4.36 18.12 38.7 20.58 -0.51     -0.8 0.22\n\n Descriptive statistics by group \ngroup: Apr\n   vars   n  mean   sd median trimmed  mad min  max range  skew kurtosis   se\nX1    1 168 34.05 4.49   33.8   34.33 7.26  24 38.7  14.7 -0.38    -1.31 0.35\n------------------------------------------------------------ \ngroup: Aug\n   vars   n  mean   sd median trimmed  mad   min   max range skew kurtosis   se\nX1    1 272 27.99 4.63  27.92   28.32 7.26 18.12 32.82  14.7 -0.4    -1.09 0.28\n------------------------------------------------------------ \ngroup: Dec\n   vars   n  mean   sd median trimmed mad   min   max range  skew kurtosis   se\nX1    1 259 31.81 4.61  35.76   32.31   0 21.06 35.76  14.7 -0.72    -0.78 0.29\n------------------------------------------------------------ \ngroup: Feb\n   vars   n  mean   sd median trimmed  mad   min   max range  skew kurtosis\nX1    1 423 31.24 4.69  30.86   31.58 7.26 21.06 35.76  14.7 -0.43    -1.23\n     se\nX1 0.23\n------------------------------------------------------------ \ngroup: Jan\n   vars   n  mean   sd median trimmed  mad   min   max range  skew kurtosis\nX1    1 201 31.84 4.33  30.86   32.23 7.26 21.06 35.76  14.7 -0.61    -0.92\n     se\nX1 0.31\n------------------------------------------------------------ \ngroup: Jul\n   vars   n  mean   sd median trimmed  mad   min   max range skew kurtosis   se\nX1    1 237 29.18 4.77  27.92   29.46 7.26 18.12 37.72  19.6 -0.5    -0.58 0.31\n------------------------------------------------------------ \ngroup: Jun\n   vars   n  mean   sd median trimmed mad   min   max range  skew kurtosis   se\nX1    1 223 34.16 4.29  37.72   34.76   0 23.02 37.72  14.7 -0.96    -0.06 0.29\n------------------------------------------------------------ \ngroup: Mar\n   vars   n  mean   sd median trimmed  mad   min  max range skew kurtosis   se\nX1    1 494 32.17 4.91   33.8    32.3 7.26 21.06 38.7 17.64 -0.3    -1.19 0.22\n------------------------------------------------------------ \ngroup: May\n   vars   n  mean   sd median trimmed mad   min   max range  skew kurtosis   se\nX1    1 241 33.88 4.44  37.72   34.32   0 23.02 37.72  14.7 -0.67     -0.9 0.29\n------------------------------------------------------------ \ngroup: Nov\n   vars   n  mean   sd median trimmed mad   min   max range  skew kurtosis   se\nX1    1 259 33.17 3.84  35.76   33.79   0 21.06 35.76  14.7 -1.18     0.12 0.24\n------------------------------------------------------------ \ngroup: Oct\n   vars   n  mean   sd median trimmed mad   min   max range  skew kurtosis   se\nX1    1 426 32.61 4.29  35.76   33.21   0 21.06 35.76  14.7 -1.01    -0.29 0.21\n------------------------------------------------------------ \ngroup: Sep\n   vars   n  mean   sd median trimmed  mad   min   max range  skew kurtosis\nX1    1 344 29.04 4.46  27.92   29.33 7.26 18.12 35.76 17.64 -0.57    -0.62\n     se\nX1 0.24\n\n Descriptive statistics by group \ngroup: card\n   vars    n mean   sd median trimmed  mad min max range skew kurtosis   se\nX1    1 3547 3.85 1.97      4    3.81 2.97   1   7     6 0.08    -1.23 0.03\n\n Descriptive statistics by group \ngroup: Americano\n   vars   n mean   sd median trimmed  mad min max range skew kurtosis   se\nX1    1 564 3.74 1.89      4    3.69 2.97   1   7     6 0.06    -1.14 0.08\n------------------------------------------------------------ \ngroup: Americano with Milk\n   vars   n mean   sd median trimmed  mad min max range skew kurtosis   se\nX1    1 809 3.82 2.02      4    3.78 2.97   1   7     6 0.12     -1.3 0.07\n------------------------------------------------------------ \ngroup: Cappuccino\n   vars   n mean sd median trimmed  mad min max range skew kurtosis   se\nX1    1 486 3.99  2      4    3.99 2.97   1   7     6 0.01    -1.23 0.09\n------------------------------------------------------------ \ngroup: Cocoa\n   vars   n mean   sd median trimmed  mad min max range skew kurtosis   se\nX1    1 239  3.7 1.94      4    3.63 2.97   1   7     6 0.17    -1.26 0.13\n------------------------------------------------------------ \ngroup: Cortado\n   vars   n mean   sd median trimmed  mad min max range  skew kurtosis   se\nX1    1 287 4.18 2.01      4    4.22 2.97   1   7     6 -0.14     -1.3 0.12\n------------------------------------------------------------ \ngroup: Espresso\n   vars   n mean   sd median trimmed  mad min max range skew kurtosis   se\nX1    1 129 4.08 1.77      4    4.08 1.48   1   7     6 0.07    -0.93 0.16\n------------------------------------------------------------ \ngroup: Hot Chocolate\n   vars   n mean   sd median trimmed  mad min max range skew kurtosis   se\nX1    1 276 3.91 1.95      4    3.89 2.97   1   7     6 0.11    -1.15 0.12\n------------------------------------------------------------ \ngroup: Latte\n   vars   n mean   sd median trimmed  mad min max range skew kurtosis   se\nX1    1 757 3.71 1.99      4    3.64 2.97   1   7     6 0.15    -1.23 0.07\n\n Descriptive statistics by group \ngroup: Afternoon\n   vars    n mean   sd median trimmed  mad min max range  skew kurtosis   se\nX1    1 1205 4.04 2.01      4    4.05 2.97   1   7     6 -0.06    -1.27 0.06\n------------------------------------------------------------ \ngroup: Morning\n   vars    n mean   sd median trimmed  mad min max range skew kurtosis   se\nX1    1 1181 3.75 1.97      4    3.69 2.97   1   7     6 0.12    -1.25 0.06\n------------------------------------------------------------ \ngroup: Night\n   vars    n mean   sd median trimmed  mad min max range skew kurtosis   se\nX1    1 1161 3.74 1.92      4    3.67 2.97   1   7     6 0.19    -1.11 0.06\n\n Descriptive statistics by group \ngroup: Fri\n   vars   n mean sd median trimmed mad min max range skew kurtosis se\nX1    1 532    5  0      5       5   0   5   5     0  NaN      NaN  0\n------------------------------------------------------------ \ngroup: Mon\n   vars   n mean sd median trimmed mad min max range skew kurtosis se\nX1    1 544    1  0      1       1   0   1   1     0  NaN      NaN  0\n------------------------------------------------------------ \ngroup: Sat\n   vars   n mean sd median trimmed mad min max range skew kurtosis se\nX1    1 470    6  0      6       6   0   6   6     0  NaN      NaN  0\n------------------------------------------------------------ \ngroup: Sun\n   vars   n mean sd median trimmed mad min max range skew kurtosis se\nX1    1 419    7  0      7       7   0   7   7     0  NaN      NaN  0\n------------------------------------------------------------ \ngroup: Thu\n   vars   n mean sd median trimmed mad min max range skew kurtosis se\nX1    1 510    4  0      4       4   0   4   4     0  NaN      NaN  0\n------------------------------------------------------------ \ngroup: Tue\n   vars   n mean sd median trimmed mad min max range skew kurtosis se\nX1    1 572    2  0      2       2   0   2   2     0  NaN      NaN  0\n------------------------------------------------------------ \ngroup: Wed\n   vars   n mean sd median trimmed mad min max range skew kurtosis se\nX1    1 500    3  0      3       3   0   3   3     0  NaN      NaN  0\n\n Descriptive statistics by group \ngroup: Apr\n   vars   n mean   sd median trimmed  mad min max range skew kurtosis   se\nX1    1 168 3.89 2.06      4    3.87 2.97   1   7     6 0.03    -1.33 0.16\n------------------------------------------------------------ \ngroup: Aug\n   vars   n mean   sd median trimmed  mad min max range  skew kurtosis   se\nX1    1 272 4.04 2.01      4    4.06 2.97   1   7     6 -0.07    -1.27 0.12\n------------------------------------------------------------ \ngroup: Dec\n   vars   n mean   sd median trimmed  mad min max range skew kurtosis   se\nX1    1 259 3.97 2.12      4    3.96 2.97   1   7     6 0.05    -1.42 0.13\n------------------------------------------------------------ \ngroup: Feb\n   vars   n mean   sd median trimmed  mad min max range skew kurtosis   se\nX1    1 423 3.47 1.87      3    3.37 2.97   1   7     6 0.25       -1 0.09\n------------------------------------------------------------ \ngroup: Jan\n   vars   n mean  sd median trimmed  mad min max range  skew kurtosis   se\nX1    1 201 3.83 1.8      4    3.85 1.48   1   7     6 -0.11    -1.13 0.13\n------------------------------------------------------------ \ngroup: Jul\n   vars   n mean   sd median trimmed  mad min max range skew kurtosis   se\nX1    1 237 3.78 1.85      3    3.73 1.48   1   7     6 0.23    -1.12 0.12\n------------------------------------------------------------ \ngroup: Jun\n   vars   n mean   sd median trimmed  mad min max range  skew kurtosis   se\nX1    1 223 4.17 2.02      4    4.21 2.97   1   7     6 -0.06     -1.3 0.14\n------------------------------------------------------------ \ngroup: Mar\n   vars   n mean   sd median trimmed  mad min max range skew kurtosis   se\nX1    1 494 3.82 1.87      4     3.8 2.97   1   7     6 0.03    -1.17 0.08\n------------------------------------------------------------ \ngroup: May\n   vars   n mean   sd median trimmed  mad min max range skew kurtosis   se\nX1    1 241 3.84 1.97      4     3.8 2.97   1   7     6 0.15    -1.12 0.13\n------------------------------------------------------------ \ngroup: Nov\n   vars   n mean   sd median trimmed  mad min max range  skew kurtosis   se\nX1    1 259 4.08 1.99      4    4.11 2.97   1   7     6 -0.16    -1.33 0.12\n------------------------------------------------------------ \ngroup: Oct\n   vars   n mean   sd median trimmed  mad min max range skew kurtosis   se\nX1    1 426  3.7 1.94      4    3.63 2.97   1   7     6 0.23    -1.11 0.09\n------------------------------------------------------------ \ngroup: Sep\n   vars   n mean   sd median trimmed  mad min max range skew kurtosis   se\nX1    1 344 3.91 2.15      4    3.88 2.97   1   7     6 0.07    -1.44 0.12\n\n Descriptive statistics by group \ngroup: card\n   vars    n mean  sd median trimmed  mad min max range skew kurtosis   se\nX1    1 3547 6.45 3.5      7    6.42 4.45   1  12    11    0    -1.38 0.06\n\n Descriptive statistics by group \ngroup: Americano\n   vars   n mean   sd median trimmed  mad min max range skew kurtosis   se\nX1    1 564 5.23 3.35      4    4.92 2.97   1  12    11 0.62    -1.03 0.14\n------------------------------------------------------------ \ngroup: Americano with Milk\n   vars   n mean   sd median trimmed  mad min max range  skew kurtosis   se\nX1    1 809 6.59 3.39      7     6.6 4.45   1  12    11 -0.11    -1.24 0.12\n------------------------------------------------------------ \ngroup: Cappuccino\n   vars   n mean   sd median trimmed  mad min max range skew kurtosis   se\nX1    1 486 6.24 3.35      6    6.15 4.45   1  12    11 0.17    -1.19 0.15\n------------------------------------------------------------ \ngroup: Cocoa\n   vars   n mean   sd median trimmed  mad min max range skew kurtosis   se\nX1    1 239 6.29 4.02      6     6.2 5.93   1  12    11  0.1    -1.71 0.26\n------------------------------------------------------------ \ngroup: Cortado\n   vars   n mean   sd median trimmed  mad min max range  skew kurtosis  se\nX1    1 287  7.1 3.37      8    7.22 2.97   1  12    11 -0.32    -1.07 0.2\n------------------------------------------------------------ \ngroup: Espresso\n   vars   n mean   sd median trimmed  mad min max range skew kurtosis  se\nX1    1 129 6.37 3.37      7    6.28 4.45   1  12    11 0.06    -1.24 0.3\n------------------------------------------------------------ \ngroup: Hot Chocolate\n   vars   n mean   sd median trimmed  mad min max range  skew kurtosis   se\nX1    1 276 6.96 3.81    7.5    7.02 5.19   1  12    11 -0.17     -1.6 0.23\n------------------------------------------------------------ \ngroup: Latte\n   vars   n mean   sd median trimmed  mad min max range  skew kurtosis   se\nX1    1 757    7 3.37      8    7.11 4.45   1  12    11 -0.29    -1.24 0.12\n\n Descriptive statistics by group \ngroup: Afternoon\n   vars    n mean   sd median trimmed  mad min max range skew kurtosis  se\nX1    1 1205 6.02 3.61      5     5.9 4.45   1  12    11 0.23    -1.45 0.1\n------------------------------------------------------------ \ngroup: Morning\n   vars    n mean   sd median trimmed  mad min max range  skew kurtosis  se\nX1    1 1181 6.74 3.44      7    6.77 4.45   1  12    11 -0.15    -1.32 0.1\n------------------------------------------------------------ \ngroup: Night\n   vars    n mean   sd median trimmed  mad min max range  skew kurtosis  se\nX1    1 1161 6.61 3.41      7    6.61 4.45   1  12    11 -0.06    -1.27 0.1\n\n Descriptive statistics by group \ngroup: Fri\n   vars   n mean   sd median trimmed  mad min max range skew kurtosis   se\nX1    1 532 6.08 3.57      6       6 4.45   1  12    11 0.14    -1.44 0.15\n------------------------------------------------------------ \ngroup: Mon\n   vars   n mean   sd median trimmed  mad min max range skew kurtosis   se\nX1    1 544  6.3 3.58      6    6.22 4.45   1  12    11 0.06    -1.46 0.15\n------------------------------------------------------------ \ngroup: Sat\n   vars   n mean   sd median trimmed  mad min max range  skew kurtosis   se\nX1    1 470 6.75 3.53      7     6.8 4.45   1  12    11 -0.11    -1.35 0.16\n------------------------------------------------------------ \ngroup: Sun\n   vars   n mean   sd median trimmed  mad min max range skew kurtosis   se\nX1    1 419 7.18 3.25      8    7.25 4.45   1  12    11 -0.2    -1.17 0.16\n------------------------------------------------------------ \ngroup: Thu\n   vars   n mean   sd median trimmed  mad min max range skew kurtosis   se\nX1    1 510 6.21 3.47      6    6.15 4.45   1  12    11 0.07    -1.37 0.15\n------------------------------------------------------------ \ngroup: Tue\n   vars   n mean   sd median trimmed  mad min max range  skew kurtosis   se\nX1    1 572 6.79 3.47      7    6.82 4.45   1  12    11 -0.14    -1.36 0.15\n------------------------------------------------------------ \ngroup: Wed\n   vars   n mean   sd median trimmed  mad min max range skew kurtosis   se\nX1    1 500 6.01 3.44      6    5.86 4.45   1  12    11  0.2    -1.33 0.15\n\n Descriptive statistics by group \ngroup: Apr\n   vars   n mean sd median trimmed mad min max range skew kurtosis se\nX1    1 168    4  0      4       4   0   4   4     0  NaN      NaN  0\n------------------------------------------------------------ \ngroup: Aug\n   vars   n mean sd median trimmed mad min max range skew kurtosis se\nX1    1 272    8  0      8       8   0   8   8     0  NaN      NaN  0\n------------------------------------------------------------ \ngroup: Dec\n   vars   n mean sd median trimmed mad min max range skew kurtosis se\nX1    1 259   12  0     12      12   0  12  12     0  NaN      NaN  0\n------------------------------------------------------------ \ngroup: Feb\n   vars   n mean sd median trimmed mad min max range skew kurtosis se\nX1    1 423    2  0      2       2   0   2   2     0  NaN      NaN  0\n------------------------------------------------------------ \ngroup: Jan\n   vars   n mean sd median trimmed mad min max range skew kurtosis se\nX1    1 201    1  0      1       1   0   1   1     0  NaN      NaN  0\n------------------------------------------------------------ \ngroup: Jul\n   vars   n mean sd median trimmed mad min max range skew kurtosis se\nX1    1 237    7  0      7       7   0   7   7     0  NaN      NaN  0\n------------------------------------------------------------ \ngroup: Jun\n   vars   n mean sd median trimmed mad min max range skew kurtosis se\nX1    1 223    6  0      6       6   0   6   6     0  NaN      NaN  0\n------------------------------------------------------------ \ngroup: Mar\n   vars   n mean sd median trimmed mad min max range skew kurtosis se\nX1    1 494    3  0      3       3   0   3   3     0  NaN      NaN  0\n------------------------------------------------------------ \ngroup: May\n   vars   n mean sd median trimmed mad min max range skew kurtosis se\nX1    1 241    5  0      5       5   0   5   5     0  NaN      NaN  0\n------------------------------------------------------------ \ngroup: Nov\n   vars   n mean sd median trimmed mad min max range skew kurtosis se\nX1    1 259   11  0     11      11   0  11  11     0  NaN      NaN  0\n------------------------------------------------------------ \ngroup: Oct\n   vars   n mean sd median trimmed mad min max range skew kurtosis se\nX1    1 426   10  0     10      10   0  10  10     0  NaN      NaN  0\n------------------------------------------------------------ \ngroup: Sep\n   vars   n mean sd median trimmed mad min max range skew kurtosis se\nX1    1 344    9  0      9       9   0   9   9     0  NaN      NaN  0\n\n\n\n\n2.2.2.2 Graphic\n\nfor (varC in varCat) {\n  for (varN in varNum) {\n    # Histogram by group in ggplot2\n  grafico &lt;- ggplot(datos, aes(x = get(varN), fill = get(varC))) + \n    geom_histogram(colour = \"black\",\n                   lwd = 0.75,\n                   linetype = 1,\n                   position = \"identity\")\n  print(grafico)\n  }\n}\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\n\n\n\n2.2.3 Categorical vs. categorical\n\n2.2.3.1 Description\n\nfor (varc1 in varCat) {\n  for (varc2 in varCat) {\n    if (varc1 != varc2) {\n      prop_table &lt;- prop.table(table(datos[, varc1], datos[, varc2]))\n      cat(\"=============\", varc1, \" vs. \", varc2, \"=========================\\n\")\n      print(prop_table)\n    }\n  }\n}\n\n============= cash_type  vs.  coffee_name =========================\n      \n        Americano Americano with Milk Cappuccino      Cocoa    Cortado\n  card 0.15900761          0.22808007 0.13701720 0.06738089 0.08091345\n      \n         Espresso Hot Chocolate      Latte\n  card 0.03636876    0.07781224 0.21341979\n============= cash_type  vs.  Time_of_Day =========================\n      \n       Afternoon   Morning     Night\n  card 0.3397237 0.3329574 0.3273189\n============= cash_type  vs.  Weekday =========================\n      \n             Fri       Mon       Sat       Sun       Thu       Tue       Wed\n  card 0.1499859 0.1533690 0.1325063 0.1181280 0.1437835 0.1612630 0.1409642\n============= cash_type  vs.  Month_name =========================\n      \n              Apr        Aug        Dec        Feb        Jan        Jul\n  card 0.04736397 0.07668452 0.07301945 0.11925571 0.05666761 0.06681703\n      \n              Jun        Mar        May        Nov        Oct        Sep\n  card 0.06287003 0.13927262 0.06794474 0.07301945 0.12010149 0.09698337\n============= coffee_name  vs.  cash_type =========================\n                     \n                            card\n  Americano           0.15900761\n  Americano with Milk 0.22808007\n  Cappuccino          0.13701720\n  Cocoa               0.06738089\n  Cortado             0.08091345\n  Espresso            0.03636876\n  Hot Chocolate       0.07781224\n  Latte               0.21341979\n============= coffee_name  vs.  Time_of_Day =========================\n                     \n                        Afternoon     Morning       Night\n  Americano           0.065689315 0.061742317 0.031575980\n  Americano with Milk 0.067380885 0.093318297 0.067380885\n  Cappuccino          0.046236256 0.034395264 0.056385678\n  Cocoa               0.021144629 0.016351847 0.029884409\n  Cortado             0.024809698 0.040315760 0.015787990\n  Espresso            0.015787990 0.012404849 0.008175923\n  Hot Chocolate       0.022554271 0.013814491 0.041443473\n  Latte               0.076120665 0.060614604 0.076684522\n============= coffee_name  vs.  Weekday =========================\n                     \n                              Fri         Mon         Sat         Sun\n  Americano           0.029602481 0.026219340 0.019453059 0.012968706\n  Americano with Milk 0.029038624 0.036086834 0.033831407 0.027910911\n  Cappuccino          0.017479560 0.020016916 0.019453059 0.019734987\n  Cocoa               0.014660276 0.009585565 0.006484353 0.006766281\n  Cortado             0.010431350 0.009867494 0.015787990 0.011840992\n  Espresso            0.005920496 0.002819284 0.003946997 0.004510854\n  Hot Chocolate       0.012686778 0.009867494 0.006766281 0.011277136\n  Latte               0.030166338 0.038906118 0.026783197 0.023118128\n                     \n                              Thu         Tue         Wed\n  Americano           0.023118128 0.022836200 0.024809698\n  Americano with Milk 0.029038624 0.040315760 0.031857908\n  Cappuccino          0.021708486 0.017761489 0.020862701\n  Cocoa               0.006484353 0.016069918 0.007330138\n  Cortado             0.011840992 0.012404849 0.008739780\n  Espresso            0.007612067 0.004510854 0.007048210\n  Hot Chocolate       0.013532563 0.013814491 0.009867494\n  Latte               0.030448266 0.033549478 0.030448266\n============= coffee_name  vs.  Month_name =========================\n                     \n                               Apr          Aug          Dec          Feb\n  Americano           0.0093036369 0.0104313504 0.0076120665 0.0329856217\n  Americano with Milk 0.0107132788 0.0202988441 0.0160699182 0.0239639132\n  Cappuccino          0.0101494220 0.0095855653 0.0107132788 0.0146602763\n  Cocoa               0.0011277136 0.0031012123 0.0059204962 0.0157879899\n  Cortado             0.0045108542 0.0112771356 0.0087397801 0.0028192839\n  Espresso            0.0011277136 0.0039469975 0.0033831407 0.0047927826\n  Hot Chocolate       0.0028192839 0.0016915703 0.0073301381 0.0090217085\n  Latte               0.0076120665 0.0163518466 0.0132506343 0.0152241331\n                     \n                               Jan          Jul          Jun          Mar\n  Americano           0.0070482098 0.0101494220 0.0039469975 0.0377784043\n  Americano with Milk 0.0146602763 0.0183253454 0.0186072738 0.0231181280\n  Cappuccino          0.0076120665 0.0090217085 0.0129687059 0.0163518466\n  Cocoa               0.0039469975 0.0025373555 0.0011277136 0.0101494220\n  Cortado             0.0062024246 0.0039469975 0.0053566394 0.0084578517\n  Espresso            0.0014096420 0.0039469975 0.0028192839 0.0053566394\n  Hot Chocolate       0.0042289259 0.0031012123 0.0039469975 0.0121229208\n  Latte               0.0115590640 0.0157879899 0.0140964195 0.0259374119\n                     \n                               May          Nov          Oct          Sep\n  Americano           0.0112771356 0.0070482098 0.0124048492 0.0090217085\n  Americano with Milk 0.0152241331 0.0146602763 0.0231181280 0.0293205526\n  Cappuccino          0.0146602763 0.0073301381 0.0124048492 0.0115590640\n  Cocoa               0.0022554271 0.0098674937 0.0090217085 0.0025373555\n  Cortado             0.0047927826 0.0036650691 0.0095855653 0.0115590640\n  Espresso            0.0019734987 0.0008457852 0.0033831407 0.0033831407\n  Hot Chocolate       0.0036650691 0.0104313504 0.0163518466 0.0031012123\n  Latte               0.0140964195 0.0191711305 0.0338314068 0.0265012687\n============= Time_of_Day  vs.  cash_type =========================\n           \n                 card\n  Afternoon 0.3397237\n  Morning   0.3329574\n  Night     0.3273189\n============= Time_of_Day  vs.  coffee_name =========================\n           \n              Americano Americano with Milk  Cappuccino       Cocoa     Cortado\n  Afternoon 0.065689315         0.067380885 0.046236256 0.021144629 0.024809698\n  Morning   0.061742317         0.093318297 0.034395264 0.016351847 0.040315760\n  Night     0.031575980         0.067380885 0.056385678 0.029884409 0.015787990\n           \n               Espresso Hot Chocolate       Latte\n  Afternoon 0.015787990   0.022554271 0.076120665\n  Morning   0.012404849   0.013814491 0.060614604\n  Night     0.008175923   0.041443473 0.076684522\n============= Time_of_Day  vs.  Weekday =========================\n           \n                   Fri        Mon        Sat        Sun        Thu        Tue\n  Afternoon 0.04849168 0.04990133 0.05469411 0.04736397 0.04764590 0.04510854\n  Morning   0.05441218 0.05441218 0.04426276 0.03383141 0.04116154 0.05835918\n  Night     0.04708204 0.04905554 0.03354948 0.03693262 0.05497604 0.05779532\n           \n                   Wed\n  Afternoon 0.04651818\n  Morning   0.04651818\n  Night     0.04792783\n============= Time_of_Day  vs.  Month_name =========================\n           \n                   Apr        Aug        Dec        Feb        Jan        Jul\n  Afternoon 0.02058077 0.02001692 0.02199041 0.04961940 0.02114463 0.01606992\n  Morning   0.01127714 0.03411334 0.02537356 0.03326755 0.01719763 0.02762898\n  Night     0.01550606 0.02255427 0.02565548 0.03636876 0.01832535 0.02311813\n           \n                   Jun        Mar        May        Nov        Oct        Sep\n  Afternoon 0.01522413 0.06146039 0.02199041 0.02847477 0.03834226 0.02480970\n  Morning   0.01888920 0.04623626 0.01691570 0.02368198 0.04341697 0.03495912\n  Night     0.02875670 0.03157598 0.02903862 0.02086270 0.03834226 0.03721455\n============= Weekday  vs.  cash_type =========================\n     \n           card\n  Fri 0.1499859\n  Mon 0.1533690\n  Sat 0.1325063\n  Sun 0.1181280\n  Thu 0.1437835\n  Tue 0.1612630\n  Wed 0.1409642\n============= Weekday  vs.  coffee_name =========================\n     \n        Americano Americano with Milk  Cappuccino       Cocoa     Cortado\n  Fri 0.029602481         0.029038624 0.017479560 0.014660276 0.010431350\n  Mon 0.026219340         0.036086834 0.020016916 0.009585565 0.009867494\n  Sat 0.019453059         0.033831407 0.019453059 0.006484353 0.015787990\n  Sun 0.012968706         0.027910911 0.019734987 0.006766281 0.011840992\n  Thu 0.023118128         0.029038624 0.021708486 0.006484353 0.011840992\n  Tue 0.022836200         0.040315760 0.017761489 0.016069918 0.012404849\n  Wed 0.024809698         0.031857908 0.020862701 0.007330138 0.008739780\n     \n         Espresso Hot Chocolate       Latte\n  Fri 0.005920496   0.012686778 0.030166338\n  Mon 0.002819284   0.009867494 0.038906118\n  Sat 0.003946997   0.006766281 0.026783197\n  Sun 0.004510854   0.011277136 0.023118128\n  Thu 0.007612067   0.013532563 0.030448266\n  Tue 0.004510854   0.013814491 0.033549478\n  Wed 0.007048210   0.009867494 0.030448266\n============= Weekday  vs.  Time_of_Day =========================\n     \n       Afternoon    Morning      Night\n  Fri 0.04849168 0.05441218 0.04708204\n  Mon 0.04990133 0.05441218 0.04905554\n  Sat 0.05469411 0.04426276 0.03354948\n  Sun 0.04736397 0.03383141 0.03693262\n  Thu 0.04764590 0.04116154 0.05497604\n  Tue 0.04510854 0.05835918 0.05779532\n  Wed 0.04651818 0.04651818 0.04792783\n============= Weekday  vs.  Month_name =========================\n     \n              Apr         Aug         Dec         Feb         Jan         Jul\n  Fri 0.007048210 0.009585565 0.008175923 0.020016916 0.011277136 0.010995207\n  Mon 0.008175923 0.011559064 0.011559064 0.024809698 0.008175923 0.006766281\n  Sat 0.006484353 0.013250634 0.011277136 0.009303637 0.009021708 0.007612067\n  Sun 0.006484353 0.010431350 0.012122921 0.009021708 0.002819284 0.006766281\n  Thu 0.006484353 0.012404849 0.007893995 0.017761489 0.010431350 0.007612067\n  Tue 0.007330138 0.009867494 0.012686778 0.015787990 0.007893995 0.013814491\n  Wed 0.005356639 0.009585565 0.009303637 0.022554271 0.007048210 0.013250634\n     \n              Jun         Mar         May         Nov         Oct         Sep\n  Fri 0.007330138 0.025091627 0.009021708 0.012122921 0.018325345 0.010995207\n  Mon 0.007612067 0.019453059 0.010431350 0.009585565 0.018325345 0.016915703\n  Sat 0.009867494 0.020298844 0.005638568 0.014942205 0.010713279 0.014096420\n  Sun 0.010995207 0.011277136 0.009867494 0.008175923 0.014096420 0.016069918\n  Thu 0.009303637 0.018889202 0.013532563 0.009867494 0.018043417 0.011559064\n  Tue 0.008739780 0.021426558 0.010149422 0.013250634 0.021990414 0.018325345\n  Wed 0.009021708 0.022836200 0.009303637 0.005074711 0.018607274 0.009021708\n============= Month_name  vs.  cash_type =========================\n     \n            card\n  Apr 0.04736397\n  Aug 0.07668452\n  Dec 0.07301945\n  Feb 0.11925571\n  Jan 0.05666761\n  Jul 0.06681703\n  Jun 0.06287003\n  Mar 0.13927262\n  May 0.06794474\n  Nov 0.07301945\n  Oct 0.12010149\n  Sep 0.09698337\n============= Month_name  vs.  coffee_name =========================\n     \n         Americano Americano with Milk   Cappuccino        Cocoa      Cortado\n  Apr 0.0093036369        0.0107132788 0.0101494220 0.0011277136 0.0045108542\n  Aug 0.0104313504        0.0202988441 0.0095855653 0.0031012123 0.0112771356\n  Dec 0.0076120665        0.0160699182 0.0107132788 0.0059204962 0.0087397801\n  Feb 0.0329856217        0.0239639132 0.0146602763 0.0157879899 0.0028192839\n  Jan 0.0070482098        0.0146602763 0.0076120665 0.0039469975 0.0062024246\n  Jul 0.0101494220        0.0183253454 0.0090217085 0.0025373555 0.0039469975\n  Jun 0.0039469975        0.0186072738 0.0129687059 0.0011277136 0.0053566394\n  Mar 0.0377784043        0.0231181280 0.0163518466 0.0101494220 0.0084578517\n  May 0.0112771356        0.0152241331 0.0146602763 0.0022554271 0.0047927826\n  Nov 0.0070482098        0.0146602763 0.0073301381 0.0098674937 0.0036650691\n  Oct 0.0124048492        0.0231181280 0.0124048492 0.0090217085 0.0095855653\n  Sep 0.0090217085        0.0293205526 0.0115590640 0.0025373555 0.0115590640\n     \n          Espresso Hot Chocolate        Latte\n  Apr 0.0011277136  0.0028192839 0.0076120665\n  Aug 0.0039469975  0.0016915703 0.0163518466\n  Dec 0.0033831407  0.0073301381 0.0132506343\n  Feb 0.0047927826  0.0090217085 0.0152241331\n  Jan 0.0014096420  0.0042289259 0.0115590640\n  Jul 0.0039469975  0.0031012123 0.0157879899\n  Jun 0.0028192839  0.0039469975 0.0140964195\n  Mar 0.0053566394  0.0121229208 0.0259374119\n  May 0.0019734987  0.0036650691 0.0140964195\n  Nov 0.0008457852  0.0104313504 0.0191711305\n  Oct 0.0033831407  0.0163518466 0.0338314068\n  Sep 0.0033831407  0.0031012123 0.0265012687\n============= Month_name  vs.  Time_of_Day =========================\n     \n       Afternoon    Morning      Night\n  Apr 0.02058077 0.01127714 0.01550606\n  Aug 0.02001692 0.03411334 0.02255427\n  Dec 0.02199041 0.02537356 0.02565548\n  Feb 0.04961940 0.03326755 0.03636876\n  Jan 0.02114463 0.01719763 0.01832535\n  Jul 0.01606992 0.02762898 0.02311813\n  Jun 0.01522413 0.01888920 0.02875670\n  Mar 0.06146039 0.04623626 0.03157598\n  May 0.02199041 0.01691570 0.02903862\n  Nov 0.02847477 0.02368198 0.02086270\n  Oct 0.03834226 0.04341697 0.03834226\n  Sep 0.02480970 0.03495912 0.03721455\n============= Month_name  vs.  Weekday =========================\n     \n              Fri         Mon         Sat         Sun         Thu         Tue\n  Apr 0.007048210 0.008175923 0.006484353 0.006484353 0.006484353 0.007330138\n  Aug 0.009585565 0.011559064 0.013250634 0.010431350 0.012404849 0.009867494\n  Dec 0.008175923 0.011559064 0.011277136 0.012122921 0.007893995 0.012686778\n  Feb 0.020016916 0.024809698 0.009303637 0.009021708 0.017761489 0.015787990\n  Jan 0.011277136 0.008175923 0.009021708 0.002819284 0.010431350 0.007893995\n  Jul 0.010995207 0.006766281 0.007612067 0.006766281 0.007612067 0.013814491\n  Jun 0.007330138 0.007612067 0.009867494 0.010995207 0.009303637 0.008739780\n  Mar 0.025091627 0.019453059 0.020298844 0.011277136 0.018889202 0.021426558\n  May 0.009021708 0.010431350 0.005638568 0.009867494 0.013532563 0.010149422\n  Nov 0.012122921 0.009585565 0.014942205 0.008175923 0.009867494 0.013250634\n  Oct 0.018325345 0.018325345 0.010713279 0.014096420 0.018043417 0.021990414\n  Sep 0.010995207 0.016915703 0.014096420 0.016069918 0.011559064 0.018325345\n     \n              Wed\n  Apr 0.005356639\n  Aug 0.009585565\n  Dec 0.009303637\n  Feb 0.022554271\n  Jan 0.007048210\n  Jul 0.013250634\n  Jun 0.009021708\n  Mar 0.022836200\n  May 0.009303637\n  Nov 0.005074711\n  Oct 0.018607274\n  Sep 0.009021708\n\n\n\n\n2.2.3.2 Graphic\n\nfor (varc1 in varCat) {\n  for (varc2 in varCat) {\n    if (varc1 != varc2) {\n      prop_table &lt;- prop.table(table(datos[, varc1], datos[, varc2]))\n      barplot(prop_table, beside = TRUE)\n    }\n  }\n}"
  },
  {
    "objectID": "material/Preprocessing/Preprocessing.html#features-selection",
    "href": "material/Preprocessing/Preprocessing.html#features-selection",
    "title": "Preparación de los datos para el modelado",
    "section": "3.1 Features selection",
    "text": "3.1 Features selection"
  },
  {
    "objectID": "material/Preprocessing/Preprocessing.html#outiliers",
    "href": "material/Preprocessing/Preprocessing.html#outiliers",
    "title": "Preparación de los datos para el modelado",
    "section": "3.2 Outiliers",
    "text": "3.2 Outiliers"
  },
  {
    "objectID": "material/Preprocessing/Preprocessing.html#imputation",
    "href": "material/Preprocessing/Preprocessing.html#imputation",
    "title": "Preparación de los datos para el modelado",
    "section": "3.3 Imputation",
    "text": "3.3 Imputation"
  },
  {
    "objectID": "material/Preprocessing/Preprocessing.html#new-variables",
    "href": "material/Preprocessing/Preprocessing.html#new-variables",
    "title": "Preparación de los datos para el modelado",
    "section": "3.4 New Variables",
    "text": "3.4 New Variables"
  },
  {
    "objectID": "material/Preprocessing/AdvancedPreprocessing.html",
    "href": "material/Preprocessing/AdvancedPreprocessing.html",
    "title": "Advance Preprocessing",
    "section": "",
    "text": "Para hacer el apartado de preprocessing vamos a utilizar la base de datos valentine_dataset.csv.\nEsta web está creada por Dante Conti y Sergi Ramírez, (c) 2025"
  },
  {
    "objectID": "material/FactorialMethods/ACS.html",
    "href": "material/FactorialMethods/ACS.html",
    "title": "Anàlisis de Correspondències Simples (ACS)",
    "section": "",
    "text": "L’anàlisi de correspondencies simples (ACS) s’utilitza per a descriure taules de contingència 1 (TC) mitjançant la representació geomètrica de les taules de condicionals fila i columna (perfils) derivades d’aquelles.\nL’objectiu de l’ACS és descriure les associacions entre les variables fila i columna, a través dels seus perfils:\n\nComparar els perfilis fila.\nComparar els perfilis columna.\nEstudiar les correspondències entre perfils fila i columna\n\nLa metodologia la va desenvolupar Benzecri, a principis dels anys 60 del segle XX en la Universitat de Renner (França). En essència. és un tipus especial d’anàlisi de components principals però realitzat sobre una taula de contingència i usant una distància euclidiana ponderada anomenada chi-quadrat (\\(\\chi^{2}\\))\n\n\nLa prova de chi-quadrat (\\(\\chi^{2}\\)) és un mètode estadístic que s’utilitza per a determinar si existeix una associació significativa entre variables categòriques comparant les freqüències observades i esperades en una taula de contingència.\nH0 : No hi ha associació significativa entre les variables.\nH1 : Hi ha una associació significativa entre les variables.\nPer a realitzar la prova chi-quadrat (\\(\\chi^{2}\\)):\n1- Crear una taula de contingència amb les freqüències observades per a cada categoria.\n2- Calcular les freqüències esperades assumint la independència entre les variables.\n3- Calcular l’estadístic chi-quadrat (\\(\\chi^{2}\\)).\nComparar l’estadístic calculat amb el valor crític de la distribució chi-quadrat (\\(\\chi^{2}\\)) per a determinar si es rebutja o no la hipòtesi nul·la.\n\\[\n  \\chi^{2} = \\sum{\\frac{(O_{ij}-E_{ij})^{2}}{E_{ij}}}\n\\] Ón:\n\n\\(chi-quadrat (\\)^{2}\\()\\): El estadístic de prova \\(\\chi^{2}\\), medeix la discrepancia entre els valors observats i els esperats\n\\(\\sum\\) (sigma): Suma els valors de cada cela de la taula de contingència.\n\\(O_{ij}\\): La freqüència observada en cada cel·la de la taula de contingència.\n\\(E_{ij}\\): La freqüència esperada en cada cel·la de la taula de contingència.\n\nCasos d’ús d’exemple\n\nAvaluar la relació entre les variables demogràfiques (per exemple, edat, gènere, ingressos) i les preferències del consumidor o el comportament de compra.\nExaminar l’associació entre els factors de risc i els resultats de les malalties, com el tabaquisme i la incidència el càncer de pulmó.\nExplorar la relació entre variables categòriques com el nivell educatiu i la situació laboral o l’afiliació política i el comportament electoral.\nDeterminar si els patrons d’herència observats són consistents amb les proporcions mendelianes esperades, o si uns certs marcadors genètics estan associats amb trets o malalties específiques.\nAvaluar la relació entre les variables de control de qualitat, com el tipus de defecte, i la línia de producció.\n\nSuposicions\n\nIndependència: L’ocurrència d’una observació no ha d’influir ni ser influenciada per una altra observació.\nCategòric: Totes dues variables són per a dades categòriques.\nMútuament excloents: Les observacions només poden pertànyer a una cel·la de la taula de contingència.\nGrandària de la mostra: Ha d’haver-hi almenys cinc observacions en cada cel·la de la taula de contingència.\n\nProves alternatives\nProva exacta de Fisher: adequada quan la grandària de la mostra és petit i les freqüències de cel·la esperades en la taula de contingència són inferiors a 5. Sovint s’utilitza com a alternativa a la prova de chi-quadrat en taules de contingència de 2x2.\nProva de McNemar: s’utilitza en analitzar dades categòriques aparellades, generalment en una taula de contingència de 2x2, on les observacions són dependents o estan relacionades. S’utilitza comunament en estudis d’abans i després o en estudis de casos i controls aparellats.\nProva de Cochran-Estovalles-Haenszel: s’utilitza en analitzar dades categòriques en estudis estratificats o aparellats. Permet la comparació de múltiples taules de contingència 2x2 mentre controla variables de confusió o factors d’estratificació.\n\nlibrary(\"factoextra\")\nlibrary(\"FactoMineR\")\nlibrary(\"gplots\")\nlibrary(\"dplyr\")\nEsta web está creada por Dante Conti y Sergi Ramírez, (c) 2025"
  },
  {
    "objectID": "material/FactorialMethods/ACS.html#introducció",
    "href": "material/FactorialMethods/ACS.html#introducció",
    "title": "Anàlisis de Correspondències Simples (ACS)",
    "section": "",
    "text": "L’anàlisi de correspondencies simples (ACS) s’utilitza per a descriure taules de contingència 1 (TC) mitjançant la representació geomètrica de les taules de condicionals fila i columna (perfils) derivades d’aquelles.\nL’objectiu de l’ACS és descriure les associacions entre les variables fila i columna, a través dels seus perfils:\n\nComparar els perfilis fila.\nComparar els perfilis columna.\nEstudiar les correspondències entre perfils fila i columna\n\nLa metodologia la va desenvolupar Benzecri, a principis dels anys 60 del segle XX en la Universitat de Renner (França). En essència. és un tipus especial d’anàlisi de components principals però realitzat sobre una taula de contingència i usant una distància euclidiana ponderada anomenada chi-quadrat (\\(\\chi^{2}\\))\n\n\nLa prova de chi-quadrat (\\(\\chi^{2}\\)) és un mètode estadístic que s’utilitza per a determinar si existeix una associació significativa entre variables categòriques comparant les freqüències observades i esperades en una taula de contingència.\nH0 : No hi ha associació significativa entre les variables.\nH1 : Hi ha una associació significativa entre les variables.\nPer a realitzar la prova chi-quadrat (\\(\\chi^{2}\\)):\n1- Crear una taula de contingència amb les freqüències observades per a cada categoria.\n2- Calcular les freqüències esperades assumint la independència entre les variables.\n3- Calcular l’estadístic chi-quadrat (\\(\\chi^{2}\\)).\nComparar l’estadístic calculat amb el valor crític de la distribució chi-quadrat (\\(\\chi^{2}\\)) per a determinar si es rebutja o no la hipòtesi nul·la.\n\\[\n  \\chi^{2} = \\sum{\\frac{(O_{ij}-E_{ij})^{2}}{E_{ij}}}\n\\] Ón:\n\n\\(chi-quadrat (\\)^{2}\\()\\): El estadístic de prova \\(\\chi^{2}\\), medeix la discrepancia entre els valors observats i els esperats\n\\(\\sum\\) (sigma): Suma els valors de cada cela de la taula de contingència.\n\\(O_{ij}\\): La freqüència observada en cada cel·la de la taula de contingència.\n\\(E_{ij}\\): La freqüència esperada en cada cel·la de la taula de contingència.\n\nCasos d’ús d’exemple\n\nAvaluar la relació entre les variables demogràfiques (per exemple, edat, gènere, ingressos) i les preferències del consumidor o el comportament de compra.\nExaminar l’associació entre els factors de risc i els resultats de les malalties, com el tabaquisme i la incidència el càncer de pulmó.\nExplorar la relació entre variables categòriques com el nivell educatiu i la situació laboral o l’afiliació política i el comportament electoral.\nDeterminar si els patrons d’herència observats són consistents amb les proporcions mendelianes esperades, o si uns certs marcadors genètics estan associats amb trets o malalties específiques.\nAvaluar la relació entre les variables de control de qualitat, com el tipus de defecte, i la línia de producció.\n\nSuposicions\n\nIndependència: L’ocurrència d’una observació no ha d’influir ni ser influenciada per una altra observació.\nCategòric: Totes dues variables són per a dades categòriques.\nMútuament excloents: Les observacions només poden pertànyer a una cel·la de la taula de contingència.\nGrandària de la mostra: Ha d’haver-hi almenys cinc observacions en cada cel·la de la taula de contingència.\n\nProves alternatives\nProva exacta de Fisher: adequada quan la grandària de la mostra és petit i les freqüències de cel·la esperades en la taula de contingència són inferiors a 5. Sovint s’utilitza com a alternativa a la prova de chi-quadrat en taules de contingència de 2x2.\nProva de McNemar: s’utilitza en analitzar dades categòriques aparellades, generalment en una taula de contingència de 2x2, on les observacions són dependents o estan relacionades. S’utilitza comunament en estudis d’abans i després o en estudis de casos i controls aparellats.\nProva de Cochran-Estovalles-Haenszel: s’utilitza en analitzar dades categòriques en estudis estratificats o aparellats. Permet la comparació de múltiples taules de contingència 2x2 mentre controla variables de confusió o factors d’estratificació.\n\nlibrary(\"factoextra\")\nlibrary(\"FactoMineR\")\nlibrary(\"gplots\")\nlibrary(\"dplyr\")"
  },
  {
    "objectID": "material/FactorialMethods/ACS.html#definició-del-problema",
    "href": "material/FactorialMethods/ACS.html#definició-del-problema",
    "title": "Anàlisis de Correspondències Simples (ACS)",
    "section": "2 Definició del problema",
    "text": "2 Definició del problema\n\ndata(\"housetasks\")\nhead(housetasks)\n\n           Wife Alternating Husband Jointly\nLaundry     156          14       2       4\nMain_meal   124          20       5       4\nDinner       77          11       7      13\nBreakfeast   82          36      15       7\nTidying      53          11       1      57\nDishes       32          24       4      53\n\ncolnames(housetasks) &lt;- c(\"Dona\", \"Alternant\", \"Marit\", \"Conjuntament\")\nrownames(housetasks) &lt;- c(\"Bugaderia\", \"Dinar\", \"Sopar\", \"Esmorçar\", \"Ordenar\", \"Netejar_Plats\", \"Compres\", \"Oficial\", \"Conduir\", \"Finances\", \"Assegurança\", \"Reparacions\", \"Vacances\")\n\ndf &lt;- as.table(as.matrix(housetasks))\ndf\n\n              Dona Alternant Marit Conjuntament\nBugaderia      156        14     2            4\nDinar          124        20     5            4\nSopar           77        11     7           13\nEsmorçar        82        36    15            7\nOrdenar         53        11     1           57\nNetejar_Plats   32        24     4           53\nCompres         33        23     9           55\nOficial         12        46    23           15\nConduir         10        51    75            3\nFinances        13        13    21           66\nAssegurança      8         1    53           77\nReparacions      0         3   160            2\nVacances         0         1     6          153\n\n\n\nballoonplot(t(df), label=F, main=\"Tareas del hogar\")"
  },
  {
    "objectID": "material/FactorialMethods/ACS.html#prova-de-la-chi2",
    "href": "material/FactorialMethods/ACS.html#prova-de-la-chi2",
    "title": "Anàlisis de Correspondències Simples (ACS)",
    "section": "3 Prova de la \\(\\chi^{2}\\)",
    "text": "3 Prova de la \\(\\chi^{2}\\)\n\\(H_{0}\\): Variables independents (hipòtesi nula) \\(H_{1}\\): Variables dependents (hipòtesi alternativa)\n\nchisq.test(housetasks)\n\n\n    Pearson's Chi-squared test\n\ndata:  housetasks\nX-squared = 1944.5, df = 36, p-value &lt; 2.2e-16\n\n\nEs refusa l’hipòtesi nula en favor de la alternativa, les parelles s’organitzen per fer les tasques de la llar."
  },
  {
    "objectID": "material/FactorialMethods/ACS.html#anàlisi-de-correspondència-simple-acs",
    "href": "material/FactorialMethods/ACS.html#anàlisi-de-correspondència-simple-acs",
    "title": "Anàlisis de Correspondències Simples (ACS)",
    "section": "4 Anàlisi de Correspondència Simple (ACS)",
    "text": "4 Anàlisi de Correspondència Simple (ACS)\n\nhousetasks_CA &lt;- CA(housetasks, graph = F)\nprint(housetasks_CA)\n\n**Results of the Correspondence Analysis (CA)**\nThe row variable has  13  categories; the column variable has 4 categories\nThe chi square of independence between the two variables is equal to 1944.456 (p-value =  0 ).\n*The results are available in the following objects:\n\n   name              description                   \n1  \"$eig\"            \"eigenvalues\"                 \n2  \"$col\"            \"results for the columns\"     \n3  \"$col$coord\"      \"coord. for the columns\"      \n4  \"$col$cos2\"       \"cos2 for the columns\"        \n5  \"$col$contrib\"    \"contributions of the columns\"\n6  \"$row\"            \"results for the rows\"        \n7  \"$row$coord\"      \"coord. for the rows\"         \n8  \"$row$cos2\"       \"cos2 for the rows\"           \n9  \"$row$contrib\"    \"contributions of the rows\"   \n10 \"$call\"           \"summary called parameters\"   \n11 \"$call$marge.col\" \"weights of the columns\"      \n12 \"$call$marge.row\" \"weights of the rows\"         \n\n\n\nhousetasks_CA$col\n\n$coord\n                   Dim 1      Dim 2       Dim 3\nDona         -0.83762154  0.3652207 -0.19991139\nAlternant    -0.06218462  0.2915938  0.84858939\nMarit         1.16091847  0.6019199 -0.18885924\nConjuntament  0.14942609 -1.0265791 -0.04644302\n\n$contrib\n                 Dim 1     Dim 2      Dim 3\nDona         44.462018 10.312237 10.8220753\nAlternant     0.103739  2.782794 82.5492464\nMarit        54.233879 17.786612  6.1331792\nConjuntament  1.200364 69.118357  0.4954991\n\n$cos2\n                   Dim 1     Dim 2       Dim 3\nDona         0.801875947 0.1524482 0.045675847\nAlternant    0.004779897 0.1051016 0.890118521\nMarit        0.772026244 0.2075420 0.020431728\nConjuntament 0.020705858 0.9772939 0.002000236\n\n$inertia\n[1] 0.3010185 0.1178242 0.3813729 0.3147248\n\n\n\nhousetasks_CA$row\n\n$coord\n                   Dim 1      Dim 2       Dim 3\nBugaderia     -0.9918368  0.4953220 -0.31672897\nDinar         -0.8755855  0.4901092 -0.16406487\nSopar         -0.6925740  0.3081043 -0.20741377\nEsmorçar      -0.5086002  0.4528038  0.22040453\nOrdenar       -0.3938084 -0.4343444 -0.09421375\nNetejar_Plats -0.1889641 -0.4419662  0.26694926\nCompres       -0.1176813 -0.4033171  0.20261512\nOficial        0.2266324  0.2536132  0.92336416\nConduir        0.7417696  0.6534143  0.54445849\nFinances       0.2707669 -0.6178684  0.03479681\nAssegurança    0.6470759 -0.4737832 -0.28936051\nReparacions    1.5287787  0.8642647 -0.47208778\nVacances       0.2524863 -1.4350066 -0.12958665\n\n$contrib\n                   Dim 1      Dim 2       Dim 3\nBugaderia     18.2867003  5.5638913  7.96842443\nDinar         12.3888433  4.7355230  1.85868941\nSopar          5.4713982  1.3210221  2.09692603\nEsmorçar       3.8249284  3.6986131  3.06939857\nOrdenar        1.9983518  2.9656441  0.48873403\nNetejar_Plats  0.4261663  2.8441170  3.63429434\nCompres        0.1755248  2.5151584  2.22335679\nOficial        0.5207837  0.7956201 36.94038942\nConduir        8.0778371  7.6468564 18.59638635\nFinances       0.8750075  5.5585460  0.06175066\nAssegurança    6.1470616  4.0203590  5.25263863\nReparacions   40.7300940 15.8806509 16.59639139\nVacances       1.0773030 42.4539986  1.21261994\n\n$cos2\n                   Dim 1      Dim 2       Dim 3\nBugaderia     0.73998741 0.18455213 0.075460467\nDinar         0.74160285 0.23235928 0.026037873\nSopar         0.77664011 0.15370323 0.069656660\nEsmorçar      0.50494329 0.40023001 0.094826699\nOrdenar       0.43981243 0.53501508 0.025172490\nNetejar_Plats 0.11811778 0.64615253 0.235729693\nCompres       0.06365362 0.74765514 0.188691242\nOficial       0.05304464 0.06642648 0.880528877\nConduir       0.43201860 0.33522911 0.232752289\nFinances      0.16067678 0.83666958 0.002653634\nAssegurança   0.57601197 0.30880208 0.115185951\nReparacions   0.70673575 0.22587147 0.067392778\nVacances      0.02979239 0.96235977 0.007847841\n\n$inertia\n [1] 0.13415976 0.09069235 0.03824633 0.04112368 0.02466697 0.01958732\n [7] 0.01497017 0.05330000 0.10150885 0.02956446 0.05793584 0.31287411\n[13] 0.19631064\n\n\n\nfviz_screeplot(housetasks_CA, addlabel=T)\n\n\n\n\n\n\n\n\nEl 89% de la variança de les variables están explicades per les dimensiones 1 i 2.\n\nfviz_ca_biplot(housetasks_CA,repel = T)\n\n\n\n\n\n\n\n\nLa descripció del gràfic és el següent:\n\nBlau: Corresponen a les files\nVermell: Corresponen a les columnes\n\nD’aqui podem extreure les següents conclusions:\n1- Les tasques de dinar, sopar, estendre i esmorçar son realitzades amb més freqüéncia per les dones.\n2- Les tasques de conduir i fer reparacions es realitzen amb més freqüéncia pels marits.\n3- Les tasques de vacances, finances i seguretat ho fan en conjunt.\nPer tal de poder descriure les dimensions, podem realitzar un gràfic de correlacions.\n\nlibrary(corrplot)\ncorrplot(housetasks_CA$col$cos2)\n\n\n\n\n\n\n\n\nUtilitzem la distància \\(cos^{2}\\) per la variable de tasques.\n\ncorrplot(housetasks_CA$row$cos2 )\n\n\n\n\n\n\n\n\nD’aquí podem extreure que:\n\nLa 1a component fa referència a tasques realitzades de manera individual\nLa 2a component fa referència a tasques realitzades de manera col·lectiva.\n\nA continuació anem a veure la contribució de cada columna a cada dimensió:\n\nfviz_contrib(housetasks_CA, choice = \"col\" ,axes = 1)\n\n\n\n\n\n\n\n\n\nfviz_contrib(housetasks_CA, choice = \"col\" ,axes = 2)\n\n\n\n\n\n\n\n\n\nfviz_contrib(housetasks_CA, choice = \"col\" ,axes = 1:2)\n\n\n\n\n\n\n\n\n\nfviz_contrib(housetasks_CA, choice = \"row\" ,axes = 1)\n\n\n\n\n\n\n\n\n\nfviz_contrib(housetasks_CA, choice = \"row\" ,axes = 2)\n\n\n\n\n\n\n\n\n\nfviz_contrib(housetasks_CA, choice = \"row\" ,axes = 1:2)\n\n\n\n\n\n\n\n\n\nfviz_ca_biplot(housetasks_CA,repel = T, arrow = c(F,T), col.col = \"cos2\", \n               gradient.cols = c(\"red\", \"yellow\", \"green\"),\n               alpha.col = \"contrib\")"
  },
  {
    "objectID": "material/FactorialMethods/ACS.html#footnotes",
    "href": "material/FactorialMethods/ACS.html#footnotes",
    "title": "Anàlisis de Correspondències Simples (ACS)",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nEn estadística les taules de contingència s’empren per a registrar i analitzar l’associació entre dues o més variables, habitualment de naturalesa qualitativa (nominals o ordinals).↩︎"
  },
  {
    "objectID": "material/Preprocessing/AdvancedPreprocessing.html#univariate",
    "href": "material/Preprocessing/AdvancedPreprocessing.html#univariate",
    "title": "Advance Preprocessing",
    "section": "3.1 Univariate",
    "text": "3.1 Univariate\nEn este caso analizamos variable por variable.\n\n3.1.1 Max and Min\nLa primera estrategia consiste en observar los valores mínimos y máximos de cada variable numérica. Esto nos da una primera idea de los rangos de los datos y de si existen valores extraños.\n\nmapply(function(x, name) {\n  cat(\"var. \", name, \": \\n\\t min: \", min(x), \"\\n\\t max: \", max(x), \"\\n\")\n  invisible(NULL)  # Evita la salida de valores NULL\n}, dades[, varNum], colnames(dades[, varNum]))\n\nvar.  Age : \n     min:  18 \n     max:  40 \nvar.  Income : \n     min:  20004 \n     max:  79998 \nvar.  Appearance_Score : \n     min:  0 \n     max:  99.99 \nvar.  Interests_Score : \n     min:  0.01 \n     max:  100 \nvar.  Confidence_Score : \n     min:  0.01 \n     max:  100 \n\n\n$Age\nNULL\n\n$Income\nNULL\n\n$Appearance_Score\nNULL\n\n$Interests_Score\nNULL\n\n$Confidence_Score\nNULL\n\n\n\n\n3.1.2 IQR\nOtra manera de detectar outliers es usando el rango intercuartílico (IQR). Se definen como outliers los puntos que quedan fuera del intervalo:\n\\[\n[Q1 - 1.5xIQR, Q3 + 1.5xIQR]\n\\]\ndonde \\(Q1\\) es el primer cuartil, \\(Q3\\) el tercer cuartil e \\(IQR = Q3 - Q1\\).\n\nlibrary(EnvStats)\n\nIQROutlier &lt;- function(variable, rmnas = TRUE) {\n  IQ &lt;- iqr(variable, na.rm = rmnas)\n  intInf &lt;- quantile(variable, probs = c(0.25, 0.75))[[1]] - 1.5*IQ\n  intSup &lt;- quantile(variable, probs = c(0.25, 0.75))[[2]] + 1.5*IQ\n  posicions &lt;- which(variable &gt;= intSup | variable &lt;= intInf)\n  if (length(posicions) &gt; 0) {\n    cat(\"Existeixen outliers en les posicions:\", paste0(posicions, collapse = \", \"))\n  } else {\n    cat(\"No existeixen outliers\")\n  }\n  return(posicions)\n}\n\n\n\nNo existeixen outliers\n\n\ninteger(0)\n\n\n\n👉 Aquí veremos cuántos valores son considerados extremos según este criterio en cada variable numérica.\n\n\n\n3.1.3 Boxplot\nVisualització basada en IQR per detectar outliers.\n\nlibrary(ggplot2)\n\nvariable &lt;- \"Age\"\n\nboxplot(dades[, variable])\n\n\n\n\n\n\n\nboxplot.stats(dades[, variable])$out\n\ninteger(0)\n\n# Crear un boxplot\nggplot(dades, aes(y = get(variable))) +\n  geom_boxplot(fill = \"skyblue\", color = \"black\") +\n  labs(title = paste0(\"Boxplot de \", variable)) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n3.1.4 Z-Score\nUn outlier es un valor amb |z| &gt; 3 deviació estándar\n\nvariable &lt;- \"Age\"\nvalorEscalado &lt;- scale(dades[, variable])\nhist(valorEscalado)\n\n\n\n\n\n\n\nggplot(data.frame(valor = valorEscalado), aes(x = valor)) +\n  geom_histogram(binwidth = 0.5, fill = \"skyblue\", color = \"black\") +  # Histograma\n  geom_vline(xintercept = c(3, -3), linetype = \"dashed\", color = \"red\", size = 1) + # Líneas horizontales\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n3.1.5 Hampel Identifier\nUtilitza la mediana i la desviació absoluta mediana (MAD) en lloc de la mitjana\n\nvariable &lt;- \"Age\"\n\nlower_bound &lt;- median(dades[, variable]) - 3 * mad(dades[, variable], constant = 1)\nupper_bound &lt;- median(dades[, variable]) + 3 * mad(dades[, variable], constant = 1)\noutlier_ind &lt;- which((dades[, variable] &lt; lower_bound) | (dades[, variable] &gt; upper_bound))\noutlier_ind\n\ninteger(0)\n\n\n\n\n3.1.6 Tests Estadístics\n\n3.1.6.1 Grubbs’ Test\nDetecta valors extrems en una distribució normal\n\nlibrary(outliers)\n\nvariable &lt;- \"Age\"\ntest &lt;- outliers::grubbs.test(dades[, variable], opposite = TRUE)\n# amb el paràmetre opposite controles quina de les dues cues están buscant\ntest\n\n\n    Grubbs test for one outlier\n\ndata:  dades[, variable]\nG = 1.65591, U = 0.99986, p-value = 1\nalternative hypothesis: highest value 40 is an outlier\n\n\n\n\n3.1.6.2 Dixon’s Test\nNomés utilitzar per a bbdd petites (entre 3 - 30) observacions\n\nvariable &lt;- \"Age\"\ntest &lt;- outliers::dixon.test(dades[, variable], opposite = FALSE)\ntest\n\n\n\n3.1.6.3 Rosner’s Test\nLa prueba de Rosner para valores atípicos tiene las ventajas de que: 1. se utiliza para detectar varios valores atípicos a la vez (a diferencia de la prueba de Grubbs y Dixon, que debe realizarse de forma iterativa para detectar múltiples valores atípicos), 2. Está diseñado para evitar el problema del enmascaramiento, donde un valor atípico cercano en valor a otro valor atípico puede pasar desapercibido.\nA diferencia de la prueba de Dixon, tenga en cuenta que la prueba de Rosner es más apropiada cuando el tamaño de la muestra es grande (n ≥ 20).\nEsta función requiere al menos dos argumentos: - los datos - la cantidad de valores atípicos sospechosos k (k = 3 como cantidad predeterminada)\nAsumeix normalitat de les dades\n\nlibrary(EnvStats)\n\nvariable &lt;- \"Age\"\ntest &lt;- EnvStats::rosnerTest(dades[, variable], k = 1)\ntest\n\n$distribution\n[1] \"Normal\"\n\n$statistic\n     R.1 \n1.666119 \n\n$sample.size\n[1] 20000\n\n$parameters\nk \n1 \n\n$alpha\n[1] 0.05\n\n$crit.value\nlambda.1 \n4.706884 \n\n$n.outliers\n[1] 0\n\n$alternative\n[1] \"Up to 1 observations are not\\n                                 from the same Distribution.\"\n\n$method\n[1] \"Rosner's Test for Outliers\"\n\n$data\n    [1] 26 23 34 37 38 40 28 32 30 28 23 25 26 23 30 28 22 20 38 27 27 27 33 33\n   [25] 33 38 26 33 22 27 23 37 35 34 19 23 40 19 25 25 33 24 24 21 22 36 37 38\n   [49] 22 29 28 30 20 25 38 29 28 33 25 31 26 38 19 30 24 21 19 21 26 25 28 34\n   [73] 18 33 22 19 34 28 37 39 26 30 26 34 38 32 29 21 31 19 21 27 19 27 26 38\n   [97] 29 37 35 26 18 32 40 27 29 39 40 23 26 40 26 31 23 30 29 34 31 26 21 22\n  [121] 37 28 20 21 35 18 22 36 40 24 18 33 25 34 34 33 25 24 35 20 19 20 20 19\n  [145] 20 39 21 31 30 28 30 18 20 32 27 29 30 18 21 35 32 19 21 33 39 19 23 18\n  [169] 31 18 37 35 32 18 26 20 24 39 20 27 38 34 39 25 25 22 26 37 40 27 34 38\n  [193] 39 34 22 34 40 30 27 39 38 40 18 24 29 27 33 25 35 37 29 20 33 33 40 38\n  [217] 20 20 32 36 20 32 22 36 32 35 34 30 20 39 24 23 22 29 20 35 21 40 24 21\n  [241] 40 20 26 31 32 32 32 18 29 33 30 34 21 32 20 26 21 19 32 36 19 21 40 22\n  [265] 19 18 36 31 28 23 23 36 29 33 40 38 31 32 39 18 33 31 30 34 28 25 36 21\n  [289] 20 21 31 30 31 30 25 20 31 33 33 21 20 27 34 18 32 22 27 32 40 22 26 37\n  [313] 38 18 38 29 40 31 26 28 33 28 19 29 25 31 30 24 24 20 23 20 40 36 37 27\n  [337] 32 18 35 33 25 34 28 21 32 35 24 23 39 27 27 34 24 39 27 31 36 20 39 24\n  [361] 25 35 29 32 40 33 30 24 29 24 20 27 18 30 19 20 33 20 28 39 34 40 21 36\n  [385] 22 32 21 38 30 36 38 25 25 25 27 33 18 21 26 20 40 31 39 22 18 21 25 20\n  [409] 39 20 31 37 37 24 26 36 38 33 24 23 24 32 24 24 29 23 20 20 28 40 19 20\n  [433] 33 38 38 34 22 25 21 27 36 19 19 37 30 33 26 20 40 18 35 23 23 34 20 20\n  [457] 29 22 37 18 22 31 25 30 34 23 38 20 18 40 35 38 33 21 32 37 34 38 36 22\n  [481] 26 18 37 35 34 29 32 33 39 35 34 35 28 34 35 39 37 40 33 40 25 36 34 29\n  [505] 39 23 35 23 38 20 26 34 28 30 38 32 39 30 18 28 40 37 33 24 37 39 23 24\n  [529] 31 24 19 27 32 29 38 39 18 22 29 28 32 35 23 40 19 28 26 19 37 40 39 24\n  [553] 23 38 24 34 24 18 40 31 27 34 25 24 40 34 27 37 39 26 26 34 38 27 38 32\n  [577] 27 27 32 36 33 26 37 21 36 23 30 35 25 32 37 27 33 30 39 21 34 25 37 36\n  [601] 20 21 31 26 21 33 25 19 23 36 27 36 31 23 27 20 21 39 35 33 20 35 19 36\n  [625] 32 33 22 36 38 39 38 24 19 37 20 29 36 35 36 37 27 35 31 40 39 30 26 23\n  [649] 34 23 25 30 38 27 23 35 33 25 23 30 39 22 28 20 31 23 18 39 29 25 20 31\n  [673] 22 26 29 26 31 26 36 38 35 20 38 20 25 28 26 28 18 22 19 24 28 35 24 32\n  [697] 18 23 34 30 19 39 18 22 27 32 28 20 26 35 36 38 19 40 36 31 38 18 24 19\n  [721] 23 37 36 38 34 34 19 24 22 25 20 23 32 38 40 36 36 22 20 33 23 25 29 27\n  [745] 40 20 30 25 32 24 28 18 20 34 28 20 40 30 25 34 31 30 31 32 20 30 33 38\n  [769] 22 30 23 24 38 38 27 25 36 37 23 37 27 29 26 27 18 29 18 26 31 26 21 32\n  [793] 30 37 34 28 40 29 31 32 22 26 19 25 28 37 29 33 24 18 33 19 29 18 23 28\n  [817] 37 34 20 31 39 33 30 32 35 29 23 39 37 30 19 36 26 18 33 35 30 29 32 30\n  [841] 20 31 32 33 23 20 33 30 39 31 30 38 31 23 39 38 20 40 40 36 18 35 38 32\n  [865] 23 39 23 30 40 20 22 22 21 39 20 25 26 18 40 33 34 26 35 39 36 26 21 38\n  [889] 21 31 25 32 37 20 35 34 37 31 28 31 27 19 18 33 28 26 35 26 24 19 24 33\n  [913] 35 35 25 25 34 31 24 24 23 24 21 38 38 39 32 31 26 33 18 22 38 24 30 20\n  [937] 27 35 25 22 22 23 40 27 38 39 18 21 35 33 39 32 31 31 32 22 28 19 32 20\n  [961] 27 28 28 31 28 27 38 19 34 23 23 33 23 31 38 18 37 40 28 21 36 21 39 38\n  [985] 37 21 26 29 30 27 26 39 18 25 28 32 30 29 25 19 22 24 19 35 30 23 37 22\n [1009] 20 32 38 29 30 38 25 21 20 36 30 39 28 32 23 30 21 18 22 35 37 27 29 19\n [1033] 26 29 26 18 34 34 22 35 20 40 40 34 29 31 29 20 18 23 18 24 34 36 38 40\n [1057] 22 40 22 37 33 18 40 36 36 40 38 21 32 27 35 34 29 26 27 22 28 35 37 18\n [1081] 21 26 37 25 40 19 33 39 33 28 30 34 34 37 33 36 24 21 26 34 24 27 20 38\n [1105] 22 29 37 23 24 30 37 39 24 40 30 34 27 23 19 26 28 39 27 27 24 37 35 34\n [1129] 36 40 29 19 39 21 29 23 28 38 21 19 20 19 34 40 22 35 23 31 22 26 25 22\n [1153] 35 38 31 26 38 23 28 36 26 30 24 36 34 26 24 30 38 31 31 36 33 25 37 25\n [1177] 19 28 26 37 36 26 18 28 30 40 21 26 34 29 35 35 28 40 22 36 34 31 26 24\n [1201] 34 33 31 38 38 25 38 20 30 26 34 18 26 22 18 22 35 29 29 32 31 29 34 31\n [1225] 32 21 32 18 37 18 20 25 39 32 23 23 30 29 28 20 18 32 24 21 36 23 28 38\n [1249] 20 37 22 33 27 31 36 25 39 24 20 33 40 22 23 21 37 37 26 35 40 32 35 39\n [1273] 37 20 35 38 40 18 32 39 21 27 36 29 22 21 26 25 33 23 35 23 24 22 24 32\n [1297] 31 21 26 22 38 35 28 38 38 39 28 30 37 37 29 24 33 19 20 30 33 35 32 34\n [1321] 29 31 25 19 19 21 26 33 32 32 18 18 33 40 33 28 25 27 20 26 28 32 34 34\n [1345] 21 33 23 21 25 31 22 23 37 25 37 26 19 34 28 27 28 32 35 32 30 19 26 31\n [1369] 22 40 39 30 37 26 22 36 28 23 39 30 37 21 31 32 28 28 31 37 27 29 21 22\n [1393] 35 34 20 18 21 32 24 37 22 39 30 22 30 20 25 39 38 31 36 36 34 35 37 21\n [1417] 32 25 20 23 39 37 38 32 18 32 33 20 35 38 40 18 31 25 22 31 32 31 26 38\n [1441] 20 36 33 40 40 37 40 31 32 24 19 36 32 37 26 32 31 22 20 40 22 35 28 29\n [1465] 23 27 24 30 39 39 20 23 24 40 20 28 24 31 33 31 21 24 40 18 38 21 33 40\n [1489] 30 35 33 18 32 39 24 21 33 34 27 29 22 29 37 22 19 30 39 35 22 32 35 33\n [1513] 37 40 19 31 23 20 30 36 35 39 40 18 23 32 23 36 27 35 24 27 38 24 31 36\n [1537] 32 25 33 21 38 29 21 40 21 30 31 35 27 33 27 35 36 22 38 35 22 34 28 31\n [1561] 27 33 22 29 24 21 25 21 35 20 21 35 20 25 34 31 21 38 32 18 36 35 24 40\n [1585] 32 34 26 36 39 32 29 39 34 38 30 18 33 39 23 19 21 24 33 27 32 22 22 39\n [1609] 35 18 21 29 33 20 37 23 26 37 39 24 31 25 31 26 20 24 30 39 32 21 37 40\n [1633] 39 28 20 21 35 26 38 37 34 22 40 37 18 19 29 36 34 24 40 40 32 30 35 28\n [1657] 25 32 23 35 22 36 20 39 40 19 35 28 25 31 26 25 22 22 30 32 36 27 21 29\n [1681] 37 40 37 30 30 29 30 36 24 24 18 38 22 19 30 19 37 34 31 40 36 32 25 23\n [1705] 40 36 26 20 40 21 34 26 35 37 31 23 34 32 29 34 30 24 38 32 35 28 32 36\n [1729] 18 40 31 32 39 35 21 18 25 38 26 39 39 39 24 29 27 30 32 19 35 33 20 38\n [1753] 27 29 35 38 19 33 18 30 39 40 24 38 27 20 23 27 28 34 18 36 37 22 36 35\n [1777] 38 24 32 34 29 26 37 25 22 25 36 18 20 26 38 19 22 37 18 36 28 29 25 40\n [1801] 36 31 23 30 29 28 29 20 32 22 24 39 26 28 26 19 37 39 22 34 32 24 25 29\n [1825] 22 34 25 23 26 39 32 20 39 30 24 25 32 29 38 34 23 27 34 21 20 37 20 38\n [1849] 38 32 40 35 19 25 23 23 33 37 36 35 33 30 23 31 19 29 36 38 23 32 19 28\n [1873] 19 34 19 24 37 40 35 35 35 34 32 29 29 40 35 20 37 34 22 20 20 27 23 21\n [1897] 36 32 21 27 30 32 18 32 37 23 34 24 38 19 25 21 34 32 36 40 35 34 36 26\n [1921] 36 26 32 24 29 29 21 35 37 19 30 25 34 22 20 31 39 30 21 38 20 35 36 22\n [1945] 38 35 30 23 29 26 33 39 40 40 20 27 18 29 33 33 25 38 30 33 18 27 21 34\n [1969] 30 38 34 23 29 21 37 32 20 21 20 27 22 29 33 40 30 32 39 19 27 29 39 37\n [1993] 40 37 22 25 18 35 36 19 18 38 20 33 20 25 38 39 18 27 21 26 32 37 33 38\n [2017] 37 36 40 34 34 39 34 23 37 29 22 33 38 35 31 39 35 22 24 40 37 34 35 24\n [2041] 19 36 37 35 37 34 38 23 24 24 34 27 26 22 30 21 28 26 24 31 18 18 37 30\n [2065] 34 33 19 36 31 25 28 39 25 40 22 18 36 21 29 23 19 22 36 23 30 28 40 18\n [2089] 36 23 25 33 21 30 31 22 26 28 27 22 19 22 31 40 40 39 21 37 27 36 36 39\n [2113] 27 28 35 24 24 21 18 23 27 38 26 28 22 23 32 30 39 23 20 29 40 28 23 31\n [2137] 22 29 28 18 39 32 40 30 40 40 23 33 30 39 28 29 26 40 19 40 21 31 28 29\n [2161] 37 24 28 40 20 28 22 28 30 26 39 39 24 33 33 30 23 34 19 37 24 20 28 25\n [2185] 39 30 18 29 39 36 31 18 24 29 39 36 33 35 19 39 40 31 24 21 39 36 19 33\n [2209] 28 32 32 39 36 40 35 40 25 26 40 20 18 35 29 40 18 18 38 29 29 25 27 19\n [2233] 31 22 22 40 38 23 30 24 24 39 33 40 26 30 21 33 21 18 27 28 33 20 33 19\n [2257] 35 25 38 24 26 24 26 29 20 27 39 26 31 33 31 36 34 40 40 28 38 29 32 34\n [2281] 18 27 20 23 22 19 28 19 38 35 18 33 20 24 18 33 27 39 30 30 25 20 25 22\n [2305] 32 39 40 30 19 28 36 19 39 31 25 36 24 28 32 23 29 30 35 18 21 27 35 40\n [2329] 36 29 19 28 26 26 36 27 36 26 26 29 37 36 22 22 18 24 33 23 38 40 18 30\n [2353] 22 26 33 20 28 39 25 22 40 21 40 38 30 21 35 21 19 25 22 27 38 24 28 28\n [2377] 36 22 26 22 18 19 21 39 20 27 30 21 25 18 23 34 20 34 34 21 23 23 27 31\n [2401] 31 23 24 36 29 26 28 24 35 29 38 24 35 22 20 19 20 24 32 26 32 23 28 40\n [2425] 28 36 30 26 23 40 27 26 30 23 20 39 23 40 31 36 31 19 36 33 29 32 20 22\n [2449] 31 27 35 25 35 26 30 26 35 37 22 24 30 36 39 39 29 38 27 31 31 24 28 20\n [2473] 34 19 32 24 18 26 35 40 34 36 24 34 21 22 39 35 25 32 25 38 21 25 24 22\n [2497] 31 27 22 18 33 20 27 33 38 30 35 24 26 21 25 38 21 38 39 39 28 23 24 35\n [2521] 29 33 34 40 40 27 23 39 23 37 29 33 23 30 35 38 21 31 23 40 39 18 34 25\n [2545] 37 18 24 26 39 35 25 37 39 32 27 23 22 31 32 23 32 29 31 31 33 35 31 37\n [2569] 20 36 31 19 25 37 26 38 40 20 35 35 20 18 40 36 40 28 35 18 39 32 20 35\n [2593] 21 32 35 35 25 33 23 34 31 31 22 38 24 31 38 27 24 33 18 29 29 36 19 21\n [2617] 37 38 21 26 23 36 36 31 19 18 37 26 36 27 40 35 29 23 26 34 18 19 19 18\n [2641] 38 35 25 25 34 39 22 22 28 27 18 27 33 40 37 26 21 38 23 38 30 36 33 21\n [2665] 32 33 28 38 18 29 36 26 39 34 40 29 33 24 37 22 24 25 22 27 39 40 24 35\n [2689] 37 21 32 20 27 39 39 35 35 33 33 29 30 21 23 26 35 30 26 38 32 38 26 36\n [2713] 37 28 35 19 31 33 20 19 31 25 28 30 34 25 27 37 30 31 40 29 32 35 32 32\n [2737] 22 19 39 36 19 28 26 19 18 31 38 27 24 38 32 25 26 32 36 29 27 22 32 21\n [2761] 38 18 24 21 39 28 25 38 18 20 37 21 34 24 36 21 23 19 26 35 32 27 22 33\n [2785] 34 27 18 26 24 26 22 24 30 23 30 32 26 32 39 32 34 38 26 21 18 25 36 20\n [2809] 18 40 35 18 23 31 21 24 36 28 34 31 24 26 40 30 19 20 32 35 36 27 36 27\n [2833] 30 30 32 23 23 24 24 18 31 39 34 31 30 22 27 36 32 24 36 28 19 35 23 37\n [2857] 37 31 19 40 20 35 20 30 21 29 27 35 18 27 24 21 21 28 30 19 34 38 35 32\n [2881] 37 27 37 32 23 34 29 20 39 29 19 31 33 32 29 38 38 22 35 26 29 37 34 28\n [2905] 40 20 40 19 34 26 20 40 31 36 40 30 24 25 24 26 20 20 34 33 19 36 29 29\n [2929] 23 38 21 34 29 35 18 39 22 40 30 23 38 36 34 33 32 25 24 27 35 39 23 33\n [2953] 32 23 34 34 38 34 20 33 38 30 40 22 38 35 30 40 27 20 29 27 22 30 40 35\n [2977] 22 37 33 28 26 23 24 21 33 38 36 28 26 18 37 23 18 31 20 36 36 30 39 26\n [3001] 24 21 24 36 25 29 23 23 37 27 33 21 35 19 38 36 23 19 38 31 30 30 25 39\n [3025] 32 30 36 38 23 19 34 18 29 21 33 24 34 21 40 35 38 29 35 25 27 29 36 39\n [3049] 38 31 34 31 30 21 39 24 24 28 35 28 23 26 39 38 35 23 29 29 23 28 27 18\n [3073] 36 37 31 31 35 21 27 39 18 23 38 25 34 24 29 31 34 27 36 21 31 27 21 18\n [3097] 37 23 30 32 27 35 39 39 28 34 30 22 39 37 30 39 32 24 35 24 27 30 34 28\n [3121] 19 33 37 27 23 31 34 18 18 33 20 27 29 19 20 31 31 21 32 40 25 32 20 22\n [3145] 20 31 22 24 40 28 18 26 35 27 22 24 21 19 23 23 36 37 30 29 40 36 30 19\n [3169] 22 29 22 18 33 38 31 38 20 33 26 31 30 38 37 18 19 27 30 26 39 29 20 27\n [3193] 39 23 26 28 23 22 29 36 25 37 39 30 29 40 18 19 35 21 36 33 32 21 40 19\n [3217] 21 36 40 25 27 20 35 28 22 28 29 30 40 24 28 40 20 31 33 21 21 33 31 23\n [3241] 36 39 35 23 35 33 32 20 25 20 26 38 22 18 20 28 34 37 27 37 39 39 24 19\n [3265] 27 22 27 39 33 18 32 18 38 21 22 27 26 32 33 28 37 32 23 28 23 38 32 18\n [3289] 22 25 40 39 34 31 22 30 26 38 33 22 24 20 27 34 31 22 24 18 24 28 39 23\n [3313] 32 21 24 25 28 36 26 29 38 38 37 37 35 29 32 40 23 34 18 33 18 28 26 22\n [3337] 31 30 26 25 38 35 23 19 24 26 38 37 31 25 22 19 36 32 26 22 32 22 24 34\n [3361] 37 32 34 37 21 36 31 38 29 39 23 25 18 19 21 26 25 33 34 35 38 25 32 39\n [3385] 21 34 21 32 33 36 25 39 20 40 38 23 28 28 23 39 38 20 29 21 33 33 31 26\n [3409] 24 29 40 27 32 24 20 39 21 31 23 38 27 18 23 33 35 37 26 36 36 36 40 36\n [3433] 24 23 20 22 34 19 28 38 36 37 39 36 31 38 21 39 36 40 19 19 35 38 26 26\n [3457] 28 34 32 22 38 21 36 32 27 38 19 23 20 36 32 31 38 35 30 18 20 19 39 31\n [3481] 38 40 34 19 38 33 33 25 25 26 20 35 25 32 30 24 34 20 23 39 22 33 26 31\n [3505] 29 18 31 29 37 22 39 18 40 29 23 28 29 23 27 20 25 36 36 40 25 23 21 28\n [3529] 20 21 27 39 20 27 34 26 20 35 29 25 25 19 22 36 28 18 26 39 24 18 29 35\n [3553] 24 20 33 35 37 27 28 18 25 19 28 35 21 39 33 19 26 25 29 32 29 25 19 34\n [3577] 29 19 22 22 27 29 39 36 29 33 36 39 33 30 23 35 34 38 20 23 24 36 18 31\n [3601] 32 24 39 20 18 22 25 30 32 19 19 26 26 21 19 32 31 18 32 30 19 32 40 38\n [3625] 40 35 18 38 25 40 31 28 20 25 28 22 19 38 37 19 25 27 39 40 37 24 40 37\n [3649] 24 27 20 23 30 22 26 22 32 19 24 27 35 23 37 34 20 39 20 40 35 25 25 18\n [3673] 19 18 23 20 38 36 29 27 21 37 21 26 36 22 40 32 38 21 37 23 32 32 27 25\n [3697] 19 22 30 28 30 28 24 23 34 28 24 29 38 33 39 18 22 20 35 24 25 38 29 34\n [3721] 18 27 32 19 29 29 20 35 26 35 34 28 37 24 35 28 18 37 27 28 21 23 29 26\n [3745] 39 34 31 34 24 27 25 22 18 24 29 18 37 27 37 35 32 30 27 25 18 30 35 29\n [3769] 20 33 19 38 24 28 24 36 33 40 25 37 27 34 20 23 34 36 21 26 37 24 18 37\n [3793] 24 40 37 39 38 39 32 38 24 24 24 39 22 26 18 18 19 28 22 23 28 29 34 23\n [3817] 26 22 28 29 31 26 21 38 27 36 26 31 29 27 29 32 24 18 18 38 33 39 34 18\n [3841] 26 33 28 38 22 19 24 32 28 26 36 30 22 31 21 33 37 22 18 22 40 20 37 38\n [3865] 31 19 27 22 32 30 24 18 22 36 22 35 32 30 29 38 37 28 35 18 24 28 30 28\n [3889] 29 23 36 22 33 27 38 33 24 36 32 25 19 23 28 34 32 23 32 23 22 31 20 19\n [3913] 39 35 35 30 22 40 28 26 23 27 33 21 31 37 29 30 28 27 31 32 36 27 39 18\n [3937] 35 33 20 27 24 35 40 38 35 27 20 18 37 29 19 35 21 31 25 22 37 20 38 37\n [3961] 38 30 20 34 40 35 27 33 33 29 25 39 34 18 39 23 18 31 40 32 21 29 39 19\n [3985] 31 23 23 27 20 38 23 34 19 31 23 25 20 33 39 34 40 27 32 26 36 38 31 20\n [4009] 18 20 22 32 26 35 35 24 24 27 28 18 21 36 37 23 19 18 30 39 28 30 22 36\n [4033] 18 23 36 22 36 33 31 26 23 30 34 34 37 18 39 36 18 27 23 25 23 22 37 18\n [4057] 38 28 37 35 39 27 26 24 22 20 23 37 18 30 19 19 28 18 29 21 36 33 32 39\n [4081] 34 25 20 33 24 39 27 18 24 38 23 30 27 21 23 35 25 18 35 35 32 38 38 35\n [4105] 39 36 39 29 18 23 37 32 34 24 25 35 18 18 25 36 27 40 35 20 19 19 34 19\n [4129] 20 19 40 34 40 39 29 24 39 33 20 34 40 27 25 25 25 19 31 22 39 31 26 39\n [4153] 25 28 20 22 29 32 28 27 26 24 25 19 37 37 23 38 24 32 25 29 31 38 37 32\n [4177] 21 36 32 22 39 36 39 22 34 23 36 37 29 18 35 25 21 34 33 34 24 37 33 26\n [4201] 35 21 29 38 25 37 27 32 18 25 23 18 21 24 26 31 20 28 37 22 40 19 29 39\n [4225] 21 27 31 28 33 29 26 39 22 33 27 32 19 39 37 25 18 35 24 23 20 34 21 30\n [4249] 29 38 21 25 33 29 31 25 33 22 34 28 20 36 27 35 25 19 36 28 20 26 23 21\n [4273] 36 32 34 40 29 34 31 32 36 38 37 36 21 35 18 22 22 21 30 34 32 31 38 24\n [4297] 27 34 28 27 26 28 26 27 35 38 34 36 18 20 31 26 30 30 21 23 40 40 39 27\n [4321] 27 28 32 39 32 26 39 19 24 20 22 18 38 26 29 26 40 26 21 27 20 40 27 39\n [4345] 28 29 38 38 23 25 30 33 34 36 31 35 30 30 31 39 29 36 37 36 18 27 36 18\n [4369] 33 38 31 36 32 30 21 21 38 22 37 37 28 34 18 23 22 32 18 30 30 30 33 26\n [4393] 38 28 30 37 27 31 37 22 25 39 31 23 33 21 21 22 32 35 26 21 34 25 36 23\n [4417] 31 36 37 25 35 35 29 37 34 37 23 22 21 24 20 32 21 33 24 29 23 24 35 23\n [4441] 34 29 25 35 27 20 24 23 35 20 32 30 28 20 32 23 19 26 25 33 20 39 38 27\n [4465] 29 28 21 39 35 21 36 30 18 37 36 28 21 39 37 31 23 27 39 34 36 25 22 33\n [4489] 29 23 21 29 30 37 29 23 26 19 38 23 24 35 29 27 36 19 27 24 31 39 23 24\n [4513] 19 29 30 18 37 23 20 32 31 39 22 27 23 35 36 25 26 38 37 19 25 30 38 33\n [4537] 31 39 23 37 31 18 22 27 35 26 33 26 28 23 24 32 31 24 37 27 27 21 36 24\n [4561] 19 30 23 37 26 32 39 37 24 26 38 22 22 23 37 19 18 29 19 29 19 35 22 28\n [4585] 25 30 33 26 21 32 23 20 39 24 22 33 30 26 20 38 33 40 18 30 34 28 39 40\n [4609] 28 35 23 37 40 22 25 21 34 38 28 38 40 21 24 33 20 22 20 37 25 37 18 29\n [4633] 26 25 18 26 38 30 33 31 35 29 35 21 25 35 19 25 22 30 19 25 36 37 22 29\n [4657] 32 28 21 38 27 40 29 19 24 38 39 33 33 37 29 25 25 36 33 30 39 23 33 38\n [4681] 18 34 34 33 24 38 24 26 25 33 36 28 23 22 23 26 18 24 27 24 30 29 24 24\n [4705] 31 36 31 37 28 27 26 19 29 40 19 37 21 34 29 19 35 31 30 18 30 40 26 31\n [4729] 24 31 37 32 35 30 27 31 39 31 35 33 29 19 35 37 29 33 21 29 40 38 20 24\n [4753] 33 30 31 29 18 36 40 28 25 37 23 19 25 39 22 24 25 35 24 36 28 27 26 30\n [4777] 22 24 37 26 24 28 36 20 38 29 26 32 34 33 27 33 25 34 27 22 38 22 28 28\n [4801] 23 31 36 22 40 19 39 31 26 21 39 39 29 18 19 38 39 37 39 23 31 25 22 38\n [4825] 29 18 32 20 23 24 26 35 27 36 31 31 37 35 31 37 35 30 31 20 21 22 29 23\n [4849] 19 32 26 20 18 23 33 30 32 28 39 35 29 35 36 32 38 34 23 29 30 25 18 32\n [4873] 40 33 31 27 31 26 23 19 37 22 23 22 39 38 18 32 34 25 23 23 32 30 29 38\n [4897] 21 31 34 18 34 32 27 20 35 20 25 39 30 23 37 23 29 26 21 36 22 25 37 18\n [4921] 23 35 21 21 36 27 20 29 22 25 38 38 25 30 23 40 39 29 22 19 32 25 25 39\n [4945] 26 30 37 40 31 34 28 23 35 19 31 39 23 18 20 26 37 23 27 40 25 23 29 40\n [4969] 23 30 29 26 30 18 31 26 32 34 26 39 36 24 18 39 39 19 35 24 22 38 30 37\n [4993] 39 40 19 26 32 33 21 30 24 40 21 32 30 39 20 33 28 32 25 25 26 34 29 27\n [5017] 36 30 21 29 38 36 29 34 26 34 40 31 34 22 27 28 25 35 19 30 21 21 21 37\n [5041] 38 18 37 27 29 31 31 36 32 29 35 26 34 21 18 40 38 31 34 33 40 36 37 20\n [5065] 40 22 35 26 38 23 18 23 19 40 24 22 23 32 40 40 28 25 36 23 35 29 39 21\n [5089] 32 31 28 33 31 24 28 31 27 32 20 27 18 29 33 23 33 19 27 29 29 24 19 30\n [5113] 22 28 40 30 26 36 19 32 29 34 32 25 30 39 28 40 33 28 32 25 37 37 36 33\n [5137] 22 29 31 25 29 33 21 20 31 31 40 36 34 29 25 19 36 30 36 22 35 21 26 37\n [5161] 32 32 36 34 29 23 29 23 37 27 40 37 18 27 18 19 26 35 31 34 20 29 28 38\n [5185] 26 30 21 35 28 31 40 19 40 38 31 33 37 37 28 34 33 33 18 35 24 32 35 35\n [5209] 27 28 24 32 18 38 23 19 40 37 20 38 19 40 25 38 39 37 33 22 35 23 35 34\n [5233] 22 31 20 31 20 38 35 26 21 22 33 23 23 38 24 40 23 22 25 38 23 21 40 26\n [5257] 34 33 30 33 31 23 26 39 28 32 36 29 22 28 38 22 29 21 23 30 32 34 27 19\n [5281] 19 20 39 29 36 35 27 37 21 35 21 22 26 20 39 26 27 34 37 40 35 21 29 39\n [5305] 20 24 37 24 29 18 26 35 40 35 22 30 40 25 39 31 32 23 30 25 26 23 21 24\n [5329] 24 35 25 25 30 24 29 29 23 31 25 22 23 25 23 21 19 37 26 35 40 29 21 20\n [5353] 30 23 32 24 33 21 32 18 19 28 25 31 27 38 26 27 37 22 36 27 26 38 36 36\n [5377] 31 27 20 26 19 40 22 22 29 29 36 40 30 32 20 19 26 28 37 22 40 28 27 25\n [5401] 40 30 22 32 31 39 24 37 26 31 34 39 24 35 34 21 26 33 27 40 21 20 18 33\n [5425] 26 22 35 31 29 35 35 32 30 18 31 40 38 24 26 19 33 33 40 26 25 19 26 24\n [5449] 24 21 31 27 39 23 18 18 38 20 18 38 38 34 35 33 33 32 35 35 34 30 29 27\n [5473] 35 19 21 23 31 35 26 20 36 25 33 34 25 33 28 25 38 19 22 33 29 22 22 21\n [5497] 39 22 25 39 25 36 28 20 29 21 27 33 27 24 32 33 38 18 24 35 34 35 30 25\n [5521] 23 30 30 26 33 33 40 24 29 19 40 22 24 31 34 40 25 18 29 36 20 24 40 18\n [5545] 26 23 26 30 21 24 36 32 32 40 39 26 28 30 31 28 20 25 29 33 23 20 32 39\n [5569] 32 30 33 39 28 38 20 29 39 22 39 29 21 29 30 25 32 28 19 30 30 27 24 26\n [5593] 33 34 29 25 18 21 39 36 21 19 28 30 20 21 25 34 25 24 18 19 22 24 24 26\n [5617] 37 28 24 32 37 18 25 33 30 32 32 37 38 21 39 35 35 18 30 36 39 34 34 21\n [5641] 20 25 38 30 31 25 25 25 32 26 25 19 27 32 27 19 25 37 37 32 31 36 36 39\n [5665] 30 28 30 32 34 31 35 28 27 30 32 26 40 38 25 33 33 37 38 20 38 20 28 33\n [5689] 19 29 20 26 30 22 19 25 20 31 33 32 28 26 21 19 39 27 23 35 25 18 24 28\n [5713] 34 40 34 40 20 32 30 32 36 32 18 40 23 29 22 26 32 37 32 28 40 23 35 38\n [5737] 37 32 22 19 34 35 32 18 18 37 40 29 37 21 19 21 22 29 36 18 28 36 24 32\n [5761] 35 36 39 31 28 21 36 22 25 32 29 29 24 27 33 30 34 23 33 34 27 39 36 35\n [5785] 36 19 18 38 36 34 31 29 40 31 21 36 29 30 40 31 36 29 33 34 18 27 22 26\n [5809] 24 31 35 39 20 30 33 33 39 18 39 18 28 22 20 34 31 39 25 31 35 39 37 22\n [5833] 19 30 20 33 38 40 21 21 22 36 30 34 26 22 21 33 40 24 27 23 37 19 19 24\n [5857] 38 37 37 30 23 30 21 38 32 22 23 23 28 40 21 20 29 23 31 33 19 36 28 34\n [5881] 34 31 40 31 29 39 23 28 24 19 35 39 40 32 34 31 31 18 24 39 37 38 22 24\n [5905] 22 32 32 34 32 23 20 24 18 24 21 25 31 24 26 31 19 24 34 39 20 24 25 31\n [5929] 35 23 37 33 27 21 36 33 25 33 26 38 25 23 32 23 23 33 37 18 33 25 34 32\n [5953] 33 36 30 30 38 30 20 38 35 31 35 37 29 25 20 38 30 39 21 21 25 28 34 28\n [5977] 23 31 37 24 27 28 38 22 26 20 28 20 32 19 37 40 36 21 23 20 38 29 32 19\n [6001] 32 34 35 30 25 20 38 28 20 32 40 33 34 37 19 32 40 30 33 21 38 38 38 27\n [6025] 28 26 37 26 30 33 28 40 40 38 19 23 28 29 20 34 30 34 33 23 22 40 28 25\n [6049] 27 39 25 20 27 40 28 40 35 24 32 38 24 19 26 20 36 28 31 32 33 23 31 29\n [6073] 25 19 21 34 24 39 35 38 31 24 19 38 33 30 31 40 37 20 39 38 24 30 36 33\n [6097] 21 38 29 33 20 23 35 34 29 40 35 20 37 23 35 38 21 34 28 27 38 25 29 32\n [6121] 24 30 29 22 27 40 20 29 24 32 34 36 23 29 19 20 32 33 23 35 40 37 24 34\n [6145] 25 34 37 20 28 31 19 19 34 18 29 25 21 33 37 34 23 40 24 19 32 26 39 19\n [6169] 32 39 30 36 37 26 20 26 21 23 36 39 20 25 33 32 34 25 33 31 20 35 21 30\n [6193] 28 18 27 37 37 24 25 28 27 23 24 38 24 36 35 30 40 22 19 19 21 38 18 29\n [6217] 23 30 26 33 18 28 33 36 18 35 38 21 36 24 20 24 32 35 20 29 36 32 22 25\n [6241] 40 22 19 20 35 18 30 25 22 40 24 21 24 27 24 26 27 30 24 32 40 36 27 20\n [6265] 33 22 21 23 25 29 26 36 24 21 36 40 21 32 32 34 27 30 35 35 19 34 33 33\n [6289] 31 23 21 29 33 28 34 20 18 33 30 38 22 30 39 24 23 25 36 18 19 33 28 21\n [6313] 39 31 30 27 21 31 37 37 39 34 39 31 37 40 37 20 18 27 26 32 36 22 28 24\n [6337] 40 38 22 20 33 38 36 18 40 25 28 19 32 29 27 18 25 27 22 21 35 38 21 29\n [6361] 35 33 33 30 39 22 33 31 21 40 18 31 18 30 30 23 33 19 38 36 30 22 25 29\n [6385] 22 26 28 30 34 28 25 18 18 26 25 37 29 21 27 35 36 36 20 39 39 28 32 23\n [6409] 19 24 29 35 25 25 24 28 38 35 34 22 40 21 23 18 36 20 34 20 19 18 35 24\n [6433] 34 35 28 34 29 23 36 30 25 32 25 30 37 27 37 24 20 25 34 20 34 37 18 22\n [6457] 23 32 36 39 25 34 34 34 27 19 25 35 40 37 21 27 34 20 37 23 24 25 18 18\n [6481] 31 33 23 28 20 25 40 38 25 30 40 40 28 35 23 39 18 38 18 18 26 38 32 35\n [6505] 35 39 29 32 39 22 28 24 32 20 19 37 23 31 19 19 36 25 26 38 19 27 29 30\n [6529] 20 34 30 25 33 26 31 18 31 40 29 26 33 28 24 28 22 27 27 33 39 34 35 18\n [6553] 26 38 38 20 37 39 32 23 35 20 39 34 20 22 40 28 38 19 26 36 37 21 23 36\n [6577] 20 27 39 22 33 23 19 19 35 23 22 38 35 28 24 36 25 26 34 18 25 37 19 22\n [6601] 18 21 34 23 38 22 34 36 19 23 32 38 31 40 35 19 19 27 20 22 28 21 35 27\n [6625] 32 32 20 40 31 39 27 33 29 23 30 22 36 36 33 27 34 18 35 18 26 21 28 24\n [6649] 28 33 38 26 24 30 29 33 24 40 29 32 31 36 32 35 18 34 34 30 18 22 38 30\n [6673] 32 21 27 19 34 34 29 36 23 24 29 18 21 27 38 38 22 26 40 28 18 21 20 24\n [6697] 29 39 39 19 37 20 25 39 26 33 30 34 31 27 34 34 34 19 37 18 19 23 39 27\n [6721] 38 19 37 33 21 20 21 25 27 23 22 36 29 24 25 27 31 37 26 37 24 28 30 37\n [6745] 23 27 22 35 26 28 33 40 38 24 28 28 40 31 25 20 19 30 21 24 29 25 19 25\n [6769] 20 22 25 23 23 31 36 35 27 37 31 18 27 40 22 29 39 18 33 27 33 29 30 19\n [6793] 25 39 32 36 33 39 32 28 40 20 37 21 28 32 35 23 34 19 29 26 38 38 36 36\n [6817] 20 35 22 39 23 20 22 31 24 19 18 31 18 32 32 35 28 40 27 30 40 29 35 32\n [6841] 28 38 35 38 33 38 23 18 32 23 31 39 33 23 20 34 32 32 31 36 35 21 35 26\n [6865] 33 22 37 34 26 35 36 31 22 19 36 39 38 35 38 33 28 36 18 40 18 34 36 20\n [6889] 25 38 26 25 18 29 36 22 34 23 25 24 19 40 36 39 32 36 24 27 38 27 27 21\n [6913] 30 38 18 22 30 33 38 26 21 31 25 31 40 36 40 19 39 31 20 30 28 35 37 40\n [6937] 23 37 31 28 39 34 20 29 23 22 35 27 40 33 31 24 32 36 38 20 25 35 32 39\n [6961] 37 18 35 36 34 28 21 37 18 32 39 25 22 28 32 36 25 25 40 18 27 32 22 31\n [6985] 38 36 26 23 27 35 33 21 24 30 34 38 23 39 29 40 40 33 39 38 33 19 34 33\n [7009] 30 37 28 40 39 26 20 23 40 21 31 30 21 22 32 22 31 29 30 37 31 39 21 35\n [7033] 20 23 18 21 28 34 40 31 18 32 28 27 25 32 26 34 36 33 22 39 18 18 25 28\n [7057] 33 28 25 37 31 22 25 31 34 35 24 22 26 30 24 19 23 20 31 24 18 19 29 18\n [7081] 37 25 35 23 38 23 35 39 34 18 19 34 31 26 34 40 29 19 20 31 18 32 33 36\n [7105] 30 26 29 36 40 35 34 37 36 23 22 30 18 27 31 29 34 20 25 18 35 21 30 39\n [7129] 20 21 20 31 18 35 25 38 32 24 30 24 31 35 18 25 19 32 20 31 29 29 35 27\n [7153] 21 33 39 19 18 21 29 19 29 19 21 21 18 26 21 40 21 35 32 32 31 21 34 30\n [7177] 19 29 21 23 18 29 23 36 28 37 24 18 27 25 37 33 34 25 36 20 26 32 38 38\n [7201] 37 32 20 28 21 40 26 32 23 38 30 27 38 32 38 27 35 35 28 25 28 28 20 30\n [7225] 26 30 27 22 37 31 35 38 30 19 25 29 25 30 21 24 27 18 20 32 30 31 28 24\n [7249] 37 38 33 22 26 20 34 39 32 27 28 32 29 23 34 34 32 22 34 27 30 39 26 26\n [7273] 29 31 25 31 28 37 33 24 35 24 32 20 18 29 40 19 23 37 20 32 24 40 27 21\n [7297] 33 33 29 23 21 23 22 32 23 29 32 24 39 30 28 24 34 33 27 24 22 24 31 38\n [7321] 27 32 30 25 32 35 27 25 20 36 19 31 23 35 29 36 28 23 40 25 39 29 32 31\n [7345] 31 33 34 35 26 38 37 40 40 35 31 20 33 19 39 23 19 26 31 29 19 33 38 35\n [7369] 31 23 28 35 28 19 36 32 27 21 25 23 23 40 20 33 28 28 37 33 21 18 40 33\n [7393] 36 34 35 20 31 32 39 24 20 36 34 24 29 23 33 19 23 33 37 18 40 32 23 40\n [7417] 25 25 31 34 27 20 18 30 22 21 30 26 29 25 38 21 38 28 30 18 30 23 29 28\n [7441] 20 38 25 25 24 31 31 38 25 40 27 37 34 40 28 30 21 25 35 25 25 33 21 31\n [7465] 24 27 34 18 32 38 20 26 24 19 37 33 25 33 21 22 34 33 36 28 25 35 23 29\n [7489] 23 22 21 33 26 33 40 18 21 21 33 40 27 33 37 38 32 25 18 36 20 27 40 35\n [7513] 38 36 27 21 22 40 39 28 28 29 26 22 25 24 40 25 22 32 27 19 32 27 20 40\n [7537] 21 20 37 40 29 39 30 32 23 22 19 23 35 34 32 26 26 30 28 20 35 37 28 18\n [7561] 36 23 38 27 30 19 34 28 24 18 40 34 37 19 32 38 37 20 19 36 21 22 32 28\n [7585] 35 18 32 29 30 19 39 19 34 19 18 29 23 18 27 32 24 23 20 29 18 28 18 27\n [7609] 35 33 37 19 26 37 29 36 25 30 23 38 23 27 19 26 21 29 29 37 25 28 36 22\n [7633] 38 27 38 32 21 40 25 33 23 37 40 35 20 40 33 28 36 27 27 25 27 21 37 30\n [7657] 27 38 25 36 24 21 38 36 32 23 22 32 27 32 24 19 27 32 26 24 21 39 23 20\n [7681] 39 22 28 24 32 20 35 34 32 18 27 20 33 32 34 27 40 25 19 27 40 34 23 22\n [7705] 20 24 21 26 29 38 35 37 30 40 32 35 40 38 33 40 33 33 34 20 19 39 32 32\n [7729] 21 38 22 23 34 20 18 30 29 39 40 40 33 24 37 29 34 19 20 22 27 32 20 22\n [7753] 18 29 32 25 23 30 36 34 19 26 34 19 24 39 24 37 21 21 23 25 25 27 37 28\n [7777] 18 31 21 23 23 23 30 28 21 28 33 22 27 33 23 25 39 38 30 31 29 30 25 39\n [7801] 21 19 32 19 26 30 29 25 35 30 34 31 36 34 24 37 29 32 24 36 27 20 37 28\n [7825] 27 23 26 33 36 18 23 25 36 28 40 34 21 18 20 23 39 39 33 33 25 27 21 18\n [7849] 20 40 40 32 33 19 27 32 26 38 37 21 18 33 38 30 21 28 37 35 21 21 35 34\n [7873] 26 19 32 25 19 30 38 40 18 29 38 30 40 23 28 30 32 34 18 37 26 39 37 25\n [7897] 23 27 32 38 36 34 25 19 18 31 31 23 33 35 33 26 21 31 33 36 19 22 39 18\n [7921] 31 30 18 26 32 19 29 36 22 20 19 40 33 33 19 35 28 26 29 34 31 33 26 22\n [7945] 32 31 26 27 35 37 35 35 19 35 38 29 34 21 26 19 31 35 24 23 23 39 37 29\n [7969] 27 22 24 28 31 28 22 37 23 30 35 35 22 33 30 30 28 20 30 20 26 20 40 19\n [7993] 18 27 32 19 18 34 18 40 20 23 35 36 26 34 34 27 36 35 36 24 39 21 38 30\n [8017] 29 27 33 18 31 32 32 33 32 23 20 19 36 27 26 27 40 21 30 33 27 31 32 21\n [8041] 39 23 21 30 23 21 36 21 25 25 36 23 39 21 29 33 18 37 34 32 19 37 33 40\n [8065] 21 39 21 26 26 33 24 30 37 24 20 19 25 37 33 20 26 21 25 23 23 22 31 32\n [8089] 36 40 32 30 21 23 18 36 33 22 25 25 28 22 20 28 19 20 29 20 38 22 23 32\n [8113] 39 33 28 39 23 25 24 22 25 36 35 24 20 37 26 33 35 21 25 34 30 25 29 25\n [8137] 38 40 19 40 37 26 22 30 28 25 34 24 19 20 30 30 27 25 32 18 38 37 24 22\n [8161] 34 23 26 19 25 21 18 38 31 18 36 31 30 24 38 19 22 22 34 32 28 19 24 34\n [8185] 20 26 28 24 34 37 26 30 25 20 26 40 31 18 35 25 31 40 23 35 29 22 38 40\n [8209] 31 35 30 24 22 36 32 40 18 39 36 19 31 31 35 29 19 32 32 26 22 22 36 38\n [8233] 34 28 34 19 29 32 39 36 18 26 21 33 38 19 24 28 40 39 26 29 25 23 31 23\n [8257] 25 39 20 28 39 38 29 33 25 26 18 27 21 31 37 18 20 40 28 24 23 39 25 21\n [8281] 27 38 18 31 22 36 33 38 30 19 36 37 29 25 32 40 33 28 19 23 27 29 40 21\n [8305] 38 23 37 40 40 27 20 36 34 39 25 37 38 34 25 33 22 20 31 25 38 18 24 31\n [8329] 23 22 34 39 34 39 25 39 32 20 27 39 40 39 26 39 28 24 23 19 36 26 31 23\n [8353] 24 39 25 39 33 20 27 32 31 30 23 21 26 26 23 33 27 29 37 30 39 31 26 25\n [8377] 21 19 34 38 28 35 30 23 37 25 34 35 25 27 39 20 19 32 34 20 24 34 27 19\n [8401] 25 40 24 31 31 36 25 28 35 25 36 24 39 37 28 39 36 18 29 24 37 30 36 31\n [8425] 19 39 35 37 38 28 21 35 36 35 31 36 31 37 37 20 34 27 32 22 19 30 37 40\n [8449] 40 39 21 34 19 32 32 33 36 24 21 29 25 24 30 24 27 35 40 22 18 34 33 23\n [8473] 40 36 22 36 40 21 23 36 22 25 21 23 30 37 40 30 32 23 32 22 36 31 39 35\n [8497] 28 29 25 36 33 26 28 35 33 35 34 32 30 20 24 30 34 35 26 37 39 20 19 21\n [8521] 27 31 33 30 32 29 25 20 30 23 36 22 35 37 26 27 24 27 36 21 28 32 32 23\n [8545] 23 19 34 23 33 30 29 23 18 36 38 40 22 25 40 36 26 29 25 27 40 40 29 22\n [8569] 18 31 32 26 27 31 37 28 28 24 39 34 39 32 30 37 24 30 37 18 30 28 32 34\n [8593] 29 40 34 28 21 19 30 26 22 39 19 18 31 19 33 28 40 39 22 36 21 22 20 35\n [8617] 38 21 21 33 18 23 26 36 18 34 32 22 35 30 36 31 39 33 27 34 33 34 19 35\n [8641] 22 34 20 38 38 31 29 19 37 20 28 25 38 39 28 22 32 20 33 25 37 37 40 22\n [8665] 35 36 20 33 37 24 39 29 19 23 27 36 29 26 24 33 29 38 40 28 26 34 20 24\n [8689] 30 21 20 25 22 18 36 20 36 26 18 22 26 28 34 38 37 30 37 39 31 36 32 38\n [8713] 20 34 23 37 33 38 26 38 38 27 33 20 26 30 21 23 18 30 22 37 27 21 39 31\n [8737] 20 22 36 18 38 19 29 27 36 25 37 27 32 27 29 35 19 28 24 19 28 26 18 33\n [8761] 36 30 40 34 24 30 33 40 22 39 33 28 21 26 22 31 20 25 28 25 34 20 39 31\n [8785] 28 35 34 23 23 19 29 36 38 27 33 39 32 20 24 36 38 19 20 25 36 37 27 37\n [8809] 39 30 38 20 30 40 36 23 36 39 21 31 30 40 35 34 19 30 39 21 26 35 18 20\n [8833] 20 26 29 29 20 33 38 21 19 30 21 29 20 18 24 26 29 33 21 37 34 19 23 24\n [8857] 18 32 36 21 40 39 22 39 18 39 26 25 21 34 35 36 21 37 18 23 25 27 19 25\n [8881] 25 33 33 39 40 18 34 40 36 19 18 27 33 22 21 33 21 39 20 34 31 40 29 26\n [8905] 38 21 20 39 25 30 18 33 27 28 39 20 22 35 19 28 38 38 28 36 23 28 39 36\n [8929] 34 30 38 32 29 27 23 33 23 18 37 37 18 30 33 38 40 36 40 36 38 27 22 34\n [8953] 29 31 29 32 35 24 25 40 34 22 36 32 28 40 23 23 21 22 40 30 34 35 23 30\n [8977] 25 26 31 32 36 31 38 27 38 30 31 36 40 36 29 19 19 31 29 34 34 34 25 29\n [9001] 22 32 33 20 29 35 30 35 32 30 37 39 36 38 38 25 32 34 30 29 35 29 38 25\n [9025] 37 18 38 32 36 28 37 31 30 36 21 35 28 23 34 39 23 35 18 22 27 31 20 35\n [9049] 27 30 24 38 29 34 39 21 23 32 32 34 37 27 23 33 27 39 29 36 39 29 37 38\n [9073] 38 22 38 19 31 34 20 21 28 26 28 29 19 20 26 35 34 34 33 27 26 31 35 40\n [9097] 18 18 35 25 38 29 19 34 40 38 24 20 35 27 32 33 23 40 26 30 19 37 27 22\n [9121] 33 30 32 26 25 22 30 30 36 39 27 24 34 25 35 26 38 29 37 29 22 31 19 38\n [9145] 33 27 21 29 31 29 28 37 23 29 30 25 18 32 19 26 19 38 31 20 34 21 28 34\n [9169] 33 25 25 35 19 24 39 40 35 20 37 20 31 40 30 30 32 23 20 29 36 18 31 33\n [9193] 37 26 28 25 27 37 34 29 27 37 36 18 38 28 36 26 35 37 24 26 31 31 37 40\n [9217] 37 38 36 33 32 32 27 23 26 32 27 24 27 37 39 26 30 38 23 18 23 18 39 27\n [9241] 24 19 20 23 19 21 32 40 29 18 28 37 37 28 40 26 29 39 21 35 25 35 35 27\n [9265] 37 39 33 27 39 24 23 40 29 27 40 18 20 19 37 25 18 35 21 21 32 23 34 40\n [9289] 21 34 27 29 36 25 26 38 40 36 26 34 30 35 29 20 22 39 23 29 29 23 30 20\n [9313] 40 34 35 27 27 21 38 31 22 23 40 33 35 36 19 33 30 39 26 34 20 21 23 34\n [9337] 28 25 22 34 39 38 24 34 22 36 38 18 38 34 40 24 30 34 21 39 33 23 30 35\n [9361] 21 27 38 29 19 31 39 35 31 34 35 26 21 36 31 26 32 27 24 29 28 38 33 24\n [9385] 39 33 33 30 24 33 21 25 35 37 27 18 35 31 35 40 32 23 23 39 19 33 25 33\n [9409] 35 30 23 30 22 25 18 31 27 31 19 19 35 39 40 35 37 37 27 18 20 39 36 22\n [9433] 34 26 36 38 24 32 33 23 34 26 31 37 27 30 19 25 21 22 18 29 35 32 22 21\n [9457] 32 27 31 36 28 33 40 36 22 36 32 37 29 39 32 31 33 36 35 30 40 40 39 27\n [9481] 35 37 20 31 24 37 25 37 31 22 28 35 30 40 27 30 30 27 29 37 25 23 29 23\n [9505] 22 22 28 40 35 26 36 40 34 35 40 21 18 38 27 39 31 20 22 30 29 25 27 40\n [9529] 36 39 26 24 27 26 20 31 35 35 19 40 23 24 20 32 25 33 27 26 20 34 22 35\n [9553] 36 31 38 37 23 30 22 26 28 40 36 38 39 24 39 19 38 40 21 38 18 38 19 24\n [9577] 35 30 31 26 35 20 33 29 27 18 31 40 28 19 40 23 27 33 21 37 22 27 20 37\n [9601] 35 36 18 38 34 21 26 20 26 25 39 31 28 27 29 29 38 29 21 38 19 19 39 31\n [9625] 34 28 26 34 30 40 22 29 36 22 32 32 19 32 22 18 23 21 33 23 19 26 20 19\n [9649] 33 28 39 24 23 27 38 22 24 28 19 32 39 23 27 18 19 19 32 24 38 36 22 31\n [9673] 18 27 39 35 19 18 24 34 22 26 37 28 25 29 27 27 40 29 23 20 22 31 36 25\n [9697] 27 23 33 31 36 28 24 39 19 32 22 28 26 38 34 28 30 18 36 37 26 28 29 31\n [9721] 30 39 33 30 21 20 20 18 33 28 40 28 22 36 39 25 39 19 29 30 30 22 24 27\n [9745] 39 20 29 32 26 27 23 34 31 31 27 22 38 32 39 22 30 27 21 30 28 34 35 36\n [9769] 40 31 20 19 29 33 29 34 24 18 27 20 22 39 36 39 37 21 31 33 31 38 40 22\n [9793] 26 24 31 29 36 20 27 35 23 34 31 20 39 29 31 26 23 21 32 32 40 29 36 32\n [9817] 23 35 28 34 28 31 31 39 21 34 38 27 28 28 18 27 37 38 27 23 19 25 31 18\n [9841] 19 25 35 25 18 25 31 24 30 27 35 28 32 26 26 40 35 28 28 38 31 31 34 30\n [9865] 39 19 26 36 31 30 22 18 29 38 20 31 34 23 24 19 39 26 22 40 33 36 21 40\n [9889] 38 29 20 39 23 25 35 35 39 35 35 36 37 39 24 39 39 22 20 26 29 34 29 28\n [9913] 26 35 33 35 21 25 40 38 35 26 19 24 21 22 33 19 23 25 32 30 31 27 23 36\n [9937] 39 36 29 26 24 22 29 31 23 23 30 27 19 22 24 18 26 22 35 28 34 26 22 34\n [9961] 21 28 20 21 33 35 30 26 18 28 26 27 27 33 31 32 20 36 30 19 35 38 21 18\n [9985] 39 30 32 31 22 33 32 29 34 38 26 37 36 37 22 18 37 39 25 31 20 38 22 36\n[10009] 34 26 38 24 37 25 35 28 18 32 37 26 40 25 27 33 40 31 38 31 39 29 30 28\n[10033] 20 39 22 24 36 26 21 21 26 18 20 39 25 29 38 39 24 34 21 20 25 38 23 28\n[10057] 18 37 38 22 19 39 23 26 27 34 30 23 22 24 36 35 36 34 25 27 18 21 22 28\n[10081] 37 34 34 37 25 23 23 32 23 27 35 26 39 25 22 33 19 27 26 25 19 27 24 22\n[10105] 36 36 31 29 29 38 40 25 38 25 28 31 35 26 27 22 23 39 29 39 30 26 37 18\n[10129] 22 20 39 34 22 40 27 34 23 36 30 20 39 29 29 28 20 32 27 33 33 28 22 28\n[10153] 25 24 39 24 22 23 36 21 35 33 26 40 24 33 35 34 23 40 20 24 22 37 29 26\n[10177] 31 31 33 22 23 40 29 21 27 31 24 23 24 28 35 27 33 35 21 31 22 25 39 19\n[10201] 33 28 24 27 35 34 36 31 26 26 32 25 19 38 25 18 36 29 32 30 31 23 32 25\n[10225] 24 35 26 39 37 34 19 38 22 39 33 40 36 33 30 28 36 39 26 29 19 20 26 28\n[10249] 29 35 37 19 35 39 24 35 29 32 26 20 30 25 20 39 34 27 38 25 24 39 19 35\n[10273] 20 36 19 40 39 29 19 31 38 36 23 37 23 33 24 40 38 32 40 27 19 29 32 27\n[10297] 35 33 34 27 37 20 26 33 18 34 32 30 25 22 25 22 34 19 26 38 36 32 39 25\n[10321] 20 30 26 32 31 18 38 32 40 33 26 40 30 25 25 32 30 36 27 24 28 26 28 40\n[10345] 39 40 33 24 35 29 22 31 22 40 32 29 28 28 32 29 24 38 24 19 23 36 21 20\n[10369] 24 40 26 24 25 18 36 36 20 32 24 29 18 27 23 30 39 37 20 18 20 37 28 28\n[10393] 24 36 19 29 20 30 33 34 20 31 18 28 23 28 22 32 21 27 36 25 20 29 33 30\n[10417] 24 37 39 29 22 36 37 32 40 38 23 26 38 32 38 34 39 33 34 34 40 22 29 31\n[10441] 39 27 36 38 34 30 19 35 39 26 37 31 19 24 34 37 36 33 30 29 19 25 36 36\n[10465] 26 25 40 31 28 36 30 26 22 29 30 19 34 30 39 40 25 28 28 38 34 40 23 38\n[10489] 19 38 18 30 18 19 26 37 31 19 29 26 36 38 37 35 33 26 40 30 39 27 32 29\n[10513] 26 29 32 26 39 38 30 34 25 19 24 30 18 35 18 38 34 18 23 36 36 40 40 32\n[10537] 40 29 27 21 27 35 33 36 38 38 23 20 36 24 32 35 29 23 37 23 22 19 29 37\n[10561] 32 18 32 35 24 34 35 36 19 21 35 25 18 23 35 19 38 31 23 26 32 21 35 21\n[10585] 34 29 18 33 35 27 36 30 27 21 24 18 34 28 33 35 24 40 21 21 29 19 32 36\n[10609] 29 29 20 21 30 20 30 19 33 23 29 33 34 19 28 40 35 27 22 24 28 25 18 26\n[10633] 24 31 21 25 28 24 18 35 31 27 32 20 40 18 35 35 29 25 18 22 26 38 25 22\n[10657] 40 21 34 32 20 28 24 35 19 29 20 29 24 34 26 32 29 40 38 20 21 37 30 22\n[10681] 36 38 20 22 33 33 21 38 34 40 27 26 23 18 28 36 25 19 19 19 21 25 22 37\n[10705] 22 33 29 31 22 26 18 32 29 34 38 30 22 27 22 28 21 29 23 39 33 18 19 27\n[10729] 23 34 40 21 24 39 23 25 26 35 20 21 37 33 23 32 37 24 22 33 38 21 18 23\n[10753] 35 37 25 30 29 33 22 18 28 30 26 29 35 33 33 29 29 31 25 36 29 22 31 26\n[10777] 35 31 28 39 22 33 27 39 30 34 35 19 31 39 30 36 30 31 21 29 19 21 24 22\n[10801] 18 31 35 29 29 29 19 37 37 27 24 23 23 21 35 25 38 31 25 33 39 37 29 33\n[10825] 27 30 33 23 25 39 37 25 22 20 26 20 39 39 24 21 35 36 22 33 30 39 21 33\n[10849] 24 19 23 39 24 23 28 31 40 34 23 23 28 38 31 33 33 40 29 21 18 26 28 39\n[10873] 33 29 29 27 28 33 33 26 29 20 29 26 28 30 23 18 24 23 25 38 25 30 37 34\n[10897] 36 32 25 35 35 20 23 33 27 40 35 24 24 25 24 19 28 29 33 33 28 40 35 38\n[10921] 37 21 33 23 39 32 32 21 27 29 32 26 28 19 27 22 19 34 18 34 21 30 25 34\n[10945] 18 36 19 36 32 20 34 27 19 20 26 39 29 21 28 30 25 27 37 36 23 30 22 38\n[10969] 38 32 30 25 39 30 29 36 28 26 27 20 34 29 35 34 33 37 38 20 39 36 19 38\n[10993] 39 35 23 24 34 38 31 30 19 29 34 37 39 40 36 23 33 35 18 33 20 28 31 35\n[11017] 23 26 23 29 25 25 36 27 20 28 22 39 23 27 28 21 34 36 38 29 18 33 31 28\n[11041] 24 29 33 36 25 34 21 37 40 37 28 18 39 31 18 27 23 30 37 30 20 36 33 33\n[11065] 36 40 30 29 28 35 23 39 21 32 36 33 35 20 23 30 21 28 36 23 40 21 23 40\n[11089] 38 28 33 37 37 37 19 27 21 39 23 33 21 24 33 18 23 39 30 38 39 23 30 18\n[11113] 18 27 20 25 37 18 39 40 25 19 18 37 21 22 34 20 40 21 31 35 27 37 29 34\n[11137] 18 24 26 30 35 28 32 38 20 38 30 35 36 29 21 39 35 26 22 28 28 36 30 36\n[11161] 39 23 28 21 28 39 23 40 22 30 36 22 23 40 18 34 30 28 20 27 19 30 31 40\n[11185] 22 30 20 34 33 36 19 26 34 21 30 24 36 38 31 21 35 19 32 30 33 27 39 32\n[11209] 28 28 21 18 33 22 23 38 26 26 18 33 23 31 34 33 28 27 30 27 30 34 34 35\n[11233] 29 32 30 39 20 21 24 38 40 31 37 28 22 40 30 27 28 33 32 31 24 35 39 39\n[11257] 34 29 23 26 18 29 25 18 40 36 18 33 36 25 20 31 40 22 39 26 21 40 25 21\n[11281] 27 40 32 35 34 34 29 25 25 21 38 19 26 20 18 18 38 30 31 37 18 37 21 26\n[11305] 32 18 22 34 28 35 33 28 28 35 33 24 21 30 26 30 34 28 37 30 35 30 33 28\n[11329] 19 38 28 26 39 20 18 32 39 29 33 23 21 19 34 19 20 24 30 29 31 26 20 28\n[11353] 26 33 29 22 26 26 31 26 28 30 21 19 19 21 29 25 29 27 22 40 20 27 23 36\n[11377] 18 24 26 18 21 21 39 24 37 28 33 40 36 24 25 23 24 38 31 32 22 25 21 29\n[11401] 36 37 37 19 38 28 39 24 28 21 36 37 35 40 28 20 39 32 23 34 34 27 18 31\n[11425] 20 33 33 22 22 28 18 32 23 38 23 29 19 37 30 22 38 38 39 39 35 32 20 25\n[11449] 36 35 23 28 29 36 20 22 30 18 26 23 37 40 19 37 19 29 29 28 18 26 40 29\n[11473] 40 40 23 31 30 22 24 19 20 26 35 32 36 39 32 23 38 34 38 23 20 21 27 20\n[11497] 29 23 33 28 40 27 22 34 32 40 36 22 19 38 30 22 29 21 35 32 39 20 30 40\n[11521] 21 23 36 35 37 35 30 31 34 21 32 19 37 22 29 40 19 37 40 32 18 30 35 25\n[11545] 18 29 35 34 25 20 40 29 21 24 33 25 22 38 33 19 19 28 22 36 29 38 28 22\n[11569] 30 27 36 23 23 40 28 37 32 32 28 31 18 26 40 35 33 26 21 22 38 39 40 30\n[11593] 23 38 29 32 34 39 37 24 33 25 39 28 25 40 39 20 34 33 31 35 26 25 28 21\n[11617] 27 18 19 19 34 23 24 26 36 18 20 25 26 25 32 30 21 37 30 23 25 22 32 38\n[11641] 38 25 38 22 20 39 21 35 39 39 37 27 32 22 36 33 37 33 36 31 21 29 22 27\n[11665] 24 37 35 36 24 18 39 28 27 18 32 38 36 33 28 30 37 21 28 35 27 34 27 38\n[11689] 39 28 35 32 35 20 39 30 20 39 23 18 27 33 33 27 18 28 39 40 36 38 22 20\n[11713] 38 28 24 37 22 35 31 24 27 25 38 35 38 20 20 27 20 29 24 21 29 20 20 23\n[11737] 26 33 18 36 26 36 36 31 18 31 28 27 40 19 32 27 32 37 18 22 37 38 25 32\n[11761] 25 20 31 33 33 40 28 26 22 34 28 24 23 40 29 22 40 36 38 25 30 36 33 24\n[11785] 23 20 22 27 36 38 20 37 29 18 35 36 23 30 35 18 18 21 24 19 22 40 33 38\n[11809] 32 26 24 38 34 19 31 23 39 40 31 32 23 25 40 29 22 36 23 39 27 33 34 19\n[11833] 18 19 26 29 37 21 37 22 26 21 32 31 19 37 26 18 37 29 21 39 25 19 26 39\n[11857] 34 18 28 37 28 31 39 37 40 21 37 36 37 19 39 24 36 21 23 29 35 35 25 33\n[11881] 30 26 24 37 39 39 28 38 33 38 24 29 18 22 19 31 27 30 36 30 28 38 34 34\n[11905] 19 25 29 33 33 28 37 22 30 28 40 31 20 30 22 26 18 23 22 34 18 39 19 20\n[11929] 24 33 24 21 18 38 23 36 37 35 22 39 18 19 38 22 25 35 25 29 32 33 19 40\n[11953] 23 39 19 40 25 31 26 18 34 33 32 27 23 23 32 29 26 38 26 30 24 22 35 28\n[11977] 33 21 32 36 28 30 22 24 29 35 20 21 37 40 38 38 35 24 22 20 32 34 28 37\n[12001] 33 40 23 21 24 30 19 24 32 18 19 27 39 28 38 30 18 29 31 29 29 31 35 32\n[12025] 39 22 18 18 27 31 40 39 33 19 34 26 36 30 27 31 26 23 35 29 18 21 37 26\n[12049] 35 19 18 32 26 31 21 25 22 18 27 37 21 35 27 38 31 39 40 19 25 18 22 39\n[12073] 33 39 27 20 33 37 28 24 25 18 19 33 33 39 30 25 27 23 35 20 26 24 34 40\n[12097] 36 38 39 18 24 20 40 24 19 28 37 20 30 18 38 38 25 22 30 18 31 19 28 22\n[12121] 40 28 29 19 35 36 23 21 28 29 28 30 26 35 34 27 21 20 30 29 38 29 34 22\n[12145] 35 33 24 24 19 29 24 21 34 25 37 40 31 34 32 34 27 21 34 24 22 18 25 26\n[12169] 34 31 26 29 23 29 20 27 36 39 29 23 36 23 29 37 33 21 39 22 34 34 26 27\n[12193] 39 39 21 19 27 23 27 39 37 21 37 21 30 20 31 23 28 24 35 25 22 24 34 28\n[12217] 31 20 26 40 24 23 18 19 25 23 32 34 22 28 28 21 18 38 26 22 20 37 37 27\n[12241] 24 35 26 21 19 21 34 25 32 22 40 19 21 32 26 40 31 37 24 31 33 30 26 33\n[12265] 31 32 22 19 35 25 32 34 31 39 18 21 24 21 28 29 33 30 25 34 33 29 38 25\n[12289] 23 26 30 35 30 40 38 34 33 40 37 32 32 33 38 26 25 40 30 22 31 25 38 33\n[12313] 30 32 24 33 28 22 38 30 35 39 30 20 20 22 22 20 30 20 33 28 28 22 32 34\n[12337] 40 26 26 31 28 26 40 39 26 31 33 37 31 22 33 20 31 34 21 32 26 19 30 38\n[12361] 32 39 37 28 24 33 38 23 21 30 28 18 31 22 19 27 18 29 18 40 21 25 32 28\n[12385] 24 36 39 40 20 22 33 35 28 19 37 32 36 38 33 30 27 38 23 31 18 32 30 40\n[12409] 26 32 33 36 18 19 26 28 33 27 23 30 35 37 32 32 24 39 40 30 35 18 19 18\n[12433] 19 23 39 26 35 40 35 30 40 22 27 36 33 25 35 35 25 24 18 37 37 23 27 28\n[12457] 24 19 32 30 35 38 29 22 28 32 35 20 38 32 25 19 32 37 28 36 18 30 28 31\n[12481] 24 30 27 20 25 19 19 23 26 23 31 19 33 35 19 27 34 32 39 33 34 19 38 36\n[12505] 28 23 26 21 34 38 27 32 23 24 37 29 32 22 28 23 25 27 32 27 40 20 26 29\n[12529] 34 22 23 38 23 40 26 19 33 24 39 36 37 33 28 32 34 35 30 32 24 23 33 31\n[12553] 30 40 38 30 28 24 27 30 21 35 33 18 32 22 24 25 29 40 34 38 35 28 36 39\n[12577] 40 19 32 27 36 33 33 35 39 31 25 25 27 40 40 38 26 24 18 38 27 25 18 20\n[12601] 38 36 21 18 28 39 30 20 36 26 28 38 24 34 26 19 34 35 32 33 19 34 34 28\n[12625] 22 33 34 27 24 31 18 24 40 40 39 28 36 18 22 26 33 24 18 35 40 31 29 30\n[12649] 37 40 21 24 25 36 21 25 21 31 27 37 23 20 30 19 24 33 38 21 24 33 18 39\n[12673] 24 35 37 26 40 23 22 27 38 35 31 27 18 33 21 35 23 37 36 21 37 18 29 33\n[12697] 37 21 38 23 29 39 30 26 25 37 21 40 38 27 18 24 39 25 20 37 18 30 28 32\n[12721] 23 21 40 35 33 20 18 36 37 30 40 40 30 25 23 21 34 19 36 18 22 21 21 40\n[12745] 25 31 28 20 29 36 30 32 32 29 36 21 24 22 30 26 34 22 29 29 27 27 38 40\n[12769] 27 25 22 20 33 38 28 36 36 20 29 27 21 30 20 39 19 31 40 23 27 37 21 25\n[12793] 34 19 24 23 29 25 37 27 34 18 36 38 38 28 18 32 21 20 34 40 29 29 23 38\n[12817] 33 27 37 29 18 26 38 21 21 35 33 23 32 36 40 26 19 25 18 19 38 39 20 25\n[12841] 34 19 39 22 35 18 36 22 24 31 21 35 18 37 32 33 21 33 34 35 23 20 22 22\n[12865] 20 28 32 18 32 25 33 40 40 30 18 18 19 20 37 24 28 29 29 24 35 30 40 23\n[12889] 33 25 20 21 40 23 34 25 26 30 24 33 29 35 30 32 22 24 40 33 25 24 30 19\n[12913] 40 24 27 27 36 23 21 30 30 20 35 28 37 32 34 32 20 21 27 24 22 22 21 32\n[12937] 28 18 23 21 26 22 36 32 35 27 26 32 37 18 30 30 33 34 32 26 27 26 21 24\n[12961] 27 20 22 34 37 25 34 23 21 38 21 36 27 18 27 40 19 36 27 40 33 21 26 28\n[12985] 19 25 37 37 34 22 37 25 21 34 40 37 28 29 38 19 21 25 33 26 18 37 38 34\n[13009] 27 36 30 35 34 38 28 20 37 35 33 31 33 29 27 34 28 18 18 31 32 21 20 40\n[13033] 30 18 36 25 27 21 35 18 20 30 35 34 40 37 23 22 31 26 31 36 38 28 34 25\n[13057] 38 34 33 36 32 18 21 30 33 34 22 21 40 23 20 28 36 26 35 36 19 32 33 40\n[13081] 26 40 39 20 32 18 23 26 32 22 27 18 23 29 37 31 30 18 22 27 33 33 25 21\n[13105] 20 23 33 32 18 30 27 31 27 39 30 37 24 32 27 28 39 40 37 26 38 31 40 40\n[13129] 25 28 25 32 33 28 24 19 27 29 40 28 33 26 18 21 35 23 18 35 30 31 35 30\n[13153] 33 31 39 31 32 28 29 40 35 22 25 21 34 29 25 27 18 23 34 24 39 38 33 33\n[13177] 18 19 39 27 31 23 33 38 24 39 21 34 35 33 28 30 27 35 21 29 22 22 31 22\n[13201] 31 22 32 34 39 34 28 25 28 33 36 33 38 31 19 25 26 36 22 20 26 34 37 29\n[13225] 20 23 18 20 32 35 38 18 35 20 23 33 19 32 20 32 32 21 25 33 35 22 37 38\n[13249] 20 27 23 29 27 30 38 35 21 20 18 27 21 27 25 35 34 27 24 32 32 35 18 20\n[13273] 34 27 40 29 20 26 26 33 30 37 34 37 38 22 19 21 34 24 35 40 32 31 38 29\n[13297] 38 22 21 33 25 36 29 38 24 30 21 38 23 27 40 28 24 30 40 32 20 38 21 19\n[13321] 22 21 30 19 28 32 37 35 31 33 21 20 40 29 20 22 34 30 29 29 39 24 38 38\n[13345] 23 26 25 20 23 21 33 24 22 37 24 28 27 37 21 22 21 30 34 24 18 30 18 30\n[13369] 36 39 23 34 32 35 22 30 24 22 21 38 22 38 36 27 32 27 23 24 36 23 31 37\n[13393] 20 31 35 35 27 24 27 39 22 26 22 27 26 26 33 35 22 30 27 36 29 26 29 18\n[13417] 28 20 26 35 34 23 23 24 23 22 37 21 18 38 30 28 20 22 33 18 37 28 29 32\n[13441] 39 24 19 30 36 26 21 31 24 26 20 24 24 26 40 32 19 24 27 30 20 24 27 19\n[13465] 27 37 19 22 21 37 18 38 37 28 31 37 18 34 40 29 33 37 26 28 26 26 31 38\n[13489] 31 23 36 35 23 39 31 23 38 32 26 38 21 30 38 28 24 38 32 38 40 40 30 20\n[13513] 28 20 34 30 28 18 36 18 35 29 40 32 31 38 33 30 25 31 21 36 40 34 22 39\n[13537] 24 18 21 20 22 33 18 36 36 40 25 34 39 25 31 24 29 32 39 38 36 25 25 40\n[13561] 39 27 38 24 30 26 19 34 24 21 36 39 19 31 24 36 36 34 26 26 27 28 40 38\n[13585] 18 25 40 40 26 37 19 19 35 22 30 24 33 28 24 18 35 33 30 25 37 39 23 38\n[13609] 29 38 21 39 35 20 22 22 40 39 26 36 26 39 26 38 39 34 22 39 23 23 36 26\n[13633] 39 21 39 19 34 25 39 36 40 38 40 27 31 28 37 33 34 18 21 27 29 31 21 33\n[13657] 27 19 31 20 37 39 32 36 35 40 34 22 22 31 35 28 39 25 34 18 20 29 29 22\n[13681] 30 27 38 32 30 18 21 21 27 30 26 38 24 24 32 27 23 22 27 27 19 20 20 39\n[13705] 26 24 31 26 34 31 23 25 39 39 39 27 39 28 33 22 23 35 27 32 30 21 19 32\n[13729] 38 24 39 27 25 33 36 40 39 25 25 26 26 22 38 33 21 20 38 19 34 23 37 40\n[13753] 38 20 40 40 27 19 36 21 39 21 25 33 28 34 36 19 38 30 20 19 34 18 35 31\n[13777] 26 27 23 25 38 28 31 21 32 34 28 26 25 35 31 21 22 40 21 36 36 29 21 39\n[13801] 31 21 39 26 29 40 24 20 30 37 26 24 30 26 28 37 32 25 23 19 39 23 32 31\n[13825] 26 35 37 32 30 32 38 40 25 38 27 38 30 22 26 23 22 26 22 28 19 26 29 40\n[13849] 27 20 26 32 29 21 38 23 18 38 20 35 26 33 20 34 32 39 35 21 32 23 27 32\n[13873] 19 29 27 40 20 35 24 23 18 39 25 40 30 23 37 36 27 23 36 22 26 32 37 28\n[13897] 31 37 33 24 34 18 30 24 28 32 24 19 39 31 39 38 36 30 35 22 22 29 33 20\n[13921] 38 31 37 32 24 27 30 26 32 35 31 35 40 32 26 26 30 23 31 19 18 19 39 33\n[13945] 18 25 37 25 20 30 34 33 20 22 30 38 35 23 22 32 23 18 37 35 34 19 21 37\n[13969] 32 22 24 35 26 32 18 29 38 24 23 27 28 38 40 23 27 18 18 20 36 31 20 37\n[13993] 40 21 30 34 24 37 35 22 20 37 28 19 24 29 22 24 32 23 23 22 23 25 33 23\n[14017] 40 28 23 37 34 21 20 18 39 20 30 34 34 23 30 36 30 34 40 32 36 20 38 36\n[14041] 24 18 28 35 32 29 31 35 39 22 39 27 24 26 28 38 26 18 31 25 21 34 27 24\n[14065] 30 33 39 39 33 25 36 18 32 30 37 32 20 30 25 22 22 40 32 38 33 27 23 28\n[14089] 22 33 38 18 36 21 33 18 32 37 30 20 38 25 31 23 18 23 30 26 39 26 29 31\n[14113] 36 28 19 22 40 36 39 39 20 26 29 32 20 39 40 22 26 40 26 25 21 39 36 35\n[14137] 36 23 35 24 19 32 24 23 37 33 21 32 18 26 39 37 35 36 33 39 36 23 23 30\n[14161] 30 37 29 34 26 25 20 23 19 21 28 22 26 28 24 26 36 25 30 40 34 20 23 28\n[14185] 34 23 19 30 35 27 34 40 25 34 19 36 22 27 21 35 20 38 34 25 23 36 29 36\n[14209] 20 31 21 36 40 39 25 36 36 32 34 33 20 29 35 36 35 39 33 32 40 39 34 26\n[14233] 35 38 27 32 38 38 20 38 19 24 28 29 24 34 24 22 25 37 39 24 39 39 19 27\n[14257] 21 39 25 28 25 34 18 20 20 22 37 39 37 33 30 29 28 27 40 27 18 25 29 38\n[14281] 37 40 28 21 28 32 26 31 19 21 31 40 36 21 32 38 40 23 20 28 37 30 40 36\n[14305] 27 34 39 24 29 28 27 18 29 21 21 21 28 37 32 37 32 37 32 23 28 20 40 21\n[14329] 40 20 40 20 39 36 23 32 25 30 20 18 27 23 31 39 26 30 27 37 40 23 33 35\n[14353] 27 21 22 28 35 34 37 29 39 32 36 29 39 34 36 24 29 36 23 36 33 32 31 36\n[14377] 35 35 32 29 28 37 19 40 40 29 27 35 21 33 33 34 22 35 34 18 36 22 30 19\n[14401] 34 27 34 40 35 36 30 27 30 24 35 21 23 34 32 21 32 39 26 29 36 33 29 27\n[14425] 23 24 22 20 29 35 28 40 35 29 22 26 36 33 39 38 36 36 24 23 23 25 20 32\n[14449] 32 30 21 21 38 29 24 28 37 25 27 38 20 32 18 33 26 23 30 29 26 37 24 20\n[14473] 32 24 21 36 21 21 24 20 34 30 19 36 28 23 28 19 19 39 35 23 25 21 18 38\n[14497] 27 39 22 23 28 21 35 23 36 27 40 29 27 26 25 37 28 21 36 33 22 21 34 38\n[14521] 18 39 36 25 31 37 22 28 38 20 39 27 27 20 21 21 32 31 39 28 22 35 21 18\n[14545] 39 22 35 32 37 32 30 34 22 30 33 26 39 40 31 20 18 27 32 22 21 33 37 26\n[14569] 37 35 32 18 29 22 28 26 26 32 28 34 20 30 36 40 34 20 33 25 30 23 24 23\n[14593] 33 38 27 38 34 32 18 18 22 30 22 34 19 35 24 28 19 30 22 24 35 32 25 20\n[14617] 26 21 26 37 31 37 36 34 29 37 22 36 18 31 33 21 20 27 28 40 22 29 21 18\n[14641] 38 23 22 34 38 26 36 38 22 18 25 21 23 23 24 27 24 40 23 26 31 18 32 21\n[14665] 29 19 34 29 27 28 39 20 25 20 36 40 28 32 27 36 24 32 29 35 40 20 37 22\n[14689] 40 30 20 32 34 36 28 19 36 19 38 28 25 31 18 31 38 27 39 29 23 27 32 28\n[14713] 22 22 28 22 25 35 40 22 36 26 28 20 38 26 24 25 19 24 40 27 30 40 33 31\n[14737] 39 37 26 33 23 28 33 20 25 39 24 33 24 21 22 18 19 39 36 25 30 25 36 32\n[14761] 22 36 32 37 22 40 31 22 31 29 31 31 37 33 36 35 27 28 27 36 25 24 19 36\n[14785] 40 28 34 25 19 31 22 21 26 18 20 29 31 39 30 30 34 27 29 26 39 19 26 29\n[14809] 32 38 31 31 26 28 18 30 38 38 20 30 37 27 22 26 36 28 33 27 39 20 29 36\n[14833] 36 30 34 28 39 29 29 20 32 20 27 37 31 25 25 25 25 34 18 22 18 23 34 34\n[14857] 26 40 38 24 33 22 19 30 31 23 31 18 24 23 20 21 36 18 18 32 29 31 27 28\n[14881] 33 21 25 18 30 36 39 30 39 37 28 25 40 28 37 28 37 21 27 37 22 27 37 23\n[14905] 37 21 28 20 21 31 36 24 22 26 33 38 31 28 26 31 38 25 29 19 29 28 28 20\n[14929] 19 29 25 34 21 27 20 26 28 30 18 18 37 25 18 40 39 32 33 18 34 33 35 40\n[14953] 23 21 18 32 19 21 38 19 23 30 28 27 19 21 18 28 37 19 20 40 32 27 23 37\n[14977] 26 22 26 27 33 39 26 32 37 19 22 26 33 23 21 36 39 40 24 31 28 35 30 34\n[15001] 29 22 33 31 29 34 27 40 30 30 30 23 24 36 19 38 37 26 37 20 38 36 29 36\n[15025] 20 21 29 21 22 20 31 18 35 28 19 28 26 32 33 35 29 23 26 31 22 21 33 37\n[15049] 25 25 25 28 29 38 39 25 18 30 34 20 23 25 31 21 24 39 25 32 19 33 33 26\n[15073] 24 36 36 37 37 35 28 36 26 30 39 35 36 22 32 29 18 25 37 40 39 37 35 20\n[15097] 28 29 23 39 28 38 34 19 40 39 37 40 29 37 40 34 32 28 37 23 24 35 31 24\n[15121] 28 32 26 39 23 21 28 19 29 25 22 25 39 33 21 35 31 35 38 36 36 18 33 28\n[15145] 27 22 25 38 28 19 18 32 23 26 18 24 29 19 27 37 18 34 31 23 21 24 29 30\n[15169] 33 19 27 27 26 31 40 36 19 24 21 20 33 31 28 19 21 36 38 27 26 29 36 38\n[15193] 23 18 38 32 26 37 29 19 20 28 30 27 28 28 33 22 32 21 19 37 18 29 21 38\n[15217] 40 37 25 20 36 27 28 30 22 24 24 28 22 30 25 31 20 26 25 26 35 19 23 37\n[15241] 27 35 38 21 23 38 25 32 19 31 28 37 36 30 29 19 24 26 31 21 22 21 26 26\n[15265] 26 26 32 40 26 33 20 27 39 28 39 29 36 31 31 37 36 29 20 19 21 33 22 23\n[15289] 23 23 25 32 21 28 37 20 18 35 34 36 22 34 36 25 29 29 22 35 31 31 39 40\n[15313] 25 31 37 35 24 39 35 20 19 35 32 25 22 39 38 29 24 29 32 19 25 31 37 20\n[15337] 23 26 32 34 18 36 26 30 29 34 21 20 24 30 25 33 18 22 30 23 34 36 37 28\n[15361] 28 25 34 19 40 21 29 38 29 36 32 32 19 35 22 31 28 39 34 40 38 18 27 35\n[15385] 23 38 37 26 24 19 20 40 18 29 25 20 32 24 40 37 25 30 21 28 36 36 25 24\n[15409] 37 30 38 27 33 22 19 29 29 35 27 24 28 28 19 19 24 40 18 19 23 19 33 18\n[15433] 37 40 29 29 39 36 34 31 32 35 23 20 18 36 20 26 27 22 32 22 25 40 37 29\n[15457] 38 35 35 36 28 25 28 40 38 30 31 28 27 34 30 27 18 24 37 37 35 21 28 38\n[15481] 32 32 28 23 26 30 21 34 37 38 40 32 39 26 38 34 34 19 20 34 36 30 20 32\n[15505] 39 20 36 24 39 39 40 25 18 24 19 21 21 38 18 22 19 40 34 29 25 34 33 22\n[15529] 19 23 26 33 36 34 26 32 20 23 39 23 22 23 25 22 32 18 23 32 32 33 40 29\n[15553] 29 39 24 38 34 33 25 30 18 20 29 18 40 26 36 24 39 31 22 24 31 31 30 39\n[15577] 19 34 39 19 18 40 27 25 38 26 35 24 27 24 38 18 36 37 18 18 37 24 25 30\n[15601] 18 25 39 32 25 39 18 24 20 36 18 20 26 37 21 18 21 22 23 24 28 22 24 22\n[15625] 24 25 30 40 24 27 33 31 29 23 33 35 28 38 22 28 35 36 19 20 32 26 27 23\n[15649] 21 37 28 38 27 21 30 28 29 20 24 25 29 36 35 23 23 18 35 21 28 29 24 18\n[15673] 34 18 36 33 25 33 37 29 37 24 20 21 36 22 37 26 38 40 38 27 26 35 30 25\n[15697] 33 31 35 23 34 21 35 33 19 40 40 32 25 37 31 24 39 36 38 34 22 34 35 31\n[15721] 18 37 19 33 26 35 36 19 21 21 27 21 22 20 31 32 31 40 19 29 27 37 22 38\n[15745] 32 38 25 22 39 23 18 25 26 35 39 35 18 28 30 37 20 28 36 29 33 26 25 32\n[15769] 26 26 38 23 29 23 20 26 36 40 20 28 26 30 28 36 18 24 34 31 22 18 39 32\n[15793] 33 34 36 30 31 28 29 35 32 20 22 24 37 38 34 40 27 18 32 37 27 31 39 21\n[15817] 35 35 35 38 40 33 27 20 29 24 39 38 30 36 32 19 37 26 23 34 40 25 19 34\n[15841] 22 24 25 19 35 24 25 38 21 23 34 35 27 26 37 19 18 34 27 38 32 39 21 39\n[15865] 24 29 20 29 32 28 40 20 40 23 38 18 27 24 31 35 33 22 18 19 22 27 18 31\n[15889] 39 25 31 21 25 24 32 39 33 39 18 31 26 30 30 27 19 35 18 23 28 39 18 20\n[15913] 25 30 36 34 18 34 27 36 26 23 18 30 27 22 30 35 18 33 30 32 25 31 30 21\n[15937] 39 36 23 31 22 34 38 26 36 19 18 39 24 30 24 23 34 38 29 36 32 38 40 23\n[15961] 35 37 34 26 18 39 38 32 30 38 20 21 40 34 37 21 36 32 22 27 31 30 18 33\n[15985] 30 30 35 28 33 37 34 33 27 32 22 36 25 27 38 20 35 36 22 28 37 38 37 22\n[16009] 37 18 23 40 40 32 33 40 20 29 31 37 37 33 24 35 27 23 21 23 40 31 22 25\n[16033] 19 30 36 18 20 18 20 18 19 25 26 32 26 26 40 28 29 37 18 29 33 25 18 21\n[16057] 25 18 37 37 20 39 36 32 32 34 18 37 20 36 32 26 23 19 26 30 18 39 35 19\n[16081] 33 25 32 39 33 31 31 23 20 39 25 30 30 32 28 23 30 36 21 35 39 31 34 30\n[16105] 30 23 18 29 34 26 29 34 32 38 24 28 18 28 32 23 21 24 27 18 38 31 18 20\n[16129] 39 19 33 29 34 23 22 23 25 24 30 40 33 33 33 25 21 19 27 28 22 38 28 23\n[16153] 38 40 21 36 20 40 27 28 39 39 26 36 35 36 19 21 21 36 39 34 37 26 35 20\n[16177] 26 23 38 36 40 28 33 26 21 34 40 23 25 19 29 39 29 34 31 26 38 40 32 27\n[16201] 28 40 29 22 39 31 32 26 38 21 20 39 22 18 36 37 18 34 39 28 25 31 22 20\n[16225] 40 39 20 37 39 36 40 26 34 30 21 18 20 35 39 25 22 33 26 36 22 34 20 30\n[16249] 22 21 38 20 27 27 33 33 30 38 37 36 36 39 36 39 26 26 33 32 36 37 25 28\n[16273] 35 32 22 29 33 19 35 37 29 34 32 24 27 26 39 25 35 24 18 26 25 28 19 33\n[16297] 38 21 18 39 38 24 23 33 20 23 35 32 37 21 18 24 31 32 24 40 23 23 29 33\n[16321] 19 36 25 18 39 32 28 23 24 27 28 30 39 39 38 36 26 25 40 25 36 26 35 38\n[16345] 19 32 36 24 39 39 19 31 23 31 23 33 35 32 32 28 20 20 34 40 31 33 18 30\n[16369] 20 28 37 22 21 30 30 24 27 28 39 38 38 35 23 25 20 28 24 34 21 29 36 21\n[16393] 36 20 32 35 22 20 29 24 39 32 29 25 26 36 35 24 26 27 26 30 21 24 33 24\n[16417] 18 33 36 40 35 27 23 30 36 39 20 19 31 32 25 21 20 21 39 32 21 30 21 19\n[16441] 22 33 31 21 28 39 35 27 20 29 31 39 23 24 39 29 35 31 28 18 28 37 27 38\n[16465] 30 31 35 28 30 39 22 26 40 36 20 32 30 20 26 31 39 26 18 29 31 22 29 22\n[16489] 32 36 21 22 28 22 22 35 29 32 29 22 29 28 29 28 25 37 33 23 20 38 21 24\n[16513] 29 21 19 27 27 35 29 38 25 18 19 28 37 38 38 20 31 40 18 22 37 27 23 32\n[16537] 27 38 19 37 40 28 37 34 37 40 36 21 28 19 21 30 34 33 27 35 31 32 25 20\n[16561] 18 37 32 26 40 19 35 33 29 31 35 23 37 20 37 22 19 35 28 31 37 28 33 36\n[16585] 28 31 34 36 28 40 30 25 20 35 37 21 33 22 33 33 28 33 22 38 23 39 21 35\n[16609] 26 34 36 25 37 34 26 33 27 23 34 29 34 37 29 33 20 30 35 24 20 36 19 39\n[16633] 27 27 31 39 28 28 19 39 23 26 33 35 38 29 35 19 32 37 26 25 20 27 34 25\n[16657] 21 37 18 24 37 30 23 26 28 32 19 20 35 34 34 26 27 22 30 29 30 39 18 23\n[16681] 23 36 31 20 37 31 38 28 35 32 23 32 23 31 24 23 30 31 30 20 25 22 32 26\n[16705] 35 39 23 25 18 33 21 19 22 38 40 21 36 25 22 24 35 30 23 38 20 27 19 25\n[16729] 34 35 23 20 21 40 33 40 37 37 27 23 33 33 29 22 33 39 25 22 37 33 28 28\n[16753] 28 32 23 37 23 40 27 25 36 36 23 32 36 37 34 18 19 25 23 25 22 40 36 25\n[16777] 38 27 36 32 31 23 40 22 40 34 24 29 40 23 38 40 23 23 38 38 20 27 40 36\n[16801] 26 22 24 39 36 39 19 36 35 29 21 40 39 30 38 35 36 31 30 29 29 20 30 22\n[16825] 20 40 26 28 33 29 40 36 30 34 40 20 27 32 38 19 37 25 22 39 30 40 38 32\n[16849] 28 31 20 25 32 24 25 35 38 21 40 39 32 40 39 20 38 40 18 21 26 27 19 36\n[16873] 40 32 28 20 23 36 37 25 30 22 25 21 39 40 39 38 29 26 29 26 28 38 24 33\n[16897] 40 37 23 29 22 20 33 39 29 31 25 25 31 29 23 38 39 30 18 37 38 38 35 39\n[16921] 25 40 18 35 26 37 22 28 35 31 19 29 23 25 36 18 38 30 20 38 26 27 31 26\n[16945] 19 35 33 30 37 34 19 19 38 35 24 21 35 35 31 29 39 29 18 31 36 37 26 19\n[16969] 24 33 19 22 32 19 22 31 32 24 39 31 32 21 34 21 18 22 39 38 18 19 25 34\n[16993] 37 24 31 20 34 34 39 27 25 24 35 22 23 24 39 37 23 40 23 38 37 31 22 18\n[17017] 26 21 21 35 19 27 20 34 21 28 20 30 34 31 31 37 34 33 29 20 39 30 26 24\n[17041] 29 30 28 31 35 34 40 32 24 20 28 22 21 29 33 20 30 24 31 34 26 38 25 32\n[17065] 27 34 28 39 27 18 20 34 33 26 26 31 40 38 30 34 35 38 32 31 37 38 36 30\n[17089] 35 25 18 18 20 19 36 22 18 21 20 26 31 20 40 21 30 26 33 20 25 19 19 31\n[17113] 24 28 18 20 25 21 39 26 25 22 40 19 33 23 36 37 24 24 21 38 29 38 25 40\n[17137] 23 27 20 19 21 22 31 32 31 27 35 36 36 34 18 36 22 36 24 29 20 19 30 23\n[17161] 26 39 37 34 30 23 37 26 20 38 20 27 35 40 36 32 38 31 39 37 27 37 19 24\n[17185] 31 38 19 35 33 30 24 26 39 29 22 31 31 21 34 32 25 27 23 39 34 37 25 18\n[17209] 33 28 24 33 31 26 28 24 19 24 37 37 20 37 18 36 25 21 31 18 22 33 28 39\n[17233] 29 29 34 18 35 23 35 22 35 33 36 32 28 21 38 32 19 20 24 40 18 26 39 32\n[17257] 28 36 37 20 33 26 27 30 37 38 26 34 22 28 18 30 28 30 27 39 35 29 21 31\n[17281] 28 26 24 38 18 21 32 36 37 36 33 33 20 22 38 24 38 22 20 22 27 31 37 20\n[17305] 29 37 34 23 35 19 22 22 31 27 40 32 21 30 22 38 26 32 33 26 30 35 24 39\n[17329] 40 23 24 20 18 33 35 35 37 24 22 25 25 32 18 21 20 25 28 39 24 30 37 27\n[17353] 31 20 32 36 18 32 28 29 29 30 20 31 25 21 36 37 19 32 25 34 28 20 38 36\n[17377] 21 21 22 20 35 23 23 34 40 36 33 22 18 25 34 25 32 21 38 36 35 34 19 31\n[17401] 40 38 20 36 30 32 33 36 24 38 39 18 39 29 33 38 34 37 40 28 25 26 37 18\n[17425] 29 38 29 39 33 29 29 34 31 30 32 27 37 18 37 19 35 34 31 31 20 22 36 20\n[17449] 29 33 36 36 32 32 25 37 23 38 28 25 22 25 30 19 34 23 38 31 20 19 40 35\n[17473] 19 40 38 18 39 29 26 21 26 21 40 30 28 33 20 29 37 35 20 18 34 19 25 37\n[17497] 31 18 27 20 36 32 33 36 22 34 35 24 18 23 35 38 28 37 25 32 36 23 31 21\n[17521] 38 19 35 38 36 27 19 21 34 23 35 26 19 22 19 40 28 30 40 26 31 35 28 31\n[17545] 26 36 32 35 23 40 25 27 30 30 24 38 31 32 39 28 38 26 37 25 36 29 26 37\n[17569] 31 28 36 35 35 20 34 35 28 27 34 25 22 34 33 34 20 39 34 26 28 20 38 21\n[17593] 28 33 24 38 32 30 29 19 30 29 20 36 27 39 38 34 36 21 26 40 37 26 34 24\n[17617] 23 36 22 34 34 20 23 36 21 25 33 32 24 29 31 32 38 24 32 35 36 33 20 36\n[17641] 22 19 21 35 29 21 22 23 30 32 19 21 40 37 24 23 40 25 40 20 22 36 28 34\n[17665] 20 29 36 24 37 26 34 29 19 37 40 33 30 35 31 19 21 22 29 29 22 34 25 35\n[17689] 29 32 29 34 40 32 24 24 20 24 34 30 39 24 29 19 37 33 19 37 28 32 29 32\n[17713] 31 34 23 29 24 26 30 33 29 33 36 28 39 33 23 26 18 18 19 38 28 40 31 18\n[17737] 34 25 29 25 24 39 28 23 36 37 35 39 19 37 38 40 39 40 18 19 32 34 36 33\n[17761] 20 35 26 34 29 37 28 39 21 39 36 38 20 31 38 38 29 24 25 39 40 21 23 28\n[17785] 32 26 30 23 24 30 33 33 27 20 37 38 28 19 38 27 28 18 19 27 19 18 24 38\n[17809] 29 30 19 25 19 26 40 31 26 22 38 37 25 36 21 20 38 22 25 19 31 25 36 25\n[17833] 29 36 21 24 40 31 18 20 37 40 19 21 26 34 40 18 24 30 19 25 29 20 40 19\n[17857] 22 24 29 38 39 25 30 19 20 39 40 38 18 40 27 34 40 19 21 20 27 28 21 39\n[17881] 28 24 27 31 31 21 30 34 30 34 23 32 27 38 40 28 28 27 23 23 21 37 33 26\n[17905] 27 28 35 21 28 22 27 25 24 32 39 36 32 24 40 25 27 32 23 36 29 24 34 40\n[17929] 22 25 19 29 23 20 38 35 40 31 18 30 19 32 21 20 34 36 34 33 32 20 39 20\n[17953] 20 26 20 19 31 34 24 27 21 39 28 35 34 21 26 39 26 18 35 29 18 24 39 23\n[17977] 31 28 20 40 28 36 28 35 27 24 34 23 35 18 35 21 32 23 39 24 21 34 37 32\n[18001] 28 38 27 37 19 20 33 28 29 26 31 18 22 38 37 38 19 25 27 33 37 28 28 36\n[18025] 21 22 25 32 27 34 19 31 22 28 18 28 22 22 20 37 26 20 18 27 20 24 24 37\n[18049] 38 23 19 25 23 37 28 33 23 21 36 32 40 22 20 27 20 32 23 40 27 40 27 30\n[18073] 28 34 18 21 30 30 19 36 35 34 18 37 28 31 29 33 20 23 24 26 37 31 31 26\n[18097] 19 19 25 27 37 25 38 20 37 32 18 31 19 18 33 31 19 30 20 26 38 28 23 27\n[18121] 37 32 19 18 34 39 33 40 28 35 22 25 21 24 22 25 18 37 38 31 29 36 28 18\n[18145] 25 37 31 25 25 35 38 35 35 36 33 20 20 30 22 27 21 35 29 36 26 33 28 38\n[18169] 23 27 32 25 31 31 22 29 37 24 33 33 27 38 29 29 28 28 33 35 32 24 33 33\n[18193] 27 29 37 33 35 22 29 34 31 25 27 22 38 25 22 36 18 22 35 31 38 25 31 32\n[18217] 28 23 18 37 28 28 39 18 21 28 23 19 27 34 38 25 27 22 29 40 34 38 35 20\n[18241] 25 40 28 32 20 18 38 18 35 25 18 22 27 31 31 32 22 21 24 31 31 29 24 21\n[18265] 33 28 32 36 35 19 34 34 36 30 30 34 20 40 21 35 30 37 27 30 34 32 18 18\n[18289] 37 38 30 29 30 40 22 22 30 30 25 30 27 23 22 35 21 31 27 25 39 19 21 35\n[18313] 20 24 26 22 28 20 20 26 18 28 28 18 40 31 18 30 33 33 28 34 22 40 34 32\n[18337] 39 21 30 31 39 37 33 28 19 22 27 20 37 34 31 37 28 36 34 36 30 38 27 30\n[18361] 24 31 36 21 18 19 25 24 34 37 38 32 19 30 37 34 26 21 40 30 29 35 36 30\n[18385] 24 27 38 21 40 24 31 40 20 36 30 25 20 25 20 38 23 35 35 25 23 32 34 40\n[18409] 35 20 34 34 22 32 39 18 22 29 18 40 24 24 27 37 31 33 19 32 34 26 24 26\n[18433] 38 36 37 23 19 22 36 31 35 34 31 37 27 39 30 38 25 29 38 22 25 35 18 27\n[18457] 20 20 37 28 23 36 32 33 27 18 26 38 26 25 38 19 30 23 34 26 28 22 18 31\n[18481] 18 39 21 32 37 27 29 30 21 30 19 20 28 18 23 29 34 34 21 36 20 29 32 24\n[18505] 31 37 40 40 28 39 34 40 23 34 39 24 30 39 24 38 32 33 27 35 33 40 22 19\n[18529] 33 18 31 33 25 33 26 40 25 19 35 37 32 35 30 22 34 23 40 29 31 28 38 33\n[18553] 39 29 33 33 39 20 26 31 40 38 22 22 28 27 39 26 25 23 40 24 27 37 23 28\n[18577] 30 19 23 27 30 18 40 38 19 21 30 22 25 37 26 29 34 27 26 20 40 37 35 18\n[18601] 36 28 27 34 21 30 33 26 31 21 23 31 20 33 20 39 26 35 19 26 22 40 31 27\n[18625] 24 26 29 32 37 27 35 32 23 33 33 18 29 19 30 40 32 25 19 18 31 27 38 18\n[18649] 30 29 35 31 24 31 18 21 30 35 32 22 29 25 29 27 38 19 33 29 30 23 32 22\n[18673] 27 21 37 37 40 40 33 28 29 19 22 23 22 19 22 19 23 36 34 32 30 21 27 20\n[18697] 40 33 31 31 26 33 36 20 34 38 39 24 32 28 35 23 26 20 24 37 30 29 24 32\n[18721] 22 29 23 25 35 25 39 26 33 39 36 21 29 27 35 24 22 26 30 30 28 23 33 39\n[18745] 20 35 18 28 39 22 22 20 27 20 32 32 26 38 36 25 32 27 21 23 31 28 40 24\n[18769] 24 31 34 21 38 24 30 33 39 32 22 37 32 23 39 36 27 31 40 29 35 37 32 39\n[18793] 32 21 24 27 26 31 25 27 38 28 22 40 34 28 25 23 25 37 20 39 33 33 22 23\n[18817] 24 32 37 38 38 30 18 23 35 18 35 37 20 36 30 19 25 24 36 19 39 30 39 39\n[18841] 38 27 19 24 21 18 29 22 19 34 21 19 19 31 20 36 26 19 28 37 21 36 37 38\n[18865] 37 32 40 36 27 27 33 34 22 40 34 19 39 34 30 21 34 21 20 34 31 35 24 38\n[18889] 29 32 38 24 20 35 36 29 27 25 36 28 23 35 30 32 19 22 30 18 27 21 30 30\n[18913] 30 29 22 28 34 33 37 36 36 32 19 38 35 27 20 35 38 39 26 29 20 36 30 26\n[18937] 40 38 38 29 40 35 23 24 34 33 36 18 25 25 30 20 35 36 31 22 20 22 27 21\n[18961] 39 22 24 29 31 18 29 21 34 31 32 29 38 35 24 40 31 34 30 29 27 25 40 22\n[18985] 33 30 26 37 20 36 21 20 40 19 21 38 20 38 27 24 40 19 18 38 23 25 35 27\n[19009] 20 33 23 39 34 21 20 39 33 24 39 30 37 22 25 38 36 37 18 35 38 35 26 31\n[19033] 39 23 31 30 18 26 19 32 20 22 29 38 36 37 29 25 28 36 29 22 18 32 28 31\n[19057] 37 27 40 37 28 30 21 20 23 39 33 31 40 35 33 38 34 23 18 38 38 40 29 32\n[19081] 29 37 36 32 24 19 21 34 34 32 28 28 37 29 20 20 29 36 40 21 25 21 37 18\n[19105] 31 27 33 19 28 18 18 20 25 33 37 20 23 18 18 20 23 32 18 25 25 20 24 40\n[19129] 25 27 36 33 30 33 38 30 23 19 33 29 40 38 22 22 30 28 35 37 39 18 30 38\n[19153] 37 36 33 38 32 19 31 30 18 38 30 40 36 34 39 40 36 33 35 20 40 23 20 23\n[19177] 32 32 39 38 36 33 18 38 34 33 26 18 31 23 37 25 20 19 21 23 37 26 22 33\n[19201] 20 39 33 39 18 21 40 19 34 38 18 34 35 18 23 20 40 37 24 24 31 26 18 36\n[19225] 29 39 37 36 34 38 37 30 23 22 24 35 18 26 39 22 35 30 22 23 28 35 18 23\n[19249] 35 26 18 37 35 26 19 23 28 37 39 33 18 34 21 26 33 32 28 34 23 37 34 35\n[19273] 18 32 26 30 23 40 34 36 22 26 31 27 31 33 20 33 22 33 27 39 38 29 36 21\n[19297] 22 37 21 26 36 39 31 28 36 40 34 37 24 32 22 29 30 39 26 27 28 19 37 26\n[19321] 37 32 21 21 26 39 22 27 31 18 27 19 27 22 37 34 40 22 38 38 30 32 25 35\n[19345] 28 30 29 25 36 36 40 20 19 18 19 22 36 22 38 38 34 28 22 39 27 34 40 32\n[19369] 24 31 22 37 22 36 33 19 38 34 27 39 36 40 35 38 20 28 38 24 18 39 24 33\n[19393] 31 29 27 26 39 21 37 34 22 40 34 21 32 23 29 39 23 34 40 37 25 22 23 32\n[19417] 27 35 39 38 34 25 33 39 35 21 18 37 40 20 32 39 22 38 33 39 31 39 28 22\n[19441] 24 39 31 28 33 30 22 26 39 32 35 29 37 34 24 28 37 26 40 18 36 35 35 31\n[19465] 33 24 37 28 38 34 18 33 25 31 38 24 22 37 19 19 26 23 37 26 28 29 35 34\n[19489] 18 20 24 32 21 20 36 34 35 34 19 36 19 20 32 20 29 23 26 32 37 19 33 20\n[19513] 33 32 25 18 25 36 32 32 19 30 29 40 32 19 24 28 32 31 28 26 29 38 32 21\n[19537] 35 26 32 35 33 20 32 34 29 31 35 31 34 29 24 31 36 40 20 31 38 28 39 18\n[19561] 29 29 29 18 19 30 36 18 29 36 33 34 32 20 34 36 40 39 27 31 36 30 28 37\n[19585] 36 19 40 34 24 30 21 38 23 27 25 32 26 23 24 32 33 19 30 36 40 29 21 27\n[19609] 29 37 24 36 30 28 21 26 28 19 26 39 30 18 21 32 18 37 25 34 21 34 19 31\n[19633] 35 29 29 37 22 39 29 25 33 27 30 33 19 28 30 26 19 27 31 28 22 20 28 27\n[19657] 33 34 35 30 18 37 27 25 33 27 32 25 27 35 26 20 22 19 39 21 28 23 35 40\n[19681] 33 18 25 23 29 31 25 22 32 37 18 32 19 38 36 20 24 26 30 29 26 32 25 23\n[19705] 36 18 21 37 24 22 27 29 26 40 30 29 39 28 33 38 32 36 30 37 28 21 35 35\n[19729] 25 40 20 27 20 30 31 33 33 19 19 37 22 27 22 36 29 40 32 21 28 35 25 24\n[19753] 33 35 31 21 35 22 37 40 37 18 32 26 23 40 21 19 34 20 19 40 34 21 24 36\n[19777] 28 40 19 27 19 31 39 30 28 25 22 37 22 33 38 30 35 35 21 20 25 32 24 20\n[19801] 21 22 20 32 23 25 36 35 28 37 36 27 18 33 32 37 28 38 36 37 32 18 19 33\n[19825] 23 26 24 35 37 37 34 31 40 20 40 25 24 19 40 19 32 40 27 34 30 23 34 25\n[19849] 20 33 19 18 32 30 35 25 21 21 32 37 26 22 26 30 25 27 19 19 31 19 21 31\n[19873] 28 39 36 38 33 35 38 26 19 20 27 30 26 27 31 30 26 26 40 26 34 19 37 35\n[19897] 36 28 34 30 39 31 25 40 20 38 29 23 38 33 21 39 27 23 18 39 27 22 19 29\n[19921] 19 26 27 29 18 25 27 35 21 19 27 18 30 37 27 37 24 35 28 38 24 19 23 35\n[19945] 31 27 27 39 27 39 32 25 20 32 28 27 29 37 28 31 39 22 26 22 29 30 34 31\n[19969] 20 27 23 35 24 29 21 27 40 31 20 34 33 32 31 31 32 30 38 26 25 29 35 26\n[19993] 36 26 27 26 21 31 33 21\n\n$data.name\n[1] \"dades[, variable]\"\n\n$bad.obs\n[1] 0\n\n$all.stats\n  i  Mean.i     SD.i Value Obs.Num    R.i+1 lambda.i+1 Outlier\n1 0 29.0338 6.622458    18      73 1.666119   4.706884   FALSE\n\nattr(,\"class\")\n[1] \"gofOutlier\"\n\ntest$all.stats\n\n  i  Mean.i     SD.i Value Obs.Num    R.i+1 lambda.i+1 Outlier\n1 0 29.0338 6.622458    18      73 1.666119   4.706884   FALSE"
  },
  {
    "objectID": "material/Preprocessing/AdvancedPreprocessing.html#multivariate",
    "href": "material/Preprocessing/AdvancedPreprocessing.html#multivariate",
    "title": "Advance Preprocessing",
    "section": "3.2 Multivariate",
    "text": "3.2 Multivariate\nHasta ahora hemos visto cada variable por separado. Sin embargo, a veces los outliers aparecen en combinación de variables.\nPara detectarlos se pueden usar métodos como la distancia de Mahalanobis o algoritmos más avanzados de detección de anomalías.\n\nlibrary(scatterplot3d)\nlibrary(readr)\ndades &lt;- readr::read_csv(\"https://archive.ics.uci.edu/ml/machine-learning-databases/forest-fires/forestfires.csv\")\ndades &lt;- data.frame(dades[, c(\"DC\", \"temp\", \"RH\")])\nscatterplot3d(dades[,\"DC\"], dades[, \"temp\"],dades[, \"RH\"])\n\n\n\n\n\n\n\n\n\nlibrary(rgl)\n\n# Plot\nrgl::plot3d(x = dades[, \"DC\"], y = dades[, \"temp\"], z = dades[, \"RH\"], \ncol = \"black\", type = 'p', radius = .1)\n\n\nlibrary(plotly)\n\n(fig &lt;- plotly::plot_ly(dades, x = ~DC, y = ~temp, z = ~RH, size = 1) %&gt;% \n       add_markers())\n\n\n\n\n\n\n3.2.1 Cas general\n\nlibrary(mvoutlier)\ndades2 &lt;- dades; Y &lt;- as.matrix(dades2)\ndistances &lt;- dd.plot(Y,quan=1/2, alpha=0.025)\n\n\n\n\n\n\n\nhead(distances$md.cla)\n\n[1] 2.151205 1.337982 2.117212 3.824607 4.157557 1.021413\n\nhead(distances$md.rob)\n\n[1] 8.223442 1.637652 2.604928 8.696220 8.422537 2.417220\n\nres &lt;- aq.plot(Y,delta=qchisq(0.975,df=ncol(Y)),quan=1/2,alpha=0.05)\n\nProjection to the first and second robust principal components.\nProportion of total variation (explained variance): 0.9693951\n\n\n\n\n\n\n\n\nstr(res)\n\nList of 1\n $ outliers: logi [1:517] TRUE FALSE FALSE TRUE TRUE FALSE ...\n\nhead(res$outliers)\n\n[1]  TRUE FALSE FALSE  TRUE  TRUE FALSE\n\ntable(res$outliers)\n\n\nFALSE  TRUE \n  388   129 \n\n#windows()\npar(mfrow=c(1, 1))\nlibrary(MVN)\n# mvnoutliers &lt;- mvn(dades, multivariateOutlierMethod = \"adj\", showOutliers = TRUE, \n#                   showNewData = TRUE)\nmvnoutliers &lt;- mvn(data = dades, mvn_test = \"royston\", \n                   univariate_test = \"AD\", \n              multivariate_outlier_method = \"adj\",\n              show_new_data = TRUE)\n\nVisualitzem tots els outliers detectats com a true\n\nhead(summary(mvnoutliers, select = \"outliers\"))\n\n    Observation Mahalanobis.Distance\n1           466               95.620\n2           464               92.910\n3           105               92.347\n4           284               91.668\n5           465               91.312\n6           285               90.872\n7           448               89.349\n8            76               85.582\n9           177               85.110\n10          197               85.110\n11          395               84.820\n12          166               82.912\n13          379               82.172\n14          412               80.371\n15          418               80.180\n16          394               79.742\n17          203               79.121\n18            4               79.053\n19          467               78.658\n20          240               78.246\n21          469               76.910\n22          391               76.746\n23          468               76.551\n24          470               76.236\n25          119               75.377\n26          388               75.344\n27           62               75.271\n28            5               73.796\n29          117               73.244\n30           98               73.243\n31           49               72.952\n32          443               72.902\n33           97               72.889\n34          396               71.492\n35           59               71.469\n36            1               71.192\n37           60               70.684\n38           20               70.109\n39          471               69.553\n40          183               69.161\n41          305               68.417\n42          191               68.156\n43          127               67.840\n44          277               67.386\n45          278               67.386\n46          279               67.386\n47          280               67.386\n48          517               67.373\n49          380               67.118\n50          106               66.945\n51          111               66.582\n52          116               66.430\n53           78               66.389\n54          115               65.699\n55          132               65.676\n56          411               65.587\n57           73               65.387\n58           77               65.351\n59          408               65.339\n60          241               65.243\n61           61               65.055\n62          187               65.055\n63           17               65.028\n64          133               64.804\n65          118               64.776\n66          242               63.968\n67          169               63.822\n68           40               63.798\n69          220               63.737\n70           50               63.488\n71          214               63.488\n72          223               63.488\n73          135               63.471\n74          147               63.165\n75          202               62.350\n76          190               62.077\n77          282               62.062\n78          189               61.798\n79           19               61.726\n80          112               61.540\n81          446               61.359\n82           70               61.172\n83           92               61.051\n84          161               60.679\n85           90               60.596\n86          215               60.585\n87          216               60.585\n88           71               60.301\n89          163               59.452\n90          107               58.594\n91          131               58.351\n92          472               56.439\n93          205               55.917\n94          281               47.755\n95          300               47.138\n96          283               44.684\n97          473               40.862\n98          274               40.080\n99          276               38.049\n100         275               37.750\n101          23               33.308\n102         224               30.706\n103         303               30.572\n104         304               30.572\n105         297               28.501\n106         474               27.684\n107         298               26.969\n108         144               24.440\n109         301               23.821\n110         444               23.544\n111         287               23.359\n112         302               22.481\n113         479               21.975\n114         139               21.533\n115         400               20.893\n116         401               20.893\n117         212               19.743\n118         475               18.348\n119         476               15.110\n120         436               14.908\n121         152               14.508\n122         299               13.807\n123         500               13.600\n124          41               13.455\n125           8               13.137\n126         295               12.864\n127          99               12.083\n\n\n$multivariate_normality\n     Test Statistic p.value     Method          MVN\n1 Royston   162.918  &lt;0.001 asymptotic ✗ Not normal\n\n$univariate_normality\n              Test Variable Statistic p.value    Normality\n1 Anderson-Darling       DC    43.098  &lt;0.001 ✗ Not normal\n2 Anderson-Darling     temp     1.812  &lt;0.001 ✗ Not normal\n3 Anderson-Darling       RH     7.871  &lt;0.001 ✗ Not normal\n\n$descriptives\n  Variable   n    Mean Std.Dev Median  Min   Max  25th  75th   Skew Kurtosis\n1       DC 517 547.940 248.066  664.2  7.9 860.6 437.7 713.9 -1.097    2.746\n2     temp 517  18.889   5.807   19.3  2.2  33.3  15.5  22.8 -0.330    3.123\n3       RH 517  44.288  16.317   42.0 15.0 100.0  33.0  53.0  0.860    3.422\n\n$multivariate_outliers\n    Observation Mahalanobis.Distance\n1           466               95.620\n2           464               92.910\n3           105               92.347\n4           284               91.668\n5           465               91.312\n6           285               90.872\n7           448               89.349\n8            76               85.582\n9           177               85.110\n10          197               85.110\n11          395               84.820\n12          166               82.912\n13          379               82.172\n14          412               80.371\n15          418               80.180\n16          394               79.742\n17          203               79.121\n18            4               79.053\n19          467               78.658\n20          240               78.246\n21          469               76.910\n22          391               76.746\n23          468               76.551\n24          470               76.236\n25          119               75.377\n26          388               75.344\n27           62               75.271\n28            5               73.796\n29          117               73.244\n30           98               73.243\n31           49               72.952\n32          443               72.902\n33           97               72.889\n34          396               71.492\n35           59               71.469\n36            1               71.192\n37           60               70.684\n38           20               70.109\n39          471               69.553\n40          183               69.161\n41          305               68.417\n42          191               68.156\n43          127               67.840\n44          277               67.386\n45          278               67.386\n46          279               67.386\n47          280               67.386\n48          517               67.373\n49          380               67.118\n50          106               66.945\n51          111               66.582\n52          116               66.430\n53           78               66.389\n54          115               65.699\n55          132               65.676\n56          411               65.587\n57           73               65.387\n58           77               65.351\n59          408               65.339\n60          241               65.243\n61           61               65.055\n62          187               65.055\n63           17               65.028\n64          133               64.804\n65          118               64.776\n66          242               63.968\n67          169               63.822\n68           40               63.798\n69          220               63.737\n70           50               63.488\n71          214               63.488\n72          223               63.488\n73          135               63.471\n74          147               63.165\n75          202               62.350\n76          190               62.077\n77          282               62.062\n78          189               61.798\n79           19               61.726\n80          112               61.540\n81          446               61.359\n82           70               61.172\n83           92               61.051\n84          161               60.679\n85           90               60.596\n86          215               60.585\n87          216               60.585\n88           71               60.301\n89          163               59.452\n90          107               58.594\n91          131               58.351\n92          472               56.439\n93          205               55.917\n94          281               47.755\n95          300               47.138\n96          283               44.684\n97          473               40.862\n98          274               40.080\n99          276               38.049\n100         275               37.750\n101          23               33.308\n102         224               30.706\n103         303               30.572\n104         304               30.572\n105         297               28.501\n106         474               27.684\n107         298               26.969\n108         144               24.440\n109         301               23.821\n110         444               23.544\n111         287               23.359\n112         302               22.481\n113         479               21.975\n114         139               21.533\n115         400               20.893\n116         401               20.893\n117         212               19.743\n118         475               18.348\n119         476               15.110\n120         436               14.908\n121         152               14.508\n122         299               13.807\n123         500               13.600\n124          41               13.455\n125           8               13.137\n126         295               12.864\n127          99               12.083\n\n$new_data\n       DC temp RH\n2   669.1 18.0 33\n3   686.9 14.6 33\n6   488.0 22.2 29\n7   495.6 24.1 27\n9   692.6 13.1 63\n10  698.6 22.8 40\n11  698.6 17.8 51\n12  713.0 19.3 38\n13  665.3 17.0 72\n14  686.5 21.3 42\n15  699.6 26.4 21\n16  713.9 22.9 44\n18  664.2 16.7 47\n21  692.6 18.3 40\n22  724.3 19.1 38\n24  537.4 19.5 43\n25  594.2 23.7 32\n26  601.4 16.3 60\n27  668.0 19.0 34\n28  686.5 19.4 48\n29  721.4 30.2 24\n30  728.6 22.8 39\n31  692.3 25.4 24\n32  709.9 11.2 78\n33  706.8 20.6 37\n34  718.3 17.7 39\n35  724.3 21.2 32\n36  730.2 18.2 62\n37  669.1 21.7 24\n38  682.6 11.3 60\n39  686.9 17.8 27\n42  624.2 18.4 42\n43  647.1 16.6 54\n44  698.6 19.6 48\n45  735.7 12.9 74\n46  692.3 25.9 24\n47  686.5 14.7 70\n48  442.9 23.0 36\n51  706.4 20.8 17\n52  631.2 21.5 34\n53  654.1 20.4 42\n54  654.1 20.4 42\n55  661.3 17.6 45\n56  706.4 27.7 24\n57  730.2 17.8 63\n58  691.8 13.8 50\n63  466.6 18.8 35\n64  631.2 20.8 33\n65  638.8 23.1 31\n66  661.3 18.6 44\n67  668.0 23.0 37\n68  668.0 19.6 33\n69  668.0 19.6 33\n72  692.3 17.7 37\n74  614.5 17.3 43\n75  713.9 27.6 30\n79  529.8 14.7 66\n80  561.6 21.6 19\n81  601.4 19.5 39\n82  631.2 17.9 44\n83  647.1 18.6 51\n84  654.1 16.6 47\n85  661.3 20.2 45\n86  706.4 21.5 15\n87  706.4 25.4 27\n88  706.4 22.4 34\n89  728.6 25.3 36\n91  624.2 14.7 59\n93  488.0 20.8 32\n94  601.4 18.2 43\n95  638.8 23.4 22\n96  704.4 17.8 64\n100 601.4 19.8 39\n101 601.4 19.8 39\n102 614.5 14.4 66\n103 647.1 20.1 40\n104 674.4 24.1 29\n108 631.2 21.4 33\n109 698.6 20.3 45\n110 709.9 17.4 56\n113 704.4 22.8 39\n114 724.3 18.9 35\n120 466.6 19.6 36\n121 608.2 10.3 74\n122 608.2 17.1 43\n123 680.7 22.5 42\n124 671.9 17.9 45\n125 692.3 19.8 50\n126 691.8 20.6 24\n128 728.6 17.2 43\n129 673.8 15.9 46\n130 691.8 15.4 35\n134 685.2 17.6 42\n136 594.2 17.6 52\n137 680.7 17.2 58\n138 686.5 15.6 66\n140 692.6 21.7 38\n141 686.5 21.9 39\n142 513.3 23.3 31\n143 529.8 21.2 51\n145 513.3 23.8 32\n146 578.8 27.4 22\n148 671.9 24.2 28\n149 647.1 17.4 43\n150 685.2 23.7 25\n151 433.3 23.2 39\n153 424.1 24.6 43\n154 692.3 20.1 47\n155 721.4 29.6 27\n156 647.1 16.4 47\n157 721.4 28.6 27\n158 654.1 18.4 45\n159 654.1 20.5 35\n160 668.0 19.0 34\n162 578.8 20.3 41\n164 674.4 17.8 56\n165 704.4 17.8 67\n167 654.1 16.6 47\n168 570.5 23.4 33\n170 578.8 20.7 45\n171 699.6 21.9 35\n172 609.6 17.4 50\n173 601.4 20.1 39\n174 686.5 17.7 39\n175 624.2 14.2 53\n176 624.2 20.3 39\n178 631.2 19.2 44\n179 735.7 18.3 45\n180 614.5 14.4 66\n181 680.7 23.9 32\n182 664.2 19.1 32\n184 696.1 16.8 45\n185 586.7 20.8 34\n186 692.6 17.6 46\n188 686.5 21.0 42\n192 578.8 24.2 28\n193 647.1 24.6 22\n194 699.6 24.3 25\n195 647.1 24.6 22\n196 586.7 23.5 36\n198 706.4 21.5 15\n199 692.6 13.9 59\n200 665.3 22.6 38\n201 692.6 21.6 33\n204 673.8 20.2 37\n206 706.4 22.1 34\n207 594.2 22.9 31\n208 692.6 20.7 37\n209 668.0 19.6 33\n210 685.2 23.2 26\n211 686.9 18.4 25\n213 692.3 20.1 47\n217 680.7 16.9 60\n218 709.9 12.4 73\n219 699.6 19.4 19\n221 631.2 16.2 59\n222 713.9 18.6 49\n225 735.7 15.4 57\n226 728.6 22.9 39\n227 696.1 16.1 44\n228 480.8 20.1 34\n229 728.6 28.3 26\n230 480.8 16.4 43\n231 699.6 26.4 21\n232 728.6 27.8 27\n233 692.6 18.7 43\n234 671.9 24.3 36\n235 674.4 17.7 25\n236 601.4 19.6 41\n237 674.4 18.2 46\n238 692.6 18.8 40\n239 674.4 25.1 27\n243 589.9 15.4 66\n244 700.7 21.9 73\n245 700.7 22.4 54\n246 700.7 26.8 38\n247 700.7 25.7 39\n248 503.6 20.7 70\n249 666.7 28.7 28\n250 666.7 21.7 40\n251 666.7 26.8 25\n252 666.7 24.0 36\n253 666.7 22.1 37\n254 565.5 21.4 38\n255 621.7 18.9 41\n256 694.8 22.3 46\n257 581.1 23.9 41\n258 581.1 21.4 44\n259 692.3 20.6 59\n260 692.3 23.7 40\n261 542.0 28.3 32\n262 573.0 11.2 84\n263 573.0 21.4 42\n264 629.1 19.3 39\n265 684.4 21.8 53\n266 550.3 22.1 54\n267 607.1 19.4 55\n268 658.2 23.7 24\n269 658.2 21.0 32\n270 658.2 19.1 53\n271 658.2 21.8 56\n272 658.2 20.1 58\n273 658.2 20.2 47\n286 411.8 23.4 40\n288 474.9 22.1 49\n289 474.9 24.2 32\n290 474.9 24.3 30\n291 474.9 18.7 53\n292 474.9 25.3 39\n293 466.3 22.9 40\n294 430.8 26.9 28\n296 430.8 22.2 48\n306 714.3 19.0 52\n307 714.3 17.1 53\n308 714.3 23.8 35\n309 758.1 16.0 45\n310 758.1 24.9 27\n311 758.1 25.3 27\n312 758.1 24.8 28\n313 706.6 12.2 78\n314 777.1 24.3 27\n315 777.1 19.7 41\n316 817.5 18.5 30\n317 739.4 18.6 24\n318 739.4 19.2 24\n319 783.5 21.6 27\n320 783.5 21.6 28\n321 783.5 18.9 34\n322 783.5 16.8 28\n323 783.5 16.8 28\n324 822.8 12.9 39\n325 726.9 13.7 56\n326 751.5 24.2 27\n327 751.5 24.1 27\n328 751.5 21.2 32\n329 751.5 19.7 35\n330 751.5 23.5 27\n331 751.5 24.2 27\n332 795.3 21.5 28\n333 795.3 17.1 41\n334 721.1 18.1 54\n335 764.0 18.0 51\n336 764.0  9.8 86\n337 764.0 19.3 44\n338 764.0 23.0 34\n339 764.0 22.7 35\n340 764.0 20.4 41\n341 764.0 19.3 44\n342 770.3 15.7 51\n343 807.1 20.6 37\n344 807.1 15.9 51\n345 807.1 12.2 66\n346 807.1 16.8 43\n347 807.1 21.3 35\n348 745.3 10.1 75\n349 745.3 17.4 57\n350 745.3 12.8 64\n351 745.3 10.1 75\n352 745.3 15.4 53\n353 745.3 20.6 43\n354 745.3 19.8 47\n355 745.3 18.7 50\n356 745.3 20.8 35\n357 745.3 20.8 35\n358 789.7 15.9 55\n359 789.7 19.7 39\n360 789.7 21.1 39\n361 789.7 18.4 42\n362 789.7 17.3 45\n363 732.3 15.2 64\n364 770.3 15.9 53\n365 770.3 21.1 35\n366 770.3 19.6 45\n367 812.1 15.9 38\n368 812.1 16.4 27\n369 744.4 16.8 47\n370 825.1 13.8 77\n371 825.1 13.8 77\n372 520.5 14.2 58\n373 664.5 10.4 75\n374 698.6 20.3 42\n375 855.3 10.3 78\n376 744.4 15.4 57\n377 672.6 21.1 54\n378 715.1 21.9 42\n381 458.8 19.3 39\n382 643.0 16.2 63\n383 690.0 28.2 29\n384 753.8 20.5 58\n385 819.1 21.3 44\n386 613.0 20.9 50\n387 750.5 20.6 55\n389 706.7 23.3 34\n390 706.7 23.3 34\n392 738.1 20.7 46\n393 825.1 21.9 43\n397 750.5 20.4 55\n398 613.0 24.3 33\n399 715.1 25.9 32\n402 731.7 22.8 46\n403 706.7 25.0 36\n404 643.0 21.3 41\n405 725.1 21.8 34\n406 680.9 27.9 27\n407 860.6 17.0 67\n409 855.3 19.9 44\n410 450.2 23.4 31\n413 442.1 22.8 27\n414 715.1 26.4 33\n415 723.1 24.1 50\n416 698.6 27.5 27\n417 575.8 26.3 39\n419 664.5 24.9 42\n420 613.0 24.8 36\n421 635.9 26.2 36\n422 690.0 30.8 19\n423 795.9 29.3 27\n424 744.4 22.3 48\n425 715.1 26.9 31\n426 753.8 20.4 56\n427 753.8 20.4 56\n428 672.6 27.9 33\n429 698.6 26.2 34\n430 613.0 24.6 44\n431 849.3 19.4 45\n432 605.3 23.3 40\n433 698.6 23.9 38\n434 723.1 20.9 66\n435 811.2 22.2 45\n437 672.6 26.8 35\n438 768.4 14.2 73\n439 715.1 23.6 53\n440 738.1 19.1 46\n441 855.3 16.2 58\n442 672.6 25.5 29\n445 855.3 16.2 58\n447 664.5 19.1 70\n449 844.0 10.5 77\n450 613.0 19.3 61\n451 690.0 23.4 49\n452 649.9 11.8 88\n453 730.6 17.7 65\n454 803.3 17.4 54\n455 753.8 16.8 56\n456 567.2 17.9 48\n457 753.8 16.6 59\n458 635.9 19.9 50\n459 715.1 18.9 64\n460 819.1 15.5 72\n461 715.1 18.9 64\n462 715.1 18.9 64\n463 825.1 14.5 76\n477 395.0 27.2 28\n478 423.4 26.1 45\n480 431.6 22.6 57\n481 560.0 30.2 25\n482 560.0 30.2 22\n483 587.1 23.4 40\n484 587.1 31.0 27\n485 587.1 33.1 25\n486 596.3 30.6 28\n487 605.8 24.1 43\n488 605.8 26.4 34\n489 605.8 19.4 71\n490 605.8 20.6 58\n491 605.8 28.7 33\n492 624.1 32.4 21\n493 633.6 32.4 27\n494 633.6 27.5 29\n495 643.0 30.8 30\n496 661.8 23.9 42\n497 661.8 32.6 26\n498 671.2 32.3 27\n499 671.2 33.3 26\n501 671.2 21.6 65\n502 671.2 21.6 65\n503 671.2 20.7 69\n504 689.1 29.2 30\n505 689.1 28.9 29\n506 744.4 26.7 35\n507 752.6 18.5 73\n508 752.6 25.9 41\n509 752.6 25.9 41\n510 752.6 21.1 71\n511 752.6 18.2 62\n512 665.6 27.8 35\n513 665.6 27.8 32\n514 665.6 21.9 71\n515 665.6 21.2 70\n516 614.7 25.6 42\n\n$data\n       DC temp  RH\n1    94.3  8.2  51\n2   669.1 18.0  33\n3   686.9 14.6  33\n4    77.5  8.3  97\n5   102.2 11.4  99\n6   488.0 22.2  29\n7   495.6 24.1  27\n8   608.2  8.0  86\n9   692.6 13.1  63\n10  698.6 22.8  40\n11  698.6 17.8  51\n12  713.0 19.3  38\n13  665.3 17.0  72\n14  686.5 21.3  42\n15  699.6 26.4  21\n16  713.9 22.9  44\n17   80.8 15.1  27\n18  664.2 16.7  47\n19   70.8 15.9  35\n20   97.1  9.3  44\n21  692.6 18.3  40\n22  724.3 19.1  38\n23  200.0 21.0  44\n24  537.4 19.5  43\n25  594.2 23.7  32\n26  601.4 16.3  60\n27  668.0 19.0  34\n28  686.5 19.4  48\n29  721.4 30.2  24\n30  728.6 22.8  39\n31  692.3 25.4  24\n32  709.9 11.2  78\n33  706.8 20.6  37\n34  718.3 17.7  39\n35  724.3 21.2  32\n36  730.2 18.2  62\n37  669.1 21.7  24\n38  682.6 11.3  60\n39  686.9 17.8  27\n40   67.6 14.1  43\n41  366.7 23.3  37\n42  624.2 18.4  42\n43  647.1 16.6  54\n44  698.6 19.6  48\n45  735.7 12.9  74\n46  692.3 25.9  24\n47  686.5 14.7  70\n48  442.9 23.0  36\n49   64.7 11.8  35\n50  103.8 11.0  46\n51  706.4 20.8  17\n52  631.2 21.5  34\n53  654.1 20.4  42\n54  654.1 20.4  42\n55  661.3 17.6  45\n56  706.4 27.7  24\n57  730.2 17.8  63\n58  691.8 13.8  50\n59   34.0 13.9  40\n60   43.0 12.3  51\n61  102.2 11.5  39\n62  102.2  5.5  59\n63  466.6 18.8  35\n64  631.2 20.8  33\n65  638.8 23.1  31\n66  661.3 18.6  44\n67  668.0 23.0  37\n68  668.0 19.6  33\n69  668.0 19.6  33\n70   77.5 17.2  26\n71   97.8 15.8  27\n72  692.3 17.7  37\n73   77.5 15.6  25\n74  614.5 17.3  43\n75  713.9 27.6  30\n76   26.6  6.7  79\n77   43.0 15.7  43\n78  103.8  8.3  72\n79  529.8 14.7  66\n80  561.6 21.6  19\n81  601.4 19.5  39\n82  631.2 17.9  44\n83  647.1 18.6  51\n84  654.1 16.6  47\n85  661.3 20.2  45\n86  706.4 21.5  15\n87  706.4 25.4  27\n88  706.4 22.4  34\n89  728.6 25.3  36\n90   80.8 17.4  25\n91  624.2 14.7  59\n92   80.8 17.4  24\n93  488.0 20.8  32\n94  601.4 18.2  43\n95  638.8 23.4  22\n96  704.4 17.8  64\n97   30.2 12.7  48\n98   15.5 17.4  24\n99  601.4 11.6  87\n100 601.4 19.8  39\n101 601.4 19.8  39\n102 614.5 14.4  66\n103 647.1 20.1  40\n104 674.4 24.1  29\n105   9.3  5.3  78\n106  57.3 12.7  52\n107  74.3 18.2  29\n108 631.2 21.4  33\n109 698.6 20.3  45\n110 709.9 17.4  56\n111  57.3 13.7  43\n112  77.5 18.8  18\n113 704.4 22.8  39\n114 724.3 18.9  35\n115  67.6 15.8  27\n116  67.6 15.5  27\n117  80.8 11.6  30\n118  80.8 15.2  27\n119  86.6 10.6  30\n120 466.6 19.6  36\n121 608.2 10.3  74\n122 608.2 17.1  43\n123 680.7 22.5  42\n124 671.9 17.9  45\n125 692.3 19.8  50\n126 691.8 20.6  24\n127 103.8  9.0  49\n128 728.6 17.2  43\n129 673.8 15.9  46\n130 691.8 15.4  35\n131  87.2 15.4  40\n132  64.7 14.0  39\n133 102.2 10.6  46\n134 685.2 17.6  42\n135  67.6 14.9  38\n136 594.2 17.6  52\n137 680.7 17.2  58\n138 686.5 15.6  66\n139 313.4 18.0  42\n140 692.6 21.7  38\n141 686.5 21.9  39\n142 513.3 23.3  31\n143 529.8 21.2  51\n144 296.3 16.6  53\n145 513.3 23.8  32\n146 578.8 27.4  22\n147  86.6 13.2  40\n148 671.9 24.2  28\n149 647.1 17.4  43\n150 685.2 23.7  25\n151 433.3 23.2  39\n152 355.2 24.8  29\n153 424.1 24.6  43\n154 692.3 20.1  47\n155 721.4 29.6  27\n156 647.1 16.4  47\n157 721.4 28.6  27\n158 654.1 18.4  45\n159 654.1 20.5  35\n160 668.0 19.0  34\n161  86.6 16.1  29\n162 578.8 20.3  41\n163 100.4 15.2  31\n164 674.4 17.8  56\n165 704.4 17.8  67\n166  55.0  5.3  70\n167 654.1 16.6  47\n168 570.5 23.4  33\n169  97.8 14.6  26\n170 578.8 20.7  45\n171 699.6 21.9  35\n172 609.6 17.4  50\n173 601.4 20.1  39\n174 686.5 17.7  39\n175 624.2 14.2  53\n176 624.2 20.3  39\n177  55.2  5.8  54\n178 631.2 19.2  44\n179 735.7 18.3  45\n180 614.5 14.4  66\n181 680.7 23.9  32\n182 664.2 19.1  32\n183  48.3 12.4  53\n184 696.1 16.8  45\n185 586.7 20.8  34\n186 692.6 17.6  46\n187 102.2 11.5  39\n188 686.5 21.0  42\n189  89.4 13.3  42\n190  92.4 11.5  60\n191  97.8 11.7  33\n192 578.8 24.2  28\n193 647.1 24.6  22\n194 699.6 24.3  25\n195 647.1 24.6  22\n196 586.7 23.5  36\n197  55.2  5.8  54\n198 706.4 21.5  15\n199 692.6 13.9  59\n200 665.3 22.6  38\n201 692.6 21.6  33\n202  83.7 12.4  54\n203  32.1  8.8  68\n204 673.8 20.2  37\n205 100.4 15.1  64\n206 706.4 22.1  34\n207 594.2 22.9  31\n208 692.6 20.7  37\n209 668.0 19.6  33\n210 685.2 23.2  26\n211 686.9 18.4  25\n212 594.2  5.1  96\n213 692.3 20.1  47\n214 103.8 11.0  46\n215  80.8 17.0  27\n216  80.8 17.0  27\n217 680.7 16.9  60\n218 709.9 12.4  73\n219 699.6 19.4  19\n220  86.6 15.2  27\n221 631.2 16.2  59\n222 713.9 18.6  49\n223 103.8 11.0  46\n224 309.9 13.4  79\n225 735.7 15.4  57\n226 728.6 22.9  39\n227 696.1 16.1  44\n228 480.8 20.1  34\n229 728.6 28.3  26\n230 480.8 16.4  43\n231 699.6 26.4  21\n232 728.6 27.8  27\n233 692.6 18.7  43\n234 671.9 24.3  36\n235 674.4 17.7  25\n236 601.4 19.6  41\n237 674.4 18.2  46\n238 692.6 18.8  40\n239 674.4 25.1  27\n240   7.9 13.4  75\n241  43.5 15.2  51\n242  85.3 16.7  20\n243 589.9 15.4  66\n244 700.7 21.9  73\n245 700.7 22.4  54\n246 700.7 26.8  38\n247 700.7 25.7  39\n248 503.6 20.7  70\n249 666.7 28.7  28\n250 666.7 21.7  40\n251 666.7 26.8  25\n252 666.7 24.0  36\n253 666.7 22.1  37\n254 565.5 21.4  38\n255 621.7 18.9  41\n256 694.8 22.3  46\n257 581.1 23.9  41\n258 581.1 21.4  44\n259 692.3 20.6  59\n260 692.3 23.7  40\n261 542.0 28.3  32\n262 573.0 11.2  84\n263 573.0 21.4  42\n264 629.1 19.3  39\n265 684.4 21.8  53\n266 550.3 22.1  54\n267 607.1 19.4  55\n268 658.2 23.7  24\n269 658.2 21.0  32\n270 658.2 19.1  53\n271 658.2 21.8  56\n272 658.2 20.1  58\n273 658.2 20.2  47\n274 353.5  4.8  57\n275 354.6  5.1  61\n276 352.0  5.1  61\n277 349.7  4.6  21\n278 349.7  4.6  21\n279 349.7  4.6  21\n280 349.7  4.6  21\n281 352.6  2.2  59\n282 349.7  5.1  24\n283 353.5  4.2  51\n284  18.7  8.8  35\n285  15.8  7.5  46\n286 411.8 23.4  40\n287 437.7 12.6  90\n288 474.9 22.1  49\n289 474.9 24.2  32\n290 474.9 24.3  30\n291 474.9 18.7  53\n292 474.9 25.3  39\n293 466.3 22.9  40\n294 430.8 26.9  28\n295 440.9 17.1  67\n296 430.8 22.2  48\n297 290.8 14.3  46\n298 290.8 15.4  45\n299 377.2 19.6  43\n300 233.8 10.6  90\n301 298.1 20.7  25\n302 298.1 19.1  39\n303 232.1 19.2  38\n304 232.1 19.2  38\n305 113.8 11.3  94\n306 714.3 19.0  52\n307 714.3 17.1  53\n308 714.3 23.8  35\n309 758.1 16.0  45\n310 758.1 24.9  27\n311 758.1 25.3  27\n312 758.1 24.8  28\n313 706.6 12.2  78\n314 777.1 24.3  27\n315 777.1 19.7  41\n316 817.5 18.5  30\n317 739.4 18.6  24\n318 739.4 19.2  24\n319 783.5 21.6  27\n320 783.5 21.6  28\n321 783.5 18.9  34\n322 783.5 16.8  28\n323 783.5 16.8  28\n324 822.8 12.9  39\n325 726.9 13.7  56\n326 751.5 24.2  27\n327 751.5 24.1  27\n328 751.5 21.2  32\n329 751.5 19.7  35\n330 751.5 23.5  27\n331 751.5 24.2  27\n332 795.3 21.5  28\n333 795.3 17.1  41\n334 721.1 18.1  54\n335 764.0 18.0  51\n336 764.0  9.8  86\n337 764.0 19.3  44\n338 764.0 23.0  34\n339 764.0 22.7  35\n340 764.0 20.4  41\n341 764.0 19.3  44\n342 770.3 15.7  51\n343 807.1 20.6  37\n344 807.1 15.9  51\n345 807.1 12.2  66\n346 807.1 16.8  43\n347 807.1 21.3  35\n348 745.3 10.1  75\n349 745.3 17.4  57\n350 745.3 12.8  64\n351 745.3 10.1  75\n352 745.3 15.4  53\n353 745.3 20.6  43\n354 745.3 19.8  47\n355 745.3 18.7  50\n356 745.3 20.8  35\n357 745.3 20.8  35\n358 789.7 15.9  55\n359 789.7 19.7  39\n360 789.7 21.1  39\n361 789.7 18.4  42\n362 789.7 17.3  45\n363 732.3 15.2  64\n364 770.3 15.9  53\n365 770.3 21.1  35\n366 770.3 19.6  45\n367 812.1 15.9  38\n368 812.1 16.4  27\n369 744.4 16.8  47\n370 825.1 13.8  77\n371 825.1 13.8  77\n372 520.5 14.2  58\n373 664.5 10.4  75\n374 698.6 20.3  42\n375 855.3 10.3  78\n376 744.4 15.4  57\n377 672.6 21.1  54\n378 715.1 21.9  42\n379  30.6  8.7  51\n380 171.4  5.2 100\n381 458.8 19.3  39\n382 643.0 16.2  63\n383 690.0 28.2  29\n384 753.8 20.5  58\n385 819.1 21.3  44\n386 613.0 20.9  50\n387 750.5 20.6  55\n388  30.6 11.6  48\n389 706.7 23.3  34\n390 706.7 23.3  34\n391  58.3  7.5  71\n392 738.1 20.7  46\n393 825.1 21.9  43\n394  25.6 15.2  19\n395  46.7  5.3  68\n396  56.9 10.1  62\n397 750.5 20.4  55\n398 613.0 24.3  33\n399 715.1 25.9  32\n400 297.7 28.0  34\n401 297.7 28.0  34\n402 731.7 22.8  46\n403 706.7 25.0  36\n404 643.0 21.3  41\n405 725.1 21.8  34\n406 680.9 27.9  27\n407 860.6 17.0  67\n408  55.0 14.2  46\n409 855.3 19.9  44\n410 450.2 23.4  31\n411  52.8 14.7  42\n412  43.6  8.2  53\n413 442.1 22.8  27\n414 715.1 26.4  33\n415 723.1 24.1  50\n416 698.6 27.5  27\n417 575.8 26.3  39\n418  28.3 13.8  24\n419 664.5 24.9  42\n420 613.0 24.8  36\n421 635.9 26.2  36\n422 690.0 30.8  19\n423 795.9 29.3  27\n424 744.4 22.3  48\n425 715.1 26.9  31\n426 753.8 20.4  56\n427 753.8 20.4  56\n428 672.6 27.9  33\n429 698.6 26.2  34\n430 613.0 24.6  44\n431 849.3 19.4  45\n432 605.3 23.3  40\n433 698.6 23.9  38\n434 723.1 20.9  66\n435 811.2 22.2  45\n436 376.6 23.8  51\n437 672.6 26.8  35\n438 768.4 14.2  73\n439 715.1 23.6  53\n440 738.1 19.1  46\n441 855.3 16.2  58\n442 672.6 25.5  29\n443  41.6 10.9  64\n444 368.3 14.8  78\n445 855.3 16.2  58\n446 100.7 17.3  80\n447 664.5 19.1  70\n448  28.3  8.9  35\n449 844.0 10.5  77\n450 613.0 19.3  61\n451 690.0 23.4  49\n452 649.9 11.8  88\n453 730.6 17.7  65\n454 803.3 17.4  54\n455 753.8 16.8  56\n456 567.2 17.9  48\n457 753.8 16.6  59\n458 635.9 19.9  50\n459 715.1 18.9  64\n460 819.1 15.5  72\n461 715.1 18.9  64\n462 715.1 18.9  64\n463 825.1 14.5  76\n464  16.2  4.6  82\n465  16.2  5.1  77\n466  15.3  4.6  59\n467  36.9 10.2  45\n468  41.1 11.2  41\n469  43.5 13.3  27\n470  25.6 13.7  33\n471  25.6 17.6  27\n472  73.7 18.0  40\n473 229.0 14.3  79\n474 252.6 24.5  50\n475 316.7 26.4  35\n476 350.2 22.7  40\n477 395.0 27.2  28\n478 423.4 26.1  45\n479 423.4 18.2  82\n480 431.6 22.6  57\n481 560.0 30.2  25\n482 560.0 30.2  22\n483 587.1 23.4  40\n484 587.1 31.0  27\n485 587.1 33.1  25\n486 596.3 30.6  28\n487 605.8 24.1  43\n488 605.8 26.4  34\n489 605.8 19.4  71\n490 605.8 20.6  58\n491 605.8 28.7  33\n492 624.1 32.4  21\n493 633.6 32.4  27\n494 633.6 27.5  29\n495 643.0 30.8  30\n496 661.8 23.9  42\n497 661.8 32.6  26\n498 671.2 32.3  27\n499 671.2 33.3  26\n500 671.2 27.3  63\n501 671.2 21.6  65\n502 671.2 21.6  65\n503 671.2 20.7  69\n504 689.1 29.2  30\n505 689.1 28.9  29\n506 744.4 26.7  35\n507 752.6 18.5  73\n508 752.6 25.9  41\n509 752.6 25.9  41\n510 752.6 21.1  71\n511 752.6 18.2  62\n512 665.6 27.8  35\n513 665.6 27.8  32\n514 665.6 21.9  71\n515 665.6 21.2  70\n516 614.7 25.6  42\n517 106.7 11.8  31\n\n\nVisualitzem les variables originals quines han donat els que no son outliers\n\nhead(summary(mvnoutliers, select = \"new_data\"))\n\n       DC temp RH\n2   669.1 18.0 33\n3   686.9 14.6 33\n6   488.0 22.2 29\n7   495.6 24.1 27\n9   692.6 13.1 63\n10  698.6 22.8 40\n11  698.6 17.8 51\n12  713.0 19.3 38\n13  665.3 17.0 72\n14  686.5 21.3 42\n15  699.6 26.4 21\n16  713.9 22.9 44\n18  664.2 16.7 47\n21  692.6 18.3 40\n22  724.3 19.1 38\n24  537.4 19.5 43\n25  594.2 23.7 32\n26  601.4 16.3 60\n27  668.0 19.0 34\n28  686.5 19.4 48\n29  721.4 30.2 24\n30  728.6 22.8 39\n31  692.3 25.4 24\n32  709.9 11.2 78\n33  706.8 20.6 37\n34  718.3 17.7 39\n35  724.3 21.2 32\n36  730.2 18.2 62\n37  669.1 21.7 24\n38  682.6 11.3 60\n39  686.9 17.8 27\n42  624.2 18.4 42\n43  647.1 16.6 54\n44  698.6 19.6 48\n45  735.7 12.9 74\n46  692.3 25.9 24\n47  686.5 14.7 70\n48  442.9 23.0 36\n51  706.4 20.8 17\n52  631.2 21.5 34\n53  654.1 20.4 42\n54  654.1 20.4 42\n55  661.3 17.6 45\n56  706.4 27.7 24\n57  730.2 17.8 63\n58  691.8 13.8 50\n63  466.6 18.8 35\n64  631.2 20.8 33\n65  638.8 23.1 31\n66  661.3 18.6 44\n67  668.0 23.0 37\n68  668.0 19.6 33\n69  668.0 19.6 33\n72  692.3 17.7 37\n74  614.5 17.3 43\n75  713.9 27.6 30\n79  529.8 14.7 66\n80  561.6 21.6 19\n81  601.4 19.5 39\n82  631.2 17.9 44\n83  647.1 18.6 51\n84  654.1 16.6 47\n85  661.3 20.2 45\n86  706.4 21.5 15\n87  706.4 25.4 27\n88  706.4 22.4 34\n89  728.6 25.3 36\n91  624.2 14.7 59\n93  488.0 20.8 32\n94  601.4 18.2 43\n95  638.8 23.4 22\n96  704.4 17.8 64\n100 601.4 19.8 39\n101 601.4 19.8 39\n102 614.5 14.4 66\n103 647.1 20.1 40\n104 674.4 24.1 29\n108 631.2 21.4 33\n109 698.6 20.3 45\n110 709.9 17.4 56\n113 704.4 22.8 39\n114 724.3 18.9 35\n120 466.6 19.6 36\n121 608.2 10.3 74\n122 608.2 17.1 43\n123 680.7 22.5 42\n124 671.9 17.9 45\n125 692.3 19.8 50\n126 691.8 20.6 24\n128 728.6 17.2 43\n129 673.8 15.9 46\n130 691.8 15.4 35\n134 685.2 17.6 42\n136 594.2 17.6 52\n137 680.7 17.2 58\n138 686.5 15.6 66\n140 692.6 21.7 38\n141 686.5 21.9 39\n142 513.3 23.3 31\n143 529.8 21.2 51\n145 513.3 23.8 32\n146 578.8 27.4 22\n148 671.9 24.2 28\n149 647.1 17.4 43\n150 685.2 23.7 25\n151 433.3 23.2 39\n153 424.1 24.6 43\n154 692.3 20.1 47\n155 721.4 29.6 27\n156 647.1 16.4 47\n157 721.4 28.6 27\n158 654.1 18.4 45\n159 654.1 20.5 35\n160 668.0 19.0 34\n162 578.8 20.3 41\n164 674.4 17.8 56\n165 704.4 17.8 67\n167 654.1 16.6 47\n168 570.5 23.4 33\n170 578.8 20.7 45\n171 699.6 21.9 35\n172 609.6 17.4 50\n173 601.4 20.1 39\n174 686.5 17.7 39\n175 624.2 14.2 53\n176 624.2 20.3 39\n178 631.2 19.2 44\n179 735.7 18.3 45\n180 614.5 14.4 66\n181 680.7 23.9 32\n182 664.2 19.1 32\n184 696.1 16.8 45\n185 586.7 20.8 34\n186 692.6 17.6 46\n188 686.5 21.0 42\n192 578.8 24.2 28\n193 647.1 24.6 22\n194 699.6 24.3 25\n195 647.1 24.6 22\n196 586.7 23.5 36\n198 706.4 21.5 15\n199 692.6 13.9 59\n200 665.3 22.6 38\n201 692.6 21.6 33\n204 673.8 20.2 37\n206 706.4 22.1 34\n207 594.2 22.9 31\n208 692.6 20.7 37\n209 668.0 19.6 33\n210 685.2 23.2 26\n211 686.9 18.4 25\n213 692.3 20.1 47\n217 680.7 16.9 60\n218 709.9 12.4 73\n219 699.6 19.4 19\n221 631.2 16.2 59\n222 713.9 18.6 49\n225 735.7 15.4 57\n226 728.6 22.9 39\n227 696.1 16.1 44\n228 480.8 20.1 34\n229 728.6 28.3 26\n230 480.8 16.4 43\n231 699.6 26.4 21\n232 728.6 27.8 27\n233 692.6 18.7 43\n234 671.9 24.3 36\n235 674.4 17.7 25\n236 601.4 19.6 41\n237 674.4 18.2 46\n238 692.6 18.8 40\n239 674.4 25.1 27\n243 589.9 15.4 66\n244 700.7 21.9 73\n245 700.7 22.4 54\n246 700.7 26.8 38\n247 700.7 25.7 39\n248 503.6 20.7 70\n249 666.7 28.7 28\n250 666.7 21.7 40\n251 666.7 26.8 25\n252 666.7 24.0 36\n253 666.7 22.1 37\n254 565.5 21.4 38\n255 621.7 18.9 41\n256 694.8 22.3 46\n257 581.1 23.9 41\n258 581.1 21.4 44\n259 692.3 20.6 59\n260 692.3 23.7 40\n261 542.0 28.3 32\n262 573.0 11.2 84\n263 573.0 21.4 42\n264 629.1 19.3 39\n265 684.4 21.8 53\n266 550.3 22.1 54\n267 607.1 19.4 55\n268 658.2 23.7 24\n269 658.2 21.0 32\n270 658.2 19.1 53\n271 658.2 21.8 56\n272 658.2 20.1 58\n273 658.2 20.2 47\n286 411.8 23.4 40\n288 474.9 22.1 49\n289 474.9 24.2 32\n290 474.9 24.3 30\n291 474.9 18.7 53\n292 474.9 25.3 39\n293 466.3 22.9 40\n294 430.8 26.9 28\n296 430.8 22.2 48\n306 714.3 19.0 52\n307 714.3 17.1 53\n308 714.3 23.8 35\n309 758.1 16.0 45\n310 758.1 24.9 27\n311 758.1 25.3 27\n312 758.1 24.8 28\n313 706.6 12.2 78\n314 777.1 24.3 27\n315 777.1 19.7 41\n316 817.5 18.5 30\n317 739.4 18.6 24\n318 739.4 19.2 24\n319 783.5 21.6 27\n320 783.5 21.6 28\n321 783.5 18.9 34\n322 783.5 16.8 28\n323 783.5 16.8 28\n324 822.8 12.9 39\n325 726.9 13.7 56\n326 751.5 24.2 27\n327 751.5 24.1 27\n328 751.5 21.2 32\n329 751.5 19.7 35\n330 751.5 23.5 27\n331 751.5 24.2 27\n332 795.3 21.5 28\n333 795.3 17.1 41\n334 721.1 18.1 54\n335 764.0 18.0 51\n336 764.0  9.8 86\n337 764.0 19.3 44\n338 764.0 23.0 34\n339 764.0 22.7 35\n340 764.0 20.4 41\n341 764.0 19.3 44\n342 770.3 15.7 51\n343 807.1 20.6 37\n344 807.1 15.9 51\n345 807.1 12.2 66\n346 807.1 16.8 43\n347 807.1 21.3 35\n348 745.3 10.1 75\n349 745.3 17.4 57\n350 745.3 12.8 64\n351 745.3 10.1 75\n352 745.3 15.4 53\n353 745.3 20.6 43\n354 745.3 19.8 47\n355 745.3 18.7 50\n356 745.3 20.8 35\n357 745.3 20.8 35\n358 789.7 15.9 55\n359 789.7 19.7 39\n360 789.7 21.1 39\n361 789.7 18.4 42\n362 789.7 17.3 45\n363 732.3 15.2 64\n364 770.3 15.9 53\n365 770.3 21.1 35\n366 770.3 19.6 45\n367 812.1 15.9 38\n368 812.1 16.4 27\n369 744.4 16.8 47\n370 825.1 13.8 77\n371 825.1 13.8 77\n372 520.5 14.2 58\n373 664.5 10.4 75\n374 698.6 20.3 42\n375 855.3 10.3 78\n376 744.4 15.4 57\n377 672.6 21.1 54\n378 715.1 21.9 42\n381 458.8 19.3 39\n382 643.0 16.2 63\n383 690.0 28.2 29\n384 753.8 20.5 58\n385 819.1 21.3 44\n386 613.0 20.9 50\n387 750.5 20.6 55\n389 706.7 23.3 34\n390 706.7 23.3 34\n392 738.1 20.7 46\n393 825.1 21.9 43\n397 750.5 20.4 55\n398 613.0 24.3 33\n399 715.1 25.9 32\n402 731.7 22.8 46\n403 706.7 25.0 36\n404 643.0 21.3 41\n405 725.1 21.8 34\n406 680.9 27.9 27\n407 860.6 17.0 67\n409 855.3 19.9 44\n410 450.2 23.4 31\n413 442.1 22.8 27\n414 715.1 26.4 33\n415 723.1 24.1 50\n416 698.6 27.5 27\n417 575.8 26.3 39\n419 664.5 24.9 42\n420 613.0 24.8 36\n421 635.9 26.2 36\n422 690.0 30.8 19\n423 795.9 29.3 27\n424 744.4 22.3 48\n425 715.1 26.9 31\n426 753.8 20.4 56\n427 753.8 20.4 56\n428 672.6 27.9 33\n429 698.6 26.2 34\n430 613.0 24.6 44\n431 849.3 19.4 45\n432 605.3 23.3 40\n433 698.6 23.9 38\n434 723.1 20.9 66\n435 811.2 22.2 45\n437 672.6 26.8 35\n438 768.4 14.2 73\n439 715.1 23.6 53\n440 738.1 19.1 46\n441 855.3 16.2 58\n442 672.6 25.5 29\n445 855.3 16.2 58\n447 664.5 19.1 70\n449 844.0 10.5 77\n450 613.0 19.3 61\n451 690.0 23.4 49\n452 649.9 11.8 88\n453 730.6 17.7 65\n454 803.3 17.4 54\n455 753.8 16.8 56\n456 567.2 17.9 48\n457 753.8 16.6 59\n458 635.9 19.9 50\n459 715.1 18.9 64\n460 819.1 15.5 72\n461 715.1 18.9 64\n462 715.1 18.9 64\n463 825.1 14.5 76\n477 395.0 27.2 28\n478 423.4 26.1 45\n480 431.6 22.6 57\n481 560.0 30.2 25\n482 560.0 30.2 22\n483 587.1 23.4 40\n484 587.1 31.0 27\n485 587.1 33.1 25\n486 596.3 30.6 28\n487 605.8 24.1 43\n488 605.8 26.4 34\n489 605.8 19.4 71\n490 605.8 20.6 58\n491 605.8 28.7 33\n492 624.1 32.4 21\n493 633.6 32.4 27\n494 633.6 27.5 29\n495 643.0 30.8 30\n496 661.8 23.9 42\n497 661.8 32.6 26\n498 671.2 32.3 27\n499 671.2 33.3 26\n501 671.2 21.6 65\n502 671.2 21.6 65\n503 671.2 20.7 69\n504 689.1 29.2 30\n505 689.1 28.9 29\n506 744.4 26.7 35\n507 752.6 18.5 73\n508 752.6 25.9 41\n509 752.6 25.9 41\n510 752.6 21.1 71\n511 752.6 18.2 62\n512 665.6 27.8 35\n513 665.6 27.8 32\n514 665.6 21.9 71\n515 665.6 21.2 70\n516 614.7 25.6 42\n\n\n$multivariate_normality\n     Test Statistic p.value     Method          MVN\n1 Royston   162.918  &lt;0.001 asymptotic ✗ Not normal\n\n$univariate_normality\n              Test Variable Statistic p.value    Normality\n1 Anderson-Darling       DC    43.098  &lt;0.001 ✗ Not normal\n2 Anderson-Darling     temp     1.812  &lt;0.001 ✗ Not normal\n3 Anderson-Darling       RH     7.871  &lt;0.001 ✗ Not normal\n\n$descriptives\n  Variable   n    Mean Std.Dev Median  Min   Max  25th  75th   Skew Kurtosis\n1       DC 517 547.940 248.066  664.2  7.9 860.6 437.7 713.9 -1.097    2.746\n2     temp 517  18.889   5.807   19.3  2.2  33.3  15.5  22.8 -0.330    3.123\n3       RH 517  44.288  16.317   42.0 15.0 100.0  33.0  53.0  0.860    3.422\n\n$multivariate_outliers\n    Observation Mahalanobis.Distance\n1           466               95.620\n2           464               92.910\n3           105               92.347\n4           284               91.668\n5           465               91.312\n6           285               90.872\n7           448               89.349\n8            76               85.582\n9           177               85.110\n10          197               85.110\n11          395               84.820\n12          166               82.912\n13          379               82.172\n14          412               80.371\n15          418               80.180\n16          394               79.742\n17          203               79.121\n18            4               79.053\n19          467               78.658\n20          240               78.246\n21          469               76.910\n22          391               76.746\n23          468               76.551\n24          470               76.236\n25          119               75.377\n26          388               75.344\n27           62               75.271\n28            5               73.796\n29          117               73.244\n30           98               73.243\n31           49               72.952\n32          443               72.902\n33           97               72.889\n34          396               71.492\n35           59               71.469\n36            1               71.192\n37           60               70.684\n38           20               70.109\n39          471               69.553\n40          183               69.161\n41          305               68.417\n42          191               68.156\n43          127               67.840\n44          277               67.386\n45          278               67.386\n46          279               67.386\n47          280               67.386\n48          517               67.373\n49          380               67.118\n50          106               66.945\n51          111               66.582\n52          116               66.430\n53           78               66.389\n54          115               65.699\n55          132               65.676\n56          411               65.587\n57           73               65.387\n58           77               65.351\n59          408               65.339\n60          241               65.243\n61           61               65.055\n62          187               65.055\n63           17               65.028\n64          133               64.804\n65          118               64.776\n66          242               63.968\n67          169               63.822\n68           40               63.798\n69          220               63.737\n70           50               63.488\n71          214               63.488\n72          223               63.488\n73          135               63.471\n74          147               63.165\n75          202               62.350\n76          190               62.077\n77          282               62.062\n78          189               61.798\n79           19               61.726\n80          112               61.540\n81          446               61.359\n82           70               61.172\n83           92               61.051\n84          161               60.679\n85           90               60.596\n86          215               60.585\n87          216               60.585\n88           71               60.301\n89          163               59.452\n90          107               58.594\n91          131               58.351\n92          472               56.439\n93          205               55.917\n94          281               47.755\n95          300               47.138\n96          283               44.684\n97          473               40.862\n98          274               40.080\n99          276               38.049\n100         275               37.750\n101          23               33.308\n102         224               30.706\n103         303               30.572\n104         304               30.572\n105         297               28.501\n106         474               27.684\n107         298               26.969\n108         144               24.440\n109         301               23.821\n110         444               23.544\n111         287               23.359\n112         302               22.481\n113         479               21.975\n114         139               21.533\n115         400               20.893\n116         401               20.893\n117         212               19.743\n118         475               18.348\n119         476               15.110\n120         436               14.908\n121         152               14.508\n122         299               13.807\n123         500               13.600\n124          41               13.455\n125           8               13.137\n126         295               12.864\n127          99               12.083\n\n$new_data\n       DC temp RH\n2   669.1 18.0 33\n3   686.9 14.6 33\n6   488.0 22.2 29\n7   495.6 24.1 27\n9   692.6 13.1 63\n10  698.6 22.8 40\n11  698.6 17.8 51\n12  713.0 19.3 38\n13  665.3 17.0 72\n14  686.5 21.3 42\n15  699.6 26.4 21\n16  713.9 22.9 44\n18  664.2 16.7 47\n21  692.6 18.3 40\n22  724.3 19.1 38\n24  537.4 19.5 43\n25  594.2 23.7 32\n26  601.4 16.3 60\n27  668.0 19.0 34\n28  686.5 19.4 48\n29  721.4 30.2 24\n30  728.6 22.8 39\n31  692.3 25.4 24\n32  709.9 11.2 78\n33  706.8 20.6 37\n34  718.3 17.7 39\n35  724.3 21.2 32\n36  730.2 18.2 62\n37  669.1 21.7 24\n38  682.6 11.3 60\n39  686.9 17.8 27\n42  624.2 18.4 42\n43  647.1 16.6 54\n44  698.6 19.6 48\n45  735.7 12.9 74\n46  692.3 25.9 24\n47  686.5 14.7 70\n48  442.9 23.0 36\n51  706.4 20.8 17\n52  631.2 21.5 34\n53  654.1 20.4 42\n54  654.1 20.4 42\n55  661.3 17.6 45\n56  706.4 27.7 24\n57  730.2 17.8 63\n58  691.8 13.8 50\n63  466.6 18.8 35\n64  631.2 20.8 33\n65  638.8 23.1 31\n66  661.3 18.6 44\n67  668.0 23.0 37\n68  668.0 19.6 33\n69  668.0 19.6 33\n72  692.3 17.7 37\n74  614.5 17.3 43\n75  713.9 27.6 30\n79  529.8 14.7 66\n80  561.6 21.6 19\n81  601.4 19.5 39\n82  631.2 17.9 44\n83  647.1 18.6 51\n84  654.1 16.6 47\n85  661.3 20.2 45\n86  706.4 21.5 15\n87  706.4 25.4 27\n88  706.4 22.4 34\n89  728.6 25.3 36\n91  624.2 14.7 59\n93  488.0 20.8 32\n94  601.4 18.2 43\n95  638.8 23.4 22\n96  704.4 17.8 64\n100 601.4 19.8 39\n101 601.4 19.8 39\n102 614.5 14.4 66\n103 647.1 20.1 40\n104 674.4 24.1 29\n108 631.2 21.4 33\n109 698.6 20.3 45\n110 709.9 17.4 56\n113 704.4 22.8 39\n114 724.3 18.9 35\n120 466.6 19.6 36\n121 608.2 10.3 74\n122 608.2 17.1 43\n123 680.7 22.5 42\n124 671.9 17.9 45\n125 692.3 19.8 50\n126 691.8 20.6 24\n128 728.6 17.2 43\n129 673.8 15.9 46\n130 691.8 15.4 35\n134 685.2 17.6 42\n136 594.2 17.6 52\n137 680.7 17.2 58\n138 686.5 15.6 66\n140 692.6 21.7 38\n141 686.5 21.9 39\n142 513.3 23.3 31\n143 529.8 21.2 51\n145 513.3 23.8 32\n146 578.8 27.4 22\n148 671.9 24.2 28\n149 647.1 17.4 43\n150 685.2 23.7 25\n151 433.3 23.2 39\n153 424.1 24.6 43\n154 692.3 20.1 47\n155 721.4 29.6 27\n156 647.1 16.4 47\n157 721.4 28.6 27\n158 654.1 18.4 45\n159 654.1 20.5 35\n160 668.0 19.0 34\n162 578.8 20.3 41\n164 674.4 17.8 56\n165 704.4 17.8 67\n167 654.1 16.6 47\n168 570.5 23.4 33\n170 578.8 20.7 45\n171 699.6 21.9 35\n172 609.6 17.4 50\n173 601.4 20.1 39\n174 686.5 17.7 39\n175 624.2 14.2 53\n176 624.2 20.3 39\n178 631.2 19.2 44\n179 735.7 18.3 45\n180 614.5 14.4 66\n181 680.7 23.9 32\n182 664.2 19.1 32\n184 696.1 16.8 45\n185 586.7 20.8 34\n186 692.6 17.6 46\n188 686.5 21.0 42\n192 578.8 24.2 28\n193 647.1 24.6 22\n194 699.6 24.3 25\n195 647.1 24.6 22\n196 586.7 23.5 36\n198 706.4 21.5 15\n199 692.6 13.9 59\n200 665.3 22.6 38\n201 692.6 21.6 33\n204 673.8 20.2 37\n206 706.4 22.1 34\n207 594.2 22.9 31\n208 692.6 20.7 37\n209 668.0 19.6 33\n210 685.2 23.2 26\n211 686.9 18.4 25\n213 692.3 20.1 47\n217 680.7 16.9 60\n218 709.9 12.4 73\n219 699.6 19.4 19\n221 631.2 16.2 59\n222 713.9 18.6 49\n225 735.7 15.4 57\n226 728.6 22.9 39\n227 696.1 16.1 44\n228 480.8 20.1 34\n229 728.6 28.3 26\n230 480.8 16.4 43\n231 699.6 26.4 21\n232 728.6 27.8 27\n233 692.6 18.7 43\n234 671.9 24.3 36\n235 674.4 17.7 25\n236 601.4 19.6 41\n237 674.4 18.2 46\n238 692.6 18.8 40\n239 674.4 25.1 27\n243 589.9 15.4 66\n244 700.7 21.9 73\n245 700.7 22.4 54\n246 700.7 26.8 38\n247 700.7 25.7 39\n248 503.6 20.7 70\n249 666.7 28.7 28\n250 666.7 21.7 40\n251 666.7 26.8 25\n252 666.7 24.0 36\n253 666.7 22.1 37\n254 565.5 21.4 38\n255 621.7 18.9 41\n256 694.8 22.3 46\n257 581.1 23.9 41\n258 581.1 21.4 44\n259 692.3 20.6 59\n260 692.3 23.7 40\n261 542.0 28.3 32\n262 573.0 11.2 84\n263 573.0 21.4 42\n264 629.1 19.3 39\n265 684.4 21.8 53\n266 550.3 22.1 54\n267 607.1 19.4 55\n268 658.2 23.7 24\n269 658.2 21.0 32\n270 658.2 19.1 53\n271 658.2 21.8 56\n272 658.2 20.1 58\n273 658.2 20.2 47\n286 411.8 23.4 40\n288 474.9 22.1 49\n289 474.9 24.2 32\n290 474.9 24.3 30\n291 474.9 18.7 53\n292 474.9 25.3 39\n293 466.3 22.9 40\n294 430.8 26.9 28\n296 430.8 22.2 48\n306 714.3 19.0 52\n307 714.3 17.1 53\n308 714.3 23.8 35\n309 758.1 16.0 45\n310 758.1 24.9 27\n311 758.1 25.3 27\n312 758.1 24.8 28\n313 706.6 12.2 78\n314 777.1 24.3 27\n315 777.1 19.7 41\n316 817.5 18.5 30\n317 739.4 18.6 24\n318 739.4 19.2 24\n319 783.5 21.6 27\n320 783.5 21.6 28\n321 783.5 18.9 34\n322 783.5 16.8 28\n323 783.5 16.8 28\n324 822.8 12.9 39\n325 726.9 13.7 56\n326 751.5 24.2 27\n327 751.5 24.1 27\n328 751.5 21.2 32\n329 751.5 19.7 35\n330 751.5 23.5 27\n331 751.5 24.2 27\n332 795.3 21.5 28\n333 795.3 17.1 41\n334 721.1 18.1 54\n335 764.0 18.0 51\n336 764.0  9.8 86\n337 764.0 19.3 44\n338 764.0 23.0 34\n339 764.0 22.7 35\n340 764.0 20.4 41\n341 764.0 19.3 44\n342 770.3 15.7 51\n343 807.1 20.6 37\n344 807.1 15.9 51\n345 807.1 12.2 66\n346 807.1 16.8 43\n347 807.1 21.3 35\n348 745.3 10.1 75\n349 745.3 17.4 57\n350 745.3 12.8 64\n351 745.3 10.1 75\n352 745.3 15.4 53\n353 745.3 20.6 43\n354 745.3 19.8 47\n355 745.3 18.7 50\n356 745.3 20.8 35\n357 745.3 20.8 35\n358 789.7 15.9 55\n359 789.7 19.7 39\n360 789.7 21.1 39\n361 789.7 18.4 42\n362 789.7 17.3 45\n363 732.3 15.2 64\n364 770.3 15.9 53\n365 770.3 21.1 35\n366 770.3 19.6 45\n367 812.1 15.9 38\n368 812.1 16.4 27\n369 744.4 16.8 47\n370 825.1 13.8 77\n371 825.1 13.8 77\n372 520.5 14.2 58\n373 664.5 10.4 75\n374 698.6 20.3 42\n375 855.3 10.3 78\n376 744.4 15.4 57\n377 672.6 21.1 54\n378 715.1 21.9 42\n381 458.8 19.3 39\n382 643.0 16.2 63\n383 690.0 28.2 29\n384 753.8 20.5 58\n385 819.1 21.3 44\n386 613.0 20.9 50\n387 750.5 20.6 55\n389 706.7 23.3 34\n390 706.7 23.3 34\n392 738.1 20.7 46\n393 825.1 21.9 43\n397 750.5 20.4 55\n398 613.0 24.3 33\n399 715.1 25.9 32\n402 731.7 22.8 46\n403 706.7 25.0 36\n404 643.0 21.3 41\n405 725.1 21.8 34\n406 680.9 27.9 27\n407 860.6 17.0 67\n409 855.3 19.9 44\n410 450.2 23.4 31\n413 442.1 22.8 27\n414 715.1 26.4 33\n415 723.1 24.1 50\n416 698.6 27.5 27\n417 575.8 26.3 39\n419 664.5 24.9 42\n420 613.0 24.8 36\n421 635.9 26.2 36\n422 690.0 30.8 19\n423 795.9 29.3 27\n424 744.4 22.3 48\n425 715.1 26.9 31\n426 753.8 20.4 56\n427 753.8 20.4 56\n428 672.6 27.9 33\n429 698.6 26.2 34\n430 613.0 24.6 44\n431 849.3 19.4 45\n432 605.3 23.3 40\n433 698.6 23.9 38\n434 723.1 20.9 66\n435 811.2 22.2 45\n437 672.6 26.8 35\n438 768.4 14.2 73\n439 715.1 23.6 53\n440 738.1 19.1 46\n441 855.3 16.2 58\n442 672.6 25.5 29\n445 855.3 16.2 58\n447 664.5 19.1 70\n449 844.0 10.5 77\n450 613.0 19.3 61\n451 690.0 23.4 49\n452 649.9 11.8 88\n453 730.6 17.7 65\n454 803.3 17.4 54\n455 753.8 16.8 56\n456 567.2 17.9 48\n457 753.8 16.6 59\n458 635.9 19.9 50\n459 715.1 18.9 64\n460 819.1 15.5 72\n461 715.1 18.9 64\n462 715.1 18.9 64\n463 825.1 14.5 76\n477 395.0 27.2 28\n478 423.4 26.1 45\n480 431.6 22.6 57\n481 560.0 30.2 25\n482 560.0 30.2 22\n483 587.1 23.4 40\n484 587.1 31.0 27\n485 587.1 33.1 25\n486 596.3 30.6 28\n487 605.8 24.1 43\n488 605.8 26.4 34\n489 605.8 19.4 71\n490 605.8 20.6 58\n491 605.8 28.7 33\n492 624.1 32.4 21\n493 633.6 32.4 27\n494 633.6 27.5 29\n495 643.0 30.8 30\n496 661.8 23.9 42\n497 661.8 32.6 26\n498 671.2 32.3 27\n499 671.2 33.3 26\n501 671.2 21.6 65\n502 671.2 21.6 65\n503 671.2 20.7 69\n504 689.1 29.2 30\n505 689.1 28.9 29\n506 744.4 26.7 35\n507 752.6 18.5 73\n508 752.6 25.9 41\n509 752.6 25.9 41\n510 752.6 21.1 71\n511 752.6 18.2 62\n512 665.6 27.8 35\n513 665.6 27.8 32\n514 665.6 21.9 71\n515 665.6 21.2 70\n516 614.7 25.6 42\n\n$data\n       DC temp  RH\n1    94.3  8.2  51\n2   669.1 18.0  33\n3   686.9 14.6  33\n4    77.5  8.3  97\n5   102.2 11.4  99\n6   488.0 22.2  29\n7   495.6 24.1  27\n8   608.2  8.0  86\n9   692.6 13.1  63\n10  698.6 22.8  40\n11  698.6 17.8  51\n12  713.0 19.3  38\n13  665.3 17.0  72\n14  686.5 21.3  42\n15  699.6 26.4  21\n16  713.9 22.9  44\n17   80.8 15.1  27\n18  664.2 16.7  47\n19   70.8 15.9  35\n20   97.1  9.3  44\n21  692.6 18.3  40\n22  724.3 19.1  38\n23  200.0 21.0  44\n24  537.4 19.5  43\n25  594.2 23.7  32\n26  601.4 16.3  60\n27  668.0 19.0  34\n28  686.5 19.4  48\n29  721.4 30.2  24\n30  728.6 22.8  39\n31  692.3 25.4  24\n32  709.9 11.2  78\n33  706.8 20.6  37\n34  718.3 17.7  39\n35  724.3 21.2  32\n36  730.2 18.2  62\n37  669.1 21.7  24\n38  682.6 11.3  60\n39  686.9 17.8  27\n40   67.6 14.1  43\n41  366.7 23.3  37\n42  624.2 18.4  42\n43  647.1 16.6  54\n44  698.6 19.6  48\n45  735.7 12.9  74\n46  692.3 25.9  24\n47  686.5 14.7  70\n48  442.9 23.0  36\n49   64.7 11.8  35\n50  103.8 11.0  46\n51  706.4 20.8  17\n52  631.2 21.5  34\n53  654.1 20.4  42\n54  654.1 20.4  42\n55  661.3 17.6  45\n56  706.4 27.7  24\n57  730.2 17.8  63\n58  691.8 13.8  50\n59   34.0 13.9  40\n60   43.0 12.3  51\n61  102.2 11.5  39\n62  102.2  5.5  59\n63  466.6 18.8  35\n64  631.2 20.8  33\n65  638.8 23.1  31\n66  661.3 18.6  44\n67  668.0 23.0  37\n68  668.0 19.6  33\n69  668.0 19.6  33\n70   77.5 17.2  26\n71   97.8 15.8  27\n72  692.3 17.7  37\n73   77.5 15.6  25\n74  614.5 17.3  43\n75  713.9 27.6  30\n76   26.6  6.7  79\n77   43.0 15.7  43\n78  103.8  8.3  72\n79  529.8 14.7  66\n80  561.6 21.6  19\n81  601.4 19.5  39\n82  631.2 17.9  44\n83  647.1 18.6  51\n84  654.1 16.6  47\n85  661.3 20.2  45\n86  706.4 21.5  15\n87  706.4 25.4  27\n88  706.4 22.4  34\n89  728.6 25.3  36\n90   80.8 17.4  25\n91  624.2 14.7  59\n92   80.8 17.4  24\n93  488.0 20.8  32\n94  601.4 18.2  43\n95  638.8 23.4  22\n96  704.4 17.8  64\n97   30.2 12.7  48\n98   15.5 17.4  24\n99  601.4 11.6  87\n100 601.4 19.8  39\n101 601.4 19.8  39\n102 614.5 14.4  66\n103 647.1 20.1  40\n104 674.4 24.1  29\n105   9.3  5.3  78\n106  57.3 12.7  52\n107  74.3 18.2  29\n108 631.2 21.4  33\n109 698.6 20.3  45\n110 709.9 17.4  56\n111  57.3 13.7  43\n112  77.5 18.8  18\n113 704.4 22.8  39\n114 724.3 18.9  35\n115  67.6 15.8  27\n116  67.6 15.5  27\n117  80.8 11.6  30\n118  80.8 15.2  27\n119  86.6 10.6  30\n120 466.6 19.6  36\n121 608.2 10.3  74\n122 608.2 17.1  43\n123 680.7 22.5  42\n124 671.9 17.9  45\n125 692.3 19.8  50\n126 691.8 20.6  24\n127 103.8  9.0  49\n128 728.6 17.2  43\n129 673.8 15.9  46\n130 691.8 15.4  35\n131  87.2 15.4  40\n132  64.7 14.0  39\n133 102.2 10.6  46\n134 685.2 17.6  42\n135  67.6 14.9  38\n136 594.2 17.6  52\n137 680.7 17.2  58\n138 686.5 15.6  66\n139 313.4 18.0  42\n140 692.6 21.7  38\n141 686.5 21.9  39\n142 513.3 23.3  31\n143 529.8 21.2  51\n144 296.3 16.6  53\n145 513.3 23.8  32\n146 578.8 27.4  22\n147  86.6 13.2  40\n148 671.9 24.2  28\n149 647.1 17.4  43\n150 685.2 23.7  25\n151 433.3 23.2  39\n152 355.2 24.8  29\n153 424.1 24.6  43\n154 692.3 20.1  47\n155 721.4 29.6  27\n156 647.1 16.4  47\n157 721.4 28.6  27\n158 654.1 18.4  45\n159 654.1 20.5  35\n160 668.0 19.0  34\n161  86.6 16.1  29\n162 578.8 20.3  41\n163 100.4 15.2  31\n164 674.4 17.8  56\n165 704.4 17.8  67\n166  55.0  5.3  70\n167 654.1 16.6  47\n168 570.5 23.4  33\n169  97.8 14.6  26\n170 578.8 20.7  45\n171 699.6 21.9  35\n172 609.6 17.4  50\n173 601.4 20.1  39\n174 686.5 17.7  39\n175 624.2 14.2  53\n176 624.2 20.3  39\n177  55.2  5.8  54\n178 631.2 19.2  44\n179 735.7 18.3  45\n180 614.5 14.4  66\n181 680.7 23.9  32\n182 664.2 19.1  32\n183  48.3 12.4  53\n184 696.1 16.8  45\n185 586.7 20.8  34\n186 692.6 17.6  46\n187 102.2 11.5  39\n188 686.5 21.0  42\n189  89.4 13.3  42\n190  92.4 11.5  60\n191  97.8 11.7  33\n192 578.8 24.2  28\n193 647.1 24.6  22\n194 699.6 24.3  25\n195 647.1 24.6  22\n196 586.7 23.5  36\n197  55.2  5.8  54\n198 706.4 21.5  15\n199 692.6 13.9  59\n200 665.3 22.6  38\n201 692.6 21.6  33\n202  83.7 12.4  54\n203  32.1  8.8  68\n204 673.8 20.2  37\n205 100.4 15.1  64\n206 706.4 22.1  34\n207 594.2 22.9  31\n208 692.6 20.7  37\n209 668.0 19.6  33\n210 685.2 23.2  26\n211 686.9 18.4  25\n212 594.2  5.1  96\n213 692.3 20.1  47\n214 103.8 11.0  46\n215  80.8 17.0  27\n216  80.8 17.0  27\n217 680.7 16.9  60\n218 709.9 12.4  73\n219 699.6 19.4  19\n220  86.6 15.2  27\n221 631.2 16.2  59\n222 713.9 18.6  49\n223 103.8 11.0  46\n224 309.9 13.4  79\n225 735.7 15.4  57\n226 728.6 22.9  39\n227 696.1 16.1  44\n228 480.8 20.1  34\n229 728.6 28.3  26\n230 480.8 16.4  43\n231 699.6 26.4  21\n232 728.6 27.8  27\n233 692.6 18.7  43\n234 671.9 24.3  36\n235 674.4 17.7  25\n236 601.4 19.6  41\n237 674.4 18.2  46\n238 692.6 18.8  40\n239 674.4 25.1  27\n240   7.9 13.4  75\n241  43.5 15.2  51\n242  85.3 16.7  20\n243 589.9 15.4  66\n244 700.7 21.9  73\n245 700.7 22.4  54\n246 700.7 26.8  38\n247 700.7 25.7  39\n248 503.6 20.7  70\n249 666.7 28.7  28\n250 666.7 21.7  40\n251 666.7 26.8  25\n252 666.7 24.0  36\n253 666.7 22.1  37\n254 565.5 21.4  38\n255 621.7 18.9  41\n256 694.8 22.3  46\n257 581.1 23.9  41\n258 581.1 21.4  44\n259 692.3 20.6  59\n260 692.3 23.7  40\n261 542.0 28.3  32\n262 573.0 11.2  84\n263 573.0 21.4  42\n264 629.1 19.3  39\n265 684.4 21.8  53\n266 550.3 22.1  54\n267 607.1 19.4  55\n268 658.2 23.7  24\n269 658.2 21.0  32\n270 658.2 19.1  53\n271 658.2 21.8  56\n272 658.2 20.1  58\n273 658.2 20.2  47\n274 353.5  4.8  57\n275 354.6  5.1  61\n276 352.0  5.1  61\n277 349.7  4.6  21\n278 349.7  4.6  21\n279 349.7  4.6  21\n280 349.7  4.6  21\n281 352.6  2.2  59\n282 349.7  5.1  24\n283 353.5  4.2  51\n284  18.7  8.8  35\n285  15.8  7.5  46\n286 411.8 23.4  40\n287 437.7 12.6  90\n288 474.9 22.1  49\n289 474.9 24.2  32\n290 474.9 24.3  30\n291 474.9 18.7  53\n292 474.9 25.3  39\n293 466.3 22.9  40\n294 430.8 26.9  28\n295 440.9 17.1  67\n296 430.8 22.2  48\n297 290.8 14.3  46\n298 290.8 15.4  45\n299 377.2 19.6  43\n300 233.8 10.6  90\n301 298.1 20.7  25\n302 298.1 19.1  39\n303 232.1 19.2  38\n304 232.1 19.2  38\n305 113.8 11.3  94\n306 714.3 19.0  52\n307 714.3 17.1  53\n308 714.3 23.8  35\n309 758.1 16.0  45\n310 758.1 24.9  27\n311 758.1 25.3  27\n312 758.1 24.8  28\n313 706.6 12.2  78\n314 777.1 24.3  27\n315 777.1 19.7  41\n316 817.5 18.5  30\n317 739.4 18.6  24\n318 739.4 19.2  24\n319 783.5 21.6  27\n320 783.5 21.6  28\n321 783.5 18.9  34\n322 783.5 16.8  28\n323 783.5 16.8  28\n324 822.8 12.9  39\n325 726.9 13.7  56\n326 751.5 24.2  27\n327 751.5 24.1  27\n328 751.5 21.2  32\n329 751.5 19.7  35\n330 751.5 23.5  27\n331 751.5 24.2  27\n332 795.3 21.5  28\n333 795.3 17.1  41\n334 721.1 18.1  54\n335 764.0 18.0  51\n336 764.0  9.8  86\n337 764.0 19.3  44\n338 764.0 23.0  34\n339 764.0 22.7  35\n340 764.0 20.4  41\n341 764.0 19.3  44\n342 770.3 15.7  51\n343 807.1 20.6  37\n344 807.1 15.9  51\n345 807.1 12.2  66\n346 807.1 16.8  43\n347 807.1 21.3  35\n348 745.3 10.1  75\n349 745.3 17.4  57\n350 745.3 12.8  64\n351 745.3 10.1  75\n352 745.3 15.4  53\n353 745.3 20.6  43\n354 745.3 19.8  47\n355 745.3 18.7  50\n356 745.3 20.8  35\n357 745.3 20.8  35\n358 789.7 15.9  55\n359 789.7 19.7  39\n360 789.7 21.1  39\n361 789.7 18.4  42\n362 789.7 17.3  45\n363 732.3 15.2  64\n364 770.3 15.9  53\n365 770.3 21.1  35\n366 770.3 19.6  45\n367 812.1 15.9  38\n368 812.1 16.4  27\n369 744.4 16.8  47\n370 825.1 13.8  77\n371 825.1 13.8  77\n372 520.5 14.2  58\n373 664.5 10.4  75\n374 698.6 20.3  42\n375 855.3 10.3  78\n376 744.4 15.4  57\n377 672.6 21.1  54\n378 715.1 21.9  42\n379  30.6  8.7  51\n380 171.4  5.2 100\n381 458.8 19.3  39\n382 643.0 16.2  63\n383 690.0 28.2  29\n384 753.8 20.5  58\n385 819.1 21.3  44\n386 613.0 20.9  50\n387 750.5 20.6  55\n388  30.6 11.6  48\n389 706.7 23.3  34\n390 706.7 23.3  34\n391  58.3  7.5  71\n392 738.1 20.7  46\n393 825.1 21.9  43\n394  25.6 15.2  19\n395  46.7  5.3  68\n396  56.9 10.1  62\n397 750.5 20.4  55\n398 613.0 24.3  33\n399 715.1 25.9  32\n400 297.7 28.0  34\n401 297.7 28.0  34\n402 731.7 22.8  46\n403 706.7 25.0  36\n404 643.0 21.3  41\n405 725.1 21.8  34\n406 680.9 27.9  27\n407 860.6 17.0  67\n408  55.0 14.2  46\n409 855.3 19.9  44\n410 450.2 23.4  31\n411  52.8 14.7  42\n412  43.6  8.2  53\n413 442.1 22.8  27\n414 715.1 26.4  33\n415 723.1 24.1  50\n416 698.6 27.5  27\n417 575.8 26.3  39\n418  28.3 13.8  24\n419 664.5 24.9  42\n420 613.0 24.8  36\n421 635.9 26.2  36\n422 690.0 30.8  19\n423 795.9 29.3  27\n424 744.4 22.3  48\n425 715.1 26.9  31\n426 753.8 20.4  56\n427 753.8 20.4  56\n428 672.6 27.9  33\n429 698.6 26.2  34\n430 613.0 24.6  44\n431 849.3 19.4  45\n432 605.3 23.3  40\n433 698.6 23.9  38\n434 723.1 20.9  66\n435 811.2 22.2  45\n436 376.6 23.8  51\n437 672.6 26.8  35\n438 768.4 14.2  73\n439 715.1 23.6  53\n440 738.1 19.1  46\n441 855.3 16.2  58\n442 672.6 25.5  29\n443  41.6 10.9  64\n444 368.3 14.8  78\n445 855.3 16.2  58\n446 100.7 17.3  80\n447 664.5 19.1  70\n448  28.3  8.9  35\n449 844.0 10.5  77\n450 613.0 19.3  61\n451 690.0 23.4  49\n452 649.9 11.8  88\n453 730.6 17.7  65\n454 803.3 17.4  54\n455 753.8 16.8  56\n456 567.2 17.9  48\n457 753.8 16.6  59\n458 635.9 19.9  50\n459 715.1 18.9  64\n460 819.1 15.5  72\n461 715.1 18.9  64\n462 715.1 18.9  64\n463 825.1 14.5  76\n464  16.2  4.6  82\n465  16.2  5.1  77\n466  15.3  4.6  59\n467  36.9 10.2  45\n468  41.1 11.2  41\n469  43.5 13.3  27\n470  25.6 13.7  33\n471  25.6 17.6  27\n472  73.7 18.0  40\n473 229.0 14.3  79\n474 252.6 24.5  50\n475 316.7 26.4  35\n476 350.2 22.7  40\n477 395.0 27.2  28\n478 423.4 26.1  45\n479 423.4 18.2  82\n480 431.6 22.6  57\n481 560.0 30.2  25\n482 560.0 30.2  22\n483 587.1 23.4  40\n484 587.1 31.0  27\n485 587.1 33.1  25\n486 596.3 30.6  28\n487 605.8 24.1  43\n488 605.8 26.4  34\n489 605.8 19.4  71\n490 605.8 20.6  58\n491 605.8 28.7  33\n492 624.1 32.4  21\n493 633.6 32.4  27\n494 633.6 27.5  29\n495 643.0 30.8  30\n496 661.8 23.9  42\n497 661.8 32.6  26\n498 671.2 32.3  27\n499 671.2 33.3  26\n500 671.2 27.3  63\n501 671.2 21.6  65\n502 671.2 21.6  65\n503 671.2 20.7  69\n504 689.1 29.2  30\n505 689.1 28.9  29\n506 744.4 26.7  35\n507 752.6 18.5  73\n508 752.6 25.9  41\n509 752.6 25.9  41\n510 752.6 21.1  71\n511 752.6 18.2  62\n512 665.6 27.8  35\n513 665.6 27.8  32\n514 665.6 21.9  71\n515 665.6 21.2  70\n516 614.7 25.6  42\n517 106.7 11.8  31\n\n\nVisualitzem els resultats del test de normalitat univariant\n\nhead(summary(mvnoutliers, select = \"mvn\"))\n\n     Test Statistic p.value     Method          MVN\n1 Royston   162.918  &lt;0.001 asymptotic ✗ Not normal\n\n\n$multivariate_normality\n     Test Statistic p.value     Method          MVN\n1 Royston   162.918  &lt;0.001 asymptotic ✗ Not normal\n\n$univariate_normality\n              Test Variable Statistic p.value    Normality\n1 Anderson-Darling       DC    43.098  &lt;0.001 ✗ Not normal\n2 Anderson-Darling     temp     1.812  &lt;0.001 ✗ Not normal\n3 Anderson-Darling       RH     7.871  &lt;0.001 ✗ Not normal\n\n$descriptives\n  Variable   n    Mean Std.Dev Median  Min   Max  25th  75th   Skew Kurtosis\n1       DC 517 547.940 248.066  664.2  7.9 860.6 437.7 713.9 -1.097    2.746\n2     temp 517  18.889   5.807   19.3  2.2  33.3  15.5  22.8 -0.330    3.123\n3       RH 517  44.288  16.317   42.0 15.0 100.0  33.0  53.0  0.860    3.422\n\n$multivariate_outliers\n    Observation Mahalanobis.Distance\n1           466               95.620\n2           464               92.910\n3           105               92.347\n4           284               91.668\n5           465               91.312\n6           285               90.872\n7           448               89.349\n8            76               85.582\n9           177               85.110\n10          197               85.110\n11          395               84.820\n12          166               82.912\n13          379               82.172\n14          412               80.371\n15          418               80.180\n16          394               79.742\n17          203               79.121\n18            4               79.053\n19          467               78.658\n20          240               78.246\n21          469               76.910\n22          391               76.746\n23          468               76.551\n24          470               76.236\n25          119               75.377\n26          388               75.344\n27           62               75.271\n28            5               73.796\n29          117               73.244\n30           98               73.243\n31           49               72.952\n32          443               72.902\n33           97               72.889\n34          396               71.492\n35           59               71.469\n36            1               71.192\n37           60               70.684\n38           20               70.109\n39          471               69.553\n40          183               69.161\n41          305               68.417\n42          191               68.156\n43          127               67.840\n44          277               67.386\n45          278               67.386\n46          279               67.386\n47          280               67.386\n48          517               67.373\n49          380               67.118\n50          106               66.945\n51          111               66.582\n52          116               66.430\n53           78               66.389\n54          115               65.699\n55          132               65.676\n56          411               65.587\n57           73               65.387\n58           77               65.351\n59          408               65.339\n60          241               65.243\n61           61               65.055\n62          187               65.055\n63           17               65.028\n64          133               64.804\n65          118               64.776\n66          242               63.968\n67          169               63.822\n68           40               63.798\n69          220               63.737\n70           50               63.488\n71          214               63.488\n72          223               63.488\n73          135               63.471\n74          147               63.165\n75          202               62.350\n76          190               62.077\n77          282               62.062\n78          189               61.798\n79           19               61.726\n80          112               61.540\n81          446               61.359\n82           70               61.172\n83           92               61.051\n84          161               60.679\n85           90               60.596\n86          215               60.585\n87          216               60.585\n88           71               60.301\n89          163               59.452\n90          107               58.594\n91          131               58.351\n92          472               56.439\n93          205               55.917\n94          281               47.755\n95          300               47.138\n96          283               44.684\n97          473               40.862\n98          274               40.080\n99          276               38.049\n100         275               37.750\n101          23               33.308\n102         224               30.706\n103         303               30.572\n104         304               30.572\n105         297               28.501\n106         474               27.684\n107         298               26.969\n108         144               24.440\n109         301               23.821\n110         444               23.544\n111         287               23.359\n112         302               22.481\n113         479               21.975\n114         139               21.533\n115         400               20.893\n116         401               20.893\n117         212               19.743\n118         475               18.348\n119         476               15.110\n120         436               14.908\n121         152               14.508\n122         299               13.807\n123         500               13.600\n124          41               13.455\n125           8               13.137\n126         295               12.864\n127          99               12.083\n\n$new_data\n       DC temp RH\n2   669.1 18.0 33\n3   686.9 14.6 33\n6   488.0 22.2 29\n7   495.6 24.1 27\n9   692.6 13.1 63\n10  698.6 22.8 40\n11  698.6 17.8 51\n12  713.0 19.3 38\n13  665.3 17.0 72\n14  686.5 21.3 42\n15  699.6 26.4 21\n16  713.9 22.9 44\n18  664.2 16.7 47\n21  692.6 18.3 40\n22  724.3 19.1 38\n24  537.4 19.5 43\n25  594.2 23.7 32\n26  601.4 16.3 60\n27  668.0 19.0 34\n28  686.5 19.4 48\n29  721.4 30.2 24\n30  728.6 22.8 39\n31  692.3 25.4 24\n32  709.9 11.2 78\n33  706.8 20.6 37\n34  718.3 17.7 39\n35  724.3 21.2 32\n36  730.2 18.2 62\n37  669.1 21.7 24\n38  682.6 11.3 60\n39  686.9 17.8 27\n42  624.2 18.4 42\n43  647.1 16.6 54\n44  698.6 19.6 48\n45  735.7 12.9 74\n46  692.3 25.9 24\n47  686.5 14.7 70\n48  442.9 23.0 36\n51  706.4 20.8 17\n52  631.2 21.5 34\n53  654.1 20.4 42\n54  654.1 20.4 42\n55  661.3 17.6 45\n56  706.4 27.7 24\n57  730.2 17.8 63\n58  691.8 13.8 50\n63  466.6 18.8 35\n64  631.2 20.8 33\n65  638.8 23.1 31\n66  661.3 18.6 44\n67  668.0 23.0 37\n68  668.0 19.6 33\n69  668.0 19.6 33\n72  692.3 17.7 37\n74  614.5 17.3 43\n75  713.9 27.6 30\n79  529.8 14.7 66\n80  561.6 21.6 19\n81  601.4 19.5 39\n82  631.2 17.9 44\n83  647.1 18.6 51\n84  654.1 16.6 47\n85  661.3 20.2 45\n86  706.4 21.5 15\n87  706.4 25.4 27\n88  706.4 22.4 34\n89  728.6 25.3 36\n91  624.2 14.7 59\n93  488.0 20.8 32\n94  601.4 18.2 43\n95  638.8 23.4 22\n96  704.4 17.8 64\n100 601.4 19.8 39\n101 601.4 19.8 39\n102 614.5 14.4 66\n103 647.1 20.1 40\n104 674.4 24.1 29\n108 631.2 21.4 33\n109 698.6 20.3 45\n110 709.9 17.4 56\n113 704.4 22.8 39\n114 724.3 18.9 35\n120 466.6 19.6 36\n121 608.2 10.3 74\n122 608.2 17.1 43\n123 680.7 22.5 42\n124 671.9 17.9 45\n125 692.3 19.8 50\n126 691.8 20.6 24\n128 728.6 17.2 43\n129 673.8 15.9 46\n130 691.8 15.4 35\n134 685.2 17.6 42\n136 594.2 17.6 52\n137 680.7 17.2 58\n138 686.5 15.6 66\n140 692.6 21.7 38\n141 686.5 21.9 39\n142 513.3 23.3 31\n143 529.8 21.2 51\n145 513.3 23.8 32\n146 578.8 27.4 22\n148 671.9 24.2 28\n149 647.1 17.4 43\n150 685.2 23.7 25\n151 433.3 23.2 39\n153 424.1 24.6 43\n154 692.3 20.1 47\n155 721.4 29.6 27\n156 647.1 16.4 47\n157 721.4 28.6 27\n158 654.1 18.4 45\n159 654.1 20.5 35\n160 668.0 19.0 34\n162 578.8 20.3 41\n164 674.4 17.8 56\n165 704.4 17.8 67\n167 654.1 16.6 47\n168 570.5 23.4 33\n170 578.8 20.7 45\n171 699.6 21.9 35\n172 609.6 17.4 50\n173 601.4 20.1 39\n174 686.5 17.7 39\n175 624.2 14.2 53\n176 624.2 20.3 39\n178 631.2 19.2 44\n179 735.7 18.3 45\n180 614.5 14.4 66\n181 680.7 23.9 32\n182 664.2 19.1 32\n184 696.1 16.8 45\n185 586.7 20.8 34\n186 692.6 17.6 46\n188 686.5 21.0 42\n192 578.8 24.2 28\n193 647.1 24.6 22\n194 699.6 24.3 25\n195 647.1 24.6 22\n196 586.7 23.5 36\n198 706.4 21.5 15\n199 692.6 13.9 59\n200 665.3 22.6 38\n201 692.6 21.6 33\n204 673.8 20.2 37\n206 706.4 22.1 34\n207 594.2 22.9 31\n208 692.6 20.7 37\n209 668.0 19.6 33\n210 685.2 23.2 26\n211 686.9 18.4 25\n213 692.3 20.1 47\n217 680.7 16.9 60\n218 709.9 12.4 73\n219 699.6 19.4 19\n221 631.2 16.2 59\n222 713.9 18.6 49\n225 735.7 15.4 57\n226 728.6 22.9 39\n227 696.1 16.1 44\n228 480.8 20.1 34\n229 728.6 28.3 26\n230 480.8 16.4 43\n231 699.6 26.4 21\n232 728.6 27.8 27\n233 692.6 18.7 43\n234 671.9 24.3 36\n235 674.4 17.7 25\n236 601.4 19.6 41\n237 674.4 18.2 46\n238 692.6 18.8 40\n239 674.4 25.1 27\n243 589.9 15.4 66\n244 700.7 21.9 73\n245 700.7 22.4 54\n246 700.7 26.8 38\n247 700.7 25.7 39\n248 503.6 20.7 70\n249 666.7 28.7 28\n250 666.7 21.7 40\n251 666.7 26.8 25\n252 666.7 24.0 36\n253 666.7 22.1 37\n254 565.5 21.4 38\n255 621.7 18.9 41\n256 694.8 22.3 46\n257 581.1 23.9 41\n258 581.1 21.4 44\n259 692.3 20.6 59\n260 692.3 23.7 40\n261 542.0 28.3 32\n262 573.0 11.2 84\n263 573.0 21.4 42\n264 629.1 19.3 39\n265 684.4 21.8 53\n266 550.3 22.1 54\n267 607.1 19.4 55\n268 658.2 23.7 24\n269 658.2 21.0 32\n270 658.2 19.1 53\n271 658.2 21.8 56\n272 658.2 20.1 58\n273 658.2 20.2 47\n286 411.8 23.4 40\n288 474.9 22.1 49\n289 474.9 24.2 32\n290 474.9 24.3 30\n291 474.9 18.7 53\n292 474.9 25.3 39\n293 466.3 22.9 40\n294 430.8 26.9 28\n296 430.8 22.2 48\n306 714.3 19.0 52\n307 714.3 17.1 53\n308 714.3 23.8 35\n309 758.1 16.0 45\n310 758.1 24.9 27\n311 758.1 25.3 27\n312 758.1 24.8 28\n313 706.6 12.2 78\n314 777.1 24.3 27\n315 777.1 19.7 41\n316 817.5 18.5 30\n317 739.4 18.6 24\n318 739.4 19.2 24\n319 783.5 21.6 27\n320 783.5 21.6 28\n321 783.5 18.9 34\n322 783.5 16.8 28\n323 783.5 16.8 28\n324 822.8 12.9 39\n325 726.9 13.7 56\n326 751.5 24.2 27\n327 751.5 24.1 27\n328 751.5 21.2 32\n329 751.5 19.7 35\n330 751.5 23.5 27\n331 751.5 24.2 27\n332 795.3 21.5 28\n333 795.3 17.1 41\n334 721.1 18.1 54\n335 764.0 18.0 51\n336 764.0  9.8 86\n337 764.0 19.3 44\n338 764.0 23.0 34\n339 764.0 22.7 35\n340 764.0 20.4 41\n341 764.0 19.3 44\n342 770.3 15.7 51\n343 807.1 20.6 37\n344 807.1 15.9 51\n345 807.1 12.2 66\n346 807.1 16.8 43\n347 807.1 21.3 35\n348 745.3 10.1 75\n349 745.3 17.4 57\n350 745.3 12.8 64\n351 745.3 10.1 75\n352 745.3 15.4 53\n353 745.3 20.6 43\n354 745.3 19.8 47\n355 745.3 18.7 50\n356 745.3 20.8 35\n357 745.3 20.8 35\n358 789.7 15.9 55\n359 789.7 19.7 39\n360 789.7 21.1 39\n361 789.7 18.4 42\n362 789.7 17.3 45\n363 732.3 15.2 64\n364 770.3 15.9 53\n365 770.3 21.1 35\n366 770.3 19.6 45\n367 812.1 15.9 38\n368 812.1 16.4 27\n369 744.4 16.8 47\n370 825.1 13.8 77\n371 825.1 13.8 77\n372 520.5 14.2 58\n373 664.5 10.4 75\n374 698.6 20.3 42\n375 855.3 10.3 78\n376 744.4 15.4 57\n377 672.6 21.1 54\n378 715.1 21.9 42\n381 458.8 19.3 39\n382 643.0 16.2 63\n383 690.0 28.2 29\n384 753.8 20.5 58\n385 819.1 21.3 44\n386 613.0 20.9 50\n387 750.5 20.6 55\n389 706.7 23.3 34\n390 706.7 23.3 34\n392 738.1 20.7 46\n393 825.1 21.9 43\n397 750.5 20.4 55\n398 613.0 24.3 33\n399 715.1 25.9 32\n402 731.7 22.8 46\n403 706.7 25.0 36\n404 643.0 21.3 41\n405 725.1 21.8 34\n406 680.9 27.9 27\n407 860.6 17.0 67\n409 855.3 19.9 44\n410 450.2 23.4 31\n413 442.1 22.8 27\n414 715.1 26.4 33\n415 723.1 24.1 50\n416 698.6 27.5 27\n417 575.8 26.3 39\n419 664.5 24.9 42\n420 613.0 24.8 36\n421 635.9 26.2 36\n422 690.0 30.8 19\n423 795.9 29.3 27\n424 744.4 22.3 48\n425 715.1 26.9 31\n426 753.8 20.4 56\n427 753.8 20.4 56\n428 672.6 27.9 33\n429 698.6 26.2 34\n430 613.0 24.6 44\n431 849.3 19.4 45\n432 605.3 23.3 40\n433 698.6 23.9 38\n434 723.1 20.9 66\n435 811.2 22.2 45\n437 672.6 26.8 35\n438 768.4 14.2 73\n439 715.1 23.6 53\n440 738.1 19.1 46\n441 855.3 16.2 58\n442 672.6 25.5 29\n445 855.3 16.2 58\n447 664.5 19.1 70\n449 844.0 10.5 77\n450 613.0 19.3 61\n451 690.0 23.4 49\n452 649.9 11.8 88\n453 730.6 17.7 65\n454 803.3 17.4 54\n455 753.8 16.8 56\n456 567.2 17.9 48\n457 753.8 16.6 59\n458 635.9 19.9 50\n459 715.1 18.9 64\n460 819.1 15.5 72\n461 715.1 18.9 64\n462 715.1 18.9 64\n463 825.1 14.5 76\n477 395.0 27.2 28\n478 423.4 26.1 45\n480 431.6 22.6 57\n481 560.0 30.2 25\n482 560.0 30.2 22\n483 587.1 23.4 40\n484 587.1 31.0 27\n485 587.1 33.1 25\n486 596.3 30.6 28\n487 605.8 24.1 43\n488 605.8 26.4 34\n489 605.8 19.4 71\n490 605.8 20.6 58\n491 605.8 28.7 33\n492 624.1 32.4 21\n493 633.6 32.4 27\n494 633.6 27.5 29\n495 643.0 30.8 30\n496 661.8 23.9 42\n497 661.8 32.6 26\n498 671.2 32.3 27\n499 671.2 33.3 26\n501 671.2 21.6 65\n502 671.2 21.6 65\n503 671.2 20.7 69\n504 689.1 29.2 30\n505 689.1 28.9 29\n506 744.4 26.7 35\n507 752.6 18.5 73\n508 752.6 25.9 41\n509 752.6 25.9 41\n510 752.6 21.1 71\n511 752.6 18.2 62\n512 665.6 27.8 35\n513 665.6 27.8 32\n514 665.6 21.9 71\n515 665.6 21.2 70\n516 614.7 25.6 42\n\n$data\n       DC temp  RH\n1    94.3  8.2  51\n2   669.1 18.0  33\n3   686.9 14.6  33\n4    77.5  8.3  97\n5   102.2 11.4  99\n6   488.0 22.2  29\n7   495.6 24.1  27\n8   608.2  8.0  86\n9   692.6 13.1  63\n10  698.6 22.8  40\n11  698.6 17.8  51\n12  713.0 19.3  38\n13  665.3 17.0  72\n14  686.5 21.3  42\n15  699.6 26.4  21\n16  713.9 22.9  44\n17   80.8 15.1  27\n18  664.2 16.7  47\n19   70.8 15.9  35\n20   97.1  9.3  44\n21  692.6 18.3  40\n22  724.3 19.1  38\n23  200.0 21.0  44\n24  537.4 19.5  43\n25  594.2 23.7  32\n26  601.4 16.3  60\n27  668.0 19.0  34\n28  686.5 19.4  48\n29  721.4 30.2  24\n30  728.6 22.8  39\n31  692.3 25.4  24\n32  709.9 11.2  78\n33  706.8 20.6  37\n34  718.3 17.7  39\n35  724.3 21.2  32\n36  730.2 18.2  62\n37  669.1 21.7  24\n38  682.6 11.3  60\n39  686.9 17.8  27\n40   67.6 14.1  43\n41  366.7 23.3  37\n42  624.2 18.4  42\n43  647.1 16.6  54\n44  698.6 19.6  48\n45  735.7 12.9  74\n46  692.3 25.9  24\n47  686.5 14.7  70\n48  442.9 23.0  36\n49   64.7 11.8  35\n50  103.8 11.0  46\n51  706.4 20.8  17\n52  631.2 21.5  34\n53  654.1 20.4  42\n54  654.1 20.4  42\n55  661.3 17.6  45\n56  706.4 27.7  24\n57  730.2 17.8  63\n58  691.8 13.8  50\n59   34.0 13.9  40\n60   43.0 12.3  51\n61  102.2 11.5  39\n62  102.2  5.5  59\n63  466.6 18.8  35\n64  631.2 20.8  33\n65  638.8 23.1  31\n66  661.3 18.6  44\n67  668.0 23.0  37\n68  668.0 19.6  33\n69  668.0 19.6  33\n70   77.5 17.2  26\n71   97.8 15.8  27\n72  692.3 17.7  37\n73   77.5 15.6  25\n74  614.5 17.3  43\n75  713.9 27.6  30\n76   26.6  6.7  79\n77   43.0 15.7  43\n78  103.8  8.3  72\n79  529.8 14.7  66\n80  561.6 21.6  19\n81  601.4 19.5  39\n82  631.2 17.9  44\n83  647.1 18.6  51\n84  654.1 16.6  47\n85  661.3 20.2  45\n86  706.4 21.5  15\n87  706.4 25.4  27\n88  706.4 22.4  34\n89  728.6 25.3  36\n90   80.8 17.4  25\n91  624.2 14.7  59\n92   80.8 17.4  24\n93  488.0 20.8  32\n94  601.4 18.2  43\n95  638.8 23.4  22\n96  704.4 17.8  64\n97   30.2 12.7  48\n98   15.5 17.4  24\n99  601.4 11.6  87\n100 601.4 19.8  39\n101 601.4 19.8  39\n102 614.5 14.4  66\n103 647.1 20.1  40\n104 674.4 24.1  29\n105   9.3  5.3  78\n106  57.3 12.7  52\n107  74.3 18.2  29\n108 631.2 21.4  33\n109 698.6 20.3  45\n110 709.9 17.4  56\n111  57.3 13.7  43\n112  77.5 18.8  18\n113 704.4 22.8  39\n114 724.3 18.9  35\n115  67.6 15.8  27\n116  67.6 15.5  27\n117  80.8 11.6  30\n118  80.8 15.2  27\n119  86.6 10.6  30\n120 466.6 19.6  36\n121 608.2 10.3  74\n122 608.2 17.1  43\n123 680.7 22.5  42\n124 671.9 17.9  45\n125 692.3 19.8  50\n126 691.8 20.6  24\n127 103.8  9.0  49\n128 728.6 17.2  43\n129 673.8 15.9  46\n130 691.8 15.4  35\n131  87.2 15.4  40\n132  64.7 14.0  39\n133 102.2 10.6  46\n134 685.2 17.6  42\n135  67.6 14.9  38\n136 594.2 17.6  52\n137 680.7 17.2  58\n138 686.5 15.6  66\n139 313.4 18.0  42\n140 692.6 21.7  38\n141 686.5 21.9  39\n142 513.3 23.3  31\n143 529.8 21.2  51\n144 296.3 16.6  53\n145 513.3 23.8  32\n146 578.8 27.4  22\n147  86.6 13.2  40\n148 671.9 24.2  28\n149 647.1 17.4  43\n150 685.2 23.7  25\n151 433.3 23.2  39\n152 355.2 24.8  29\n153 424.1 24.6  43\n154 692.3 20.1  47\n155 721.4 29.6  27\n156 647.1 16.4  47\n157 721.4 28.6  27\n158 654.1 18.4  45\n159 654.1 20.5  35\n160 668.0 19.0  34\n161  86.6 16.1  29\n162 578.8 20.3  41\n163 100.4 15.2  31\n164 674.4 17.8  56\n165 704.4 17.8  67\n166  55.0  5.3  70\n167 654.1 16.6  47\n168 570.5 23.4  33\n169  97.8 14.6  26\n170 578.8 20.7  45\n171 699.6 21.9  35\n172 609.6 17.4  50\n173 601.4 20.1  39\n174 686.5 17.7  39\n175 624.2 14.2  53\n176 624.2 20.3  39\n177  55.2  5.8  54\n178 631.2 19.2  44\n179 735.7 18.3  45\n180 614.5 14.4  66\n181 680.7 23.9  32\n182 664.2 19.1  32\n183  48.3 12.4  53\n184 696.1 16.8  45\n185 586.7 20.8  34\n186 692.6 17.6  46\n187 102.2 11.5  39\n188 686.5 21.0  42\n189  89.4 13.3  42\n190  92.4 11.5  60\n191  97.8 11.7  33\n192 578.8 24.2  28\n193 647.1 24.6  22\n194 699.6 24.3  25\n195 647.1 24.6  22\n196 586.7 23.5  36\n197  55.2  5.8  54\n198 706.4 21.5  15\n199 692.6 13.9  59\n200 665.3 22.6  38\n201 692.6 21.6  33\n202  83.7 12.4  54\n203  32.1  8.8  68\n204 673.8 20.2  37\n205 100.4 15.1  64\n206 706.4 22.1  34\n207 594.2 22.9  31\n208 692.6 20.7  37\n209 668.0 19.6  33\n210 685.2 23.2  26\n211 686.9 18.4  25\n212 594.2  5.1  96\n213 692.3 20.1  47\n214 103.8 11.0  46\n215  80.8 17.0  27\n216  80.8 17.0  27\n217 680.7 16.9  60\n218 709.9 12.4  73\n219 699.6 19.4  19\n220  86.6 15.2  27\n221 631.2 16.2  59\n222 713.9 18.6  49\n223 103.8 11.0  46\n224 309.9 13.4  79\n225 735.7 15.4  57\n226 728.6 22.9  39\n227 696.1 16.1  44\n228 480.8 20.1  34\n229 728.6 28.3  26\n230 480.8 16.4  43\n231 699.6 26.4  21\n232 728.6 27.8  27\n233 692.6 18.7  43\n234 671.9 24.3  36\n235 674.4 17.7  25\n236 601.4 19.6  41\n237 674.4 18.2  46\n238 692.6 18.8  40\n239 674.4 25.1  27\n240   7.9 13.4  75\n241  43.5 15.2  51\n242  85.3 16.7  20\n243 589.9 15.4  66\n244 700.7 21.9  73\n245 700.7 22.4  54\n246 700.7 26.8  38\n247 700.7 25.7  39\n248 503.6 20.7  70\n249 666.7 28.7  28\n250 666.7 21.7  40\n251 666.7 26.8  25\n252 666.7 24.0  36\n253 666.7 22.1  37\n254 565.5 21.4  38\n255 621.7 18.9  41\n256 694.8 22.3  46\n257 581.1 23.9  41\n258 581.1 21.4  44\n259 692.3 20.6  59\n260 692.3 23.7  40\n261 542.0 28.3  32\n262 573.0 11.2  84\n263 573.0 21.4  42\n264 629.1 19.3  39\n265 684.4 21.8  53\n266 550.3 22.1  54\n267 607.1 19.4  55\n268 658.2 23.7  24\n269 658.2 21.0  32\n270 658.2 19.1  53\n271 658.2 21.8  56\n272 658.2 20.1  58\n273 658.2 20.2  47\n274 353.5  4.8  57\n275 354.6  5.1  61\n276 352.0  5.1  61\n277 349.7  4.6  21\n278 349.7  4.6  21\n279 349.7  4.6  21\n280 349.7  4.6  21\n281 352.6  2.2  59\n282 349.7  5.1  24\n283 353.5  4.2  51\n284  18.7  8.8  35\n285  15.8  7.5  46\n286 411.8 23.4  40\n287 437.7 12.6  90\n288 474.9 22.1  49\n289 474.9 24.2  32\n290 474.9 24.3  30\n291 474.9 18.7  53\n292 474.9 25.3  39\n293 466.3 22.9  40\n294 430.8 26.9  28\n295 440.9 17.1  67\n296 430.8 22.2  48\n297 290.8 14.3  46\n298 290.8 15.4  45\n299 377.2 19.6  43\n300 233.8 10.6  90\n301 298.1 20.7  25\n302 298.1 19.1  39\n303 232.1 19.2  38\n304 232.1 19.2  38\n305 113.8 11.3  94\n306 714.3 19.0  52\n307 714.3 17.1  53\n308 714.3 23.8  35\n309 758.1 16.0  45\n310 758.1 24.9  27\n311 758.1 25.3  27\n312 758.1 24.8  28\n313 706.6 12.2  78\n314 777.1 24.3  27\n315 777.1 19.7  41\n316 817.5 18.5  30\n317 739.4 18.6  24\n318 739.4 19.2  24\n319 783.5 21.6  27\n320 783.5 21.6  28\n321 783.5 18.9  34\n322 783.5 16.8  28\n323 783.5 16.8  28\n324 822.8 12.9  39\n325 726.9 13.7  56\n326 751.5 24.2  27\n327 751.5 24.1  27\n328 751.5 21.2  32\n329 751.5 19.7  35\n330 751.5 23.5  27\n331 751.5 24.2  27\n332 795.3 21.5  28\n333 795.3 17.1  41\n334 721.1 18.1  54\n335 764.0 18.0  51\n336 764.0  9.8  86\n337 764.0 19.3  44\n338 764.0 23.0  34\n339 764.0 22.7  35\n340 764.0 20.4  41\n341 764.0 19.3  44\n342 770.3 15.7  51\n343 807.1 20.6  37\n344 807.1 15.9  51\n345 807.1 12.2  66\n346 807.1 16.8  43\n347 807.1 21.3  35\n348 745.3 10.1  75\n349 745.3 17.4  57\n350 745.3 12.8  64\n351 745.3 10.1  75\n352 745.3 15.4  53\n353 745.3 20.6  43\n354 745.3 19.8  47\n355 745.3 18.7  50\n356 745.3 20.8  35\n357 745.3 20.8  35\n358 789.7 15.9  55\n359 789.7 19.7  39\n360 789.7 21.1  39\n361 789.7 18.4  42\n362 789.7 17.3  45\n363 732.3 15.2  64\n364 770.3 15.9  53\n365 770.3 21.1  35\n366 770.3 19.6  45\n367 812.1 15.9  38\n368 812.1 16.4  27\n369 744.4 16.8  47\n370 825.1 13.8  77\n371 825.1 13.8  77\n372 520.5 14.2  58\n373 664.5 10.4  75\n374 698.6 20.3  42\n375 855.3 10.3  78\n376 744.4 15.4  57\n377 672.6 21.1  54\n378 715.1 21.9  42\n379  30.6  8.7  51\n380 171.4  5.2 100\n381 458.8 19.3  39\n382 643.0 16.2  63\n383 690.0 28.2  29\n384 753.8 20.5  58\n385 819.1 21.3  44\n386 613.0 20.9  50\n387 750.5 20.6  55\n388  30.6 11.6  48\n389 706.7 23.3  34\n390 706.7 23.3  34\n391  58.3  7.5  71\n392 738.1 20.7  46\n393 825.1 21.9  43\n394  25.6 15.2  19\n395  46.7  5.3  68\n396  56.9 10.1  62\n397 750.5 20.4  55\n398 613.0 24.3  33\n399 715.1 25.9  32\n400 297.7 28.0  34\n401 297.7 28.0  34\n402 731.7 22.8  46\n403 706.7 25.0  36\n404 643.0 21.3  41\n405 725.1 21.8  34\n406 680.9 27.9  27\n407 860.6 17.0  67\n408  55.0 14.2  46\n409 855.3 19.9  44\n410 450.2 23.4  31\n411  52.8 14.7  42\n412  43.6  8.2  53\n413 442.1 22.8  27\n414 715.1 26.4  33\n415 723.1 24.1  50\n416 698.6 27.5  27\n417 575.8 26.3  39\n418  28.3 13.8  24\n419 664.5 24.9  42\n420 613.0 24.8  36\n421 635.9 26.2  36\n422 690.0 30.8  19\n423 795.9 29.3  27\n424 744.4 22.3  48\n425 715.1 26.9  31\n426 753.8 20.4  56\n427 753.8 20.4  56\n428 672.6 27.9  33\n429 698.6 26.2  34\n430 613.0 24.6  44\n431 849.3 19.4  45\n432 605.3 23.3  40\n433 698.6 23.9  38\n434 723.1 20.9  66\n435 811.2 22.2  45\n436 376.6 23.8  51\n437 672.6 26.8  35\n438 768.4 14.2  73\n439 715.1 23.6  53\n440 738.1 19.1  46\n441 855.3 16.2  58\n442 672.6 25.5  29\n443  41.6 10.9  64\n444 368.3 14.8  78\n445 855.3 16.2  58\n446 100.7 17.3  80\n447 664.5 19.1  70\n448  28.3  8.9  35\n449 844.0 10.5  77\n450 613.0 19.3  61\n451 690.0 23.4  49\n452 649.9 11.8  88\n453 730.6 17.7  65\n454 803.3 17.4  54\n455 753.8 16.8  56\n456 567.2 17.9  48\n457 753.8 16.6  59\n458 635.9 19.9  50\n459 715.1 18.9  64\n460 819.1 15.5  72\n461 715.1 18.9  64\n462 715.1 18.9  64\n463 825.1 14.5  76\n464  16.2  4.6  82\n465  16.2  5.1  77\n466  15.3  4.6  59\n467  36.9 10.2  45\n468  41.1 11.2  41\n469  43.5 13.3  27\n470  25.6 13.7  33\n471  25.6 17.6  27\n472  73.7 18.0  40\n473 229.0 14.3  79\n474 252.6 24.5  50\n475 316.7 26.4  35\n476 350.2 22.7  40\n477 395.0 27.2  28\n478 423.4 26.1  45\n479 423.4 18.2  82\n480 431.6 22.6  57\n481 560.0 30.2  25\n482 560.0 30.2  22\n483 587.1 23.4  40\n484 587.1 31.0  27\n485 587.1 33.1  25\n486 596.3 30.6  28\n487 605.8 24.1  43\n488 605.8 26.4  34\n489 605.8 19.4  71\n490 605.8 20.6  58\n491 605.8 28.7  33\n492 624.1 32.4  21\n493 633.6 32.4  27\n494 633.6 27.5  29\n495 643.0 30.8  30\n496 661.8 23.9  42\n497 661.8 32.6  26\n498 671.2 32.3  27\n499 671.2 33.3  26\n500 671.2 27.3  63\n501 671.2 21.6  65\n502 671.2 21.6  65\n503 671.2 20.7  69\n504 689.1 29.2  30\n505 689.1 28.9  29\n506 744.4 26.7  35\n507 752.6 18.5  73\n508 752.6 25.9  41\n509 752.6 25.9  41\n510 752.6 21.1  71\n511 752.6 18.2  62\n512 665.6 27.8  35\n513 665.6 27.8  32\n514 665.6 21.9  71\n515 665.6 21.2  70\n516 614.7 25.6  42\n517 106.7 11.8  31\n\n\ni multivariant\n\nhead(summary(mvnoutliers, select = \"univariate\"))\n\n              Test Variable Statistic p.value    Normality\n1 Anderson-Darling       DC    43.098  &lt;0.001 ✗ Not normal\n2 Anderson-Darling     temp     1.812  &lt;0.001 ✗ Not normal\n3 Anderson-Darling       RH     7.871  &lt;0.001 ✗ Not normal\n\n\n$multivariate_normality\n     Test Statistic p.value     Method          MVN\n1 Royston   162.918  &lt;0.001 asymptotic ✗ Not normal\n\n$univariate_normality\n              Test Variable Statistic p.value    Normality\n1 Anderson-Darling       DC    43.098  &lt;0.001 ✗ Not normal\n2 Anderson-Darling     temp     1.812  &lt;0.001 ✗ Not normal\n3 Anderson-Darling       RH     7.871  &lt;0.001 ✗ Not normal\n\n$descriptives\n  Variable   n    Mean Std.Dev Median  Min   Max  25th  75th   Skew Kurtosis\n1       DC 517 547.940 248.066  664.2  7.9 860.6 437.7 713.9 -1.097    2.746\n2     temp 517  18.889   5.807   19.3  2.2  33.3  15.5  22.8 -0.330    3.123\n3       RH 517  44.288  16.317   42.0 15.0 100.0  33.0  53.0  0.860    3.422\n\n$multivariate_outliers\n    Observation Mahalanobis.Distance\n1           466               95.620\n2           464               92.910\n3           105               92.347\n4           284               91.668\n5           465               91.312\n6           285               90.872\n7           448               89.349\n8            76               85.582\n9           177               85.110\n10          197               85.110\n11          395               84.820\n12          166               82.912\n13          379               82.172\n14          412               80.371\n15          418               80.180\n16          394               79.742\n17          203               79.121\n18            4               79.053\n19          467               78.658\n20          240               78.246\n21          469               76.910\n22          391               76.746\n23          468               76.551\n24          470               76.236\n25          119               75.377\n26          388               75.344\n27           62               75.271\n28            5               73.796\n29          117               73.244\n30           98               73.243\n31           49               72.952\n32          443               72.902\n33           97               72.889\n34          396               71.492\n35           59               71.469\n36            1               71.192\n37           60               70.684\n38           20               70.109\n39          471               69.553\n40          183               69.161\n41          305               68.417\n42          191               68.156\n43          127               67.840\n44          277               67.386\n45          278               67.386\n46          279               67.386\n47          280               67.386\n48          517               67.373\n49          380               67.118\n50          106               66.945\n51          111               66.582\n52          116               66.430\n53           78               66.389\n54          115               65.699\n55          132               65.676\n56          411               65.587\n57           73               65.387\n58           77               65.351\n59          408               65.339\n60          241               65.243\n61           61               65.055\n62          187               65.055\n63           17               65.028\n64          133               64.804\n65          118               64.776\n66          242               63.968\n67          169               63.822\n68           40               63.798\n69          220               63.737\n70           50               63.488\n71          214               63.488\n72          223               63.488\n73          135               63.471\n74          147               63.165\n75          202               62.350\n76          190               62.077\n77          282               62.062\n78          189               61.798\n79           19               61.726\n80          112               61.540\n81          446               61.359\n82           70               61.172\n83           92               61.051\n84          161               60.679\n85           90               60.596\n86          215               60.585\n87          216               60.585\n88           71               60.301\n89          163               59.452\n90          107               58.594\n91          131               58.351\n92          472               56.439\n93          205               55.917\n94          281               47.755\n95          300               47.138\n96          283               44.684\n97          473               40.862\n98          274               40.080\n99          276               38.049\n100         275               37.750\n101          23               33.308\n102         224               30.706\n103         303               30.572\n104         304               30.572\n105         297               28.501\n106         474               27.684\n107         298               26.969\n108         144               24.440\n109         301               23.821\n110         444               23.544\n111         287               23.359\n112         302               22.481\n113         479               21.975\n114         139               21.533\n115         400               20.893\n116         401               20.893\n117         212               19.743\n118         475               18.348\n119         476               15.110\n120         436               14.908\n121         152               14.508\n122         299               13.807\n123         500               13.600\n124          41               13.455\n125           8               13.137\n126         295               12.864\n127          99               12.083\n\n$new_data\n       DC temp RH\n2   669.1 18.0 33\n3   686.9 14.6 33\n6   488.0 22.2 29\n7   495.6 24.1 27\n9   692.6 13.1 63\n10  698.6 22.8 40\n11  698.6 17.8 51\n12  713.0 19.3 38\n13  665.3 17.0 72\n14  686.5 21.3 42\n15  699.6 26.4 21\n16  713.9 22.9 44\n18  664.2 16.7 47\n21  692.6 18.3 40\n22  724.3 19.1 38\n24  537.4 19.5 43\n25  594.2 23.7 32\n26  601.4 16.3 60\n27  668.0 19.0 34\n28  686.5 19.4 48\n29  721.4 30.2 24\n30  728.6 22.8 39\n31  692.3 25.4 24\n32  709.9 11.2 78\n33  706.8 20.6 37\n34  718.3 17.7 39\n35  724.3 21.2 32\n36  730.2 18.2 62\n37  669.1 21.7 24\n38  682.6 11.3 60\n39  686.9 17.8 27\n42  624.2 18.4 42\n43  647.1 16.6 54\n44  698.6 19.6 48\n45  735.7 12.9 74\n46  692.3 25.9 24\n47  686.5 14.7 70\n48  442.9 23.0 36\n51  706.4 20.8 17\n52  631.2 21.5 34\n53  654.1 20.4 42\n54  654.1 20.4 42\n55  661.3 17.6 45\n56  706.4 27.7 24\n57  730.2 17.8 63\n58  691.8 13.8 50\n63  466.6 18.8 35\n64  631.2 20.8 33\n65  638.8 23.1 31\n66  661.3 18.6 44\n67  668.0 23.0 37\n68  668.0 19.6 33\n69  668.0 19.6 33\n72  692.3 17.7 37\n74  614.5 17.3 43\n75  713.9 27.6 30\n79  529.8 14.7 66\n80  561.6 21.6 19\n81  601.4 19.5 39\n82  631.2 17.9 44\n83  647.1 18.6 51\n84  654.1 16.6 47\n85  661.3 20.2 45\n86  706.4 21.5 15\n87  706.4 25.4 27\n88  706.4 22.4 34\n89  728.6 25.3 36\n91  624.2 14.7 59\n93  488.0 20.8 32\n94  601.4 18.2 43\n95  638.8 23.4 22\n96  704.4 17.8 64\n100 601.4 19.8 39\n101 601.4 19.8 39\n102 614.5 14.4 66\n103 647.1 20.1 40\n104 674.4 24.1 29\n108 631.2 21.4 33\n109 698.6 20.3 45\n110 709.9 17.4 56\n113 704.4 22.8 39\n114 724.3 18.9 35\n120 466.6 19.6 36\n121 608.2 10.3 74\n122 608.2 17.1 43\n123 680.7 22.5 42\n124 671.9 17.9 45\n125 692.3 19.8 50\n126 691.8 20.6 24\n128 728.6 17.2 43\n129 673.8 15.9 46\n130 691.8 15.4 35\n134 685.2 17.6 42\n136 594.2 17.6 52\n137 680.7 17.2 58\n138 686.5 15.6 66\n140 692.6 21.7 38\n141 686.5 21.9 39\n142 513.3 23.3 31\n143 529.8 21.2 51\n145 513.3 23.8 32\n146 578.8 27.4 22\n148 671.9 24.2 28\n149 647.1 17.4 43\n150 685.2 23.7 25\n151 433.3 23.2 39\n153 424.1 24.6 43\n154 692.3 20.1 47\n155 721.4 29.6 27\n156 647.1 16.4 47\n157 721.4 28.6 27\n158 654.1 18.4 45\n159 654.1 20.5 35\n160 668.0 19.0 34\n162 578.8 20.3 41\n164 674.4 17.8 56\n165 704.4 17.8 67\n167 654.1 16.6 47\n168 570.5 23.4 33\n170 578.8 20.7 45\n171 699.6 21.9 35\n172 609.6 17.4 50\n173 601.4 20.1 39\n174 686.5 17.7 39\n175 624.2 14.2 53\n176 624.2 20.3 39\n178 631.2 19.2 44\n179 735.7 18.3 45\n180 614.5 14.4 66\n181 680.7 23.9 32\n182 664.2 19.1 32\n184 696.1 16.8 45\n185 586.7 20.8 34\n186 692.6 17.6 46\n188 686.5 21.0 42\n192 578.8 24.2 28\n193 647.1 24.6 22\n194 699.6 24.3 25\n195 647.1 24.6 22\n196 586.7 23.5 36\n198 706.4 21.5 15\n199 692.6 13.9 59\n200 665.3 22.6 38\n201 692.6 21.6 33\n204 673.8 20.2 37\n206 706.4 22.1 34\n207 594.2 22.9 31\n208 692.6 20.7 37\n209 668.0 19.6 33\n210 685.2 23.2 26\n211 686.9 18.4 25\n213 692.3 20.1 47\n217 680.7 16.9 60\n218 709.9 12.4 73\n219 699.6 19.4 19\n221 631.2 16.2 59\n222 713.9 18.6 49\n225 735.7 15.4 57\n226 728.6 22.9 39\n227 696.1 16.1 44\n228 480.8 20.1 34\n229 728.6 28.3 26\n230 480.8 16.4 43\n231 699.6 26.4 21\n232 728.6 27.8 27\n233 692.6 18.7 43\n234 671.9 24.3 36\n235 674.4 17.7 25\n236 601.4 19.6 41\n237 674.4 18.2 46\n238 692.6 18.8 40\n239 674.4 25.1 27\n243 589.9 15.4 66\n244 700.7 21.9 73\n245 700.7 22.4 54\n246 700.7 26.8 38\n247 700.7 25.7 39\n248 503.6 20.7 70\n249 666.7 28.7 28\n250 666.7 21.7 40\n251 666.7 26.8 25\n252 666.7 24.0 36\n253 666.7 22.1 37\n254 565.5 21.4 38\n255 621.7 18.9 41\n256 694.8 22.3 46\n257 581.1 23.9 41\n258 581.1 21.4 44\n259 692.3 20.6 59\n260 692.3 23.7 40\n261 542.0 28.3 32\n262 573.0 11.2 84\n263 573.0 21.4 42\n264 629.1 19.3 39\n265 684.4 21.8 53\n266 550.3 22.1 54\n267 607.1 19.4 55\n268 658.2 23.7 24\n269 658.2 21.0 32\n270 658.2 19.1 53\n271 658.2 21.8 56\n272 658.2 20.1 58\n273 658.2 20.2 47\n286 411.8 23.4 40\n288 474.9 22.1 49\n289 474.9 24.2 32\n290 474.9 24.3 30\n291 474.9 18.7 53\n292 474.9 25.3 39\n293 466.3 22.9 40\n294 430.8 26.9 28\n296 430.8 22.2 48\n306 714.3 19.0 52\n307 714.3 17.1 53\n308 714.3 23.8 35\n309 758.1 16.0 45\n310 758.1 24.9 27\n311 758.1 25.3 27\n312 758.1 24.8 28\n313 706.6 12.2 78\n314 777.1 24.3 27\n315 777.1 19.7 41\n316 817.5 18.5 30\n317 739.4 18.6 24\n318 739.4 19.2 24\n319 783.5 21.6 27\n320 783.5 21.6 28\n321 783.5 18.9 34\n322 783.5 16.8 28\n323 783.5 16.8 28\n324 822.8 12.9 39\n325 726.9 13.7 56\n326 751.5 24.2 27\n327 751.5 24.1 27\n328 751.5 21.2 32\n329 751.5 19.7 35\n330 751.5 23.5 27\n331 751.5 24.2 27\n332 795.3 21.5 28\n333 795.3 17.1 41\n334 721.1 18.1 54\n335 764.0 18.0 51\n336 764.0  9.8 86\n337 764.0 19.3 44\n338 764.0 23.0 34\n339 764.0 22.7 35\n340 764.0 20.4 41\n341 764.0 19.3 44\n342 770.3 15.7 51\n343 807.1 20.6 37\n344 807.1 15.9 51\n345 807.1 12.2 66\n346 807.1 16.8 43\n347 807.1 21.3 35\n348 745.3 10.1 75\n349 745.3 17.4 57\n350 745.3 12.8 64\n351 745.3 10.1 75\n352 745.3 15.4 53\n353 745.3 20.6 43\n354 745.3 19.8 47\n355 745.3 18.7 50\n356 745.3 20.8 35\n357 745.3 20.8 35\n358 789.7 15.9 55\n359 789.7 19.7 39\n360 789.7 21.1 39\n361 789.7 18.4 42\n362 789.7 17.3 45\n363 732.3 15.2 64\n364 770.3 15.9 53\n365 770.3 21.1 35\n366 770.3 19.6 45\n367 812.1 15.9 38\n368 812.1 16.4 27\n369 744.4 16.8 47\n370 825.1 13.8 77\n371 825.1 13.8 77\n372 520.5 14.2 58\n373 664.5 10.4 75\n374 698.6 20.3 42\n375 855.3 10.3 78\n376 744.4 15.4 57\n377 672.6 21.1 54\n378 715.1 21.9 42\n381 458.8 19.3 39\n382 643.0 16.2 63\n383 690.0 28.2 29\n384 753.8 20.5 58\n385 819.1 21.3 44\n386 613.0 20.9 50\n387 750.5 20.6 55\n389 706.7 23.3 34\n390 706.7 23.3 34\n392 738.1 20.7 46\n393 825.1 21.9 43\n397 750.5 20.4 55\n398 613.0 24.3 33\n399 715.1 25.9 32\n402 731.7 22.8 46\n403 706.7 25.0 36\n404 643.0 21.3 41\n405 725.1 21.8 34\n406 680.9 27.9 27\n407 860.6 17.0 67\n409 855.3 19.9 44\n410 450.2 23.4 31\n413 442.1 22.8 27\n414 715.1 26.4 33\n415 723.1 24.1 50\n416 698.6 27.5 27\n417 575.8 26.3 39\n419 664.5 24.9 42\n420 613.0 24.8 36\n421 635.9 26.2 36\n422 690.0 30.8 19\n423 795.9 29.3 27\n424 744.4 22.3 48\n425 715.1 26.9 31\n426 753.8 20.4 56\n427 753.8 20.4 56\n428 672.6 27.9 33\n429 698.6 26.2 34\n430 613.0 24.6 44\n431 849.3 19.4 45\n432 605.3 23.3 40\n433 698.6 23.9 38\n434 723.1 20.9 66\n435 811.2 22.2 45\n437 672.6 26.8 35\n438 768.4 14.2 73\n439 715.1 23.6 53\n440 738.1 19.1 46\n441 855.3 16.2 58\n442 672.6 25.5 29\n445 855.3 16.2 58\n447 664.5 19.1 70\n449 844.0 10.5 77\n450 613.0 19.3 61\n451 690.0 23.4 49\n452 649.9 11.8 88\n453 730.6 17.7 65\n454 803.3 17.4 54\n455 753.8 16.8 56\n456 567.2 17.9 48\n457 753.8 16.6 59\n458 635.9 19.9 50\n459 715.1 18.9 64\n460 819.1 15.5 72\n461 715.1 18.9 64\n462 715.1 18.9 64\n463 825.1 14.5 76\n477 395.0 27.2 28\n478 423.4 26.1 45\n480 431.6 22.6 57\n481 560.0 30.2 25\n482 560.0 30.2 22\n483 587.1 23.4 40\n484 587.1 31.0 27\n485 587.1 33.1 25\n486 596.3 30.6 28\n487 605.8 24.1 43\n488 605.8 26.4 34\n489 605.8 19.4 71\n490 605.8 20.6 58\n491 605.8 28.7 33\n492 624.1 32.4 21\n493 633.6 32.4 27\n494 633.6 27.5 29\n495 643.0 30.8 30\n496 661.8 23.9 42\n497 661.8 32.6 26\n498 671.2 32.3 27\n499 671.2 33.3 26\n501 671.2 21.6 65\n502 671.2 21.6 65\n503 671.2 20.7 69\n504 689.1 29.2 30\n505 689.1 28.9 29\n506 744.4 26.7 35\n507 752.6 18.5 73\n508 752.6 25.9 41\n509 752.6 25.9 41\n510 752.6 21.1 71\n511 752.6 18.2 62\n512 665.6 27.8 35\n513 665.6 27.8 32\n514 665.6 21.9 71\n515 665.6 21.2 70\n516 614.7 25.6 42\n\n$data\n       DC temp  RH\n1    94.3  8.2  51\n2   669.1 18.0  33\n3   686.9 14.6  33\n4    77.5  8.3  97\n5   102.2 11.4  99\n6   488.0 22.2  29\n7   495.6 24.1  27\n8   608.2  8.0  86\n9   692.6 13.1  63\n10  698.6 22.8  40\n11  698.6 17.8  51\n12  713.0 19.3  38\n13  665.3 17.0  72\n14  686.5 21.3  42\n15  699.6 26.4  21\n16  713.9 22.9  44\n17   80.8 15.1  27\n18  664.2 16.7  47\n19   70.8 15.9  35\n20   97.1  9.3  44\n21  692.6 18.3  40\n22  724.3 19.1  38\n23  200.0 21.0  44\n24  537.4 19.5  43\n25  594.2 23.7  32\n26  601.4 16.3  60\n27  668.0 19.0  34\n28  686.5 19.4  48\n29  721.4 30.2  24\n30  728.6 22.8  39\n31  692.3 25.4  24\n32  709.9 11.2  78\n33  706.8 20.6  37\n34  718.3 17.7  39\n35  724.3 21.2  32\n36  730.2 18.2  62\n37  669.1 21.7  24\n38  682.6 11.3  60\n39  686.9 17.8  27\n40   67.6 14.1  43\n41  366.7 23.3  37\n42  624.2 18.4  42\n43  647.1 16.6  54\n44  698.6 19.6  48\n45  735.7 12.9  74\n46  692.3 25.9  24\n47  686.5 14.7  70\n48  442.9 23.0  36\n49   64.7 11.8  35\n50  103.8 11.0  46\n51  706.4 20.8  17\n52  631.2 21.5  34\n53  654.1 20.4  42\n54  654.1 20.4  42\n55  661.3 17.6  45\n56  706.4 27.7  24\n57  730.2 17.8  63\n58  691.8 13.8  50\n59   34.0 13.9  40\n60   43.0 12.3  51\n61  102.2 11.5  39\n62  102.2  5.5  59\n63  466.6 18.8  35\n64  631.2 20.8  33\n65  638.8 23.1  31\n66  661.3 18.6  44\n67  668.0 23.0  37\n68  668.0 19.6  33\n69  668.0 19.6  33\n70   77.5 17.2  26\n71   97.8 15.8  27\n72  692.3 17.7  37\n73   77.5 15.6  25\n74  614.5 17.3  43\n75  713.9 27.6  30\n76   26.6  6.7  79\n77   43.0 15.7  43\n78  103.8  8.3  72\n79  529.8 14.7  66\n80  561.6 21.6  19\n81  601.4 19.5  39\n82  631.2 17.9  44\n83  647.1 18.6  51\n84  654.1 16.6  47\n85  661.3 20.2  45\n86  706.4 21.5  15\n87  706.4 25.4  27\n88  706.4 22.4  34\n89  728.6 25.3  36\n90   80.8 17.4  25\n91  624.2 14.7  59\n92   80.8 17.4  24\n93  488.0 20.8  32\n94  601.4 18.2  43\n95  638.8 23.4  22\n96  704.4 17.8  64\n97   30.2 12.7  48\n98   15.5 17.4  24\n99  601.4 11.6  87\n100 601.4 19.8  39\n101 601.4 19.8  39\n102 614.5 14.4  66\n103 647.1 20.1  40\n104 674.4 24.1  29\n105   9.3  5.3  78\n106  57.3 12.7  52\n107  74.3 18.2  29\n108 631.2 21.4  33\n109 698.6 20.3  45\n110 709.9 17.4  56\n111  57.3 13.7  43\n112  77.5 18.8  18\n113 704.4 22.8  39\n114 724.3 18.9  35\n115  67.6 15.8  27\n116  67.6 15.5  27\n117  80.8 11.6  30\n118  80.8 15.2  27\n119  86.6 10.6  30\n120 466.6 19.6  36\n121 608.2 10.3  74\n122 608.2 17.1  43\n123 680.7 22.5  42\n124 671.9 17.9  45\n125 692.3 19.8  50\n126 691.8 20.6  24\n127 103.8  9.0  49\n128 728.6 17.2  43\n129 673.8 15.9  46\n130 691.8 15.4  35\n131  87.2 15.4  40\n132  64.7 14.0  39\n133 102.2 10.6  46\n134 685.2 17.6  42\n135  67.6 14.9  38\n136 594.2 17.6  52\n137 680.7 17.2  58\n138 686.5 15.6  66\n139 313.4 18.0  42\n140 692.6 21.7  38\n141 686.5 21.9  39\n142 513.3 23.3  31\n143 529.8 21.2  51\n144 296.3 16.6  53\n145 513.3 23.8  32\n146 578.8 27.4  22\n147  86.6 13.2  40\n148 671.9 24.2  28\n149 647.1 17.4  43\n150 685.2 23.7  25\n151 433.3 23.2  39\n152 355.2 24.8  29\n153 424.1 24.6  43\n154 692.3 20.1  47\n155 721.4 29.6  27\n156 647.1 16.4  47\n157 721.4 28.6  27\n158 654.1 18.4  45\n159 654.1 20.5  35\n160 668.0 19.0  34\n161  86.6 16.1  29\n162 578.8 20.3  41\n163 100.4 15.2  31\n164 674.4 17.8  56\n165 704.4 17.8  67\n166  55.0  5.3  70\n167 654.1 16.6  47\n168 570.5 23.4  33\n169  97.8 14.6  26\n170 578.8 20.7  45\n171 699.6 21.9  35\n172 609.6 17.4  50\n173 601.4 20.1  39\n174 686.5 17.7  39\n175 624.2 14.2  53\n176 624.2 20.3  39\n177  55.2  5.8  54\n178 631.2 19.2  44\n179 735.7 18.3  45\n180 614.5 14.4  66\n181 680.7 23.9  32\n182 664.2 19.1  32\n183  48.3 12.4  53\n184 696.1 16.8  45\n185 586.7 20.8  34\n186 692.6 17.6  46\n187 102.2 11.5  39\n188 686.5 21.0  42\n189  89.4 13.3  42\n190  92.4 11.5  60\n191  97.8 11.7  33\n192 578.8 24.2  28\n193 647.1 24.6  22\n194 699.6 24.3  25\n195 647.1 24.6  22\n196 586.7 23.5  36\n197  55.2  5.8  54\n198 706.4 21.5  15\n199 692.6 13.9  59\n200 665.3 22.6  38\n201 692.6 21.6  33\n202  83.7 12.4  54\n203  32.1  8.8  68\n204 673.8 20.2  37\n205 100.4 15.1  64\n206 706.4 22.1  34\n207 594.2 22.9  31\n208 692.6 20.7  37\n209 668.0 19.6  33\n210 685.2 23.2  26\n211 686.9 18.4  25\n212 594.2  5.1  96\n213 692.3 20.1  47\n214 103.8 11.0  46\n215  80.8 17.0  27\n216  80.8 17.0  27\n217 680.7 16.9  60\n218 709.9 12.4  73\n219 699.6 19.4  19\n220  86.6 15.2  27\n221 631.2 16.2  59\n222 713.9 18.6  49\n223 103.8 11.0  46\n224 309.9 13.4  79\n225 735.7 15.4  57\n226 728.6 22.9  39\n227 696.1 16.1  44\n228 480.8 20.1  34\n229 728.6 28.3  26\n230 480.8 16.4  43\n231 699.6 26.4  21\n232 728.6 27.8  27\n233 692.6 18.7  43\n234 671.9 24.3  36\n235 674.4 17.7  25\n236 601.4 19.6  41\n237 674.4 18.2  46\n238 692.6 18.8  40\n239 674.4 25.1  27\n240   7.9 13.4  75\n241  43.5 15.2  51\n242  85.3 16.7  20\n243 589.9 15.4  66\n244 700.7 21.9  73\n245 700.7 22.4  54\n246 700.7 26.8  38\n247 700.7 25.7  39\n248 503.6 20.7  70\n249 666.7 28.7  28\n250 666.7 21.7  40\n251 666.7 26.8  25\n252 666.7 24.0  36\n253 666.7 22.1  37\n254 565.5 21.4  38\n255 621.7 18.9  41\n256 694.8 22.3  46\n257 581.1 23.9  41\n258 581.1 21.4  44\n259 692.3 20.6  59\n260 692.3 23.7  40\n261 542.0 28.3  32\n262 573.0 11.2  84\n263 573.0 21.4  42\n264 629.1 19.3  39\n265 684.4 21.8  53\n266 550.3 22.1  54\n267 607.1 19.4  55\n268 658.2 23.7  24\n269 658.2 21.0  32\n270 658.2 19.1  53\n271 658.2 21.8  56\n272 658.2 20.1  58\n273 658.2 20.2  47\n274 353.5  4.8  57\n275 354.6  5.1  61\n276 352.0  5.1  61\n277 349.7  4.6  21\n278 349.7  4.6  21\n279 349.7  4.6  21\n280 349.7  4.6  21\n281 352.6  2.2  59\n282 349.7  5.1  24\n283 353.5  4.2  51\n284  18.7  8.8  35\n285  15.8  7.5  46\n286 411.8 23.4  40\n287 437.7 12.6  90\n288 474.9 22.1  49\n289 474.9 24.2  32\n290 474.9 24.3  30\n291 474.9 18.7  53\n292 474.9 25.3  39\n293 466.3 22.9  40\n294 430.8 26.9  28\n295 440.9 17.1  67\n296 430.8 22.2  48\n297 290.8 14.3  46\n298 290.8 15.4  45\n299 377.2 19.6  43\n300 233.8 10.6  90\n301 298.1 20.7  25\n302 298.1 19.1  39\n303 232.1 19.2  38\n304 232.1 19.2  38\n305 113.8 11.3  94\n306 714.3 19.0  52\n307 714.3 17.1  53\n308 714.3 23.8  35\n309 758.1 16.0  45\n310 758.1 24.9  27\n311 758.1 25.3  27\n312 758.1 24.8  28\n313 706.6 12.2  78\n314 777.1 24.3  27\n315 777.1 19.7  41\n316 817.5 18.5  30\n317 739.4 18.6  24\n318 739.4 19.2  24\n319 783.5 21.6  27\n320 783.5 21.6  28\n321 783.5 18.9  34\n322 783.5 16.8  28\n323 783.5 16.8  28\n324 822.8 12.9  39\n325 726.9 13.7  56\n326 751.5 24.2  27\n327 751.5 24.1  27\n328 751.5 21.2  32\n329 751.5 19.7  35\n330 751.5 23.5  27\n331 751.5 24.2  27\n332 795.3 21.5  28\n333 795.3 17.1  41\n334 721.1 18.1  54\n335 764.0 18.0  51\n336 764.0  9.8  86\n337 764.0 19.3  44\n338 764.0 23.0  34\n339 764.0 22.7  35\n340 764.0 20.4  41\n341 764.0 19.3  44\n342 770.3 15.7  51\n343 807.1 20.6  37\n344 807.1 15.9  51\n345 807.1 12.2  66\n346 807.1 16.8  43\n347 807.1 21.3  35\n348 745.3 10.1  75\n349 745.3 17.4  57\n350 745.3 12.8  64\n351 745.3 10.1  75\n352 745.3 15.4  53\n353 745.3 20.6  43\n354 745.3 19.8  47\n355 745.3 18.7  50\n356 745.3 20.8  35\n357 745.3 20.8  35\n358 789.7 15.9  55\n359 789.7 19.7  39\n360 789.7 21.1  39\n361 789.7 18.4  42\n362 789.7 17.3  45\n363 732.3 15.2  64\n364 770.3 15.9  53\n365 770.3 21.1  35\n366 770.3 19.6  45\n367 812.1 15.9  38\n368 812.1 16.4  27\n369 744.4 16.8  47\n370 825.1 13.8  77\n371 825.1 13.8  77\n372 520.5 14.2  58\n373 664.5 10.4  75\n374 698.6 20.3  42\n375 855.3 10.3  78\n376 744.4 15.4  57\n377 672.6 21.1  54\n378 715.1 21.9  42\n379  30.6  8.7  51\n380 171.4  5.2 100\n381 458.8 19.3  39\n382 643.0 16.2  63\n383 690.0 28.2  29\n384 753.8 20.5  58\n385 819.1 21.3  44\n386 613.0 20.9  50\n387 750.5 20.6  55\n388  30.6 11.6  48\n389 706.7 23.3  34\n390 706.7 23.3  34\n391  58.3  7.5  71\n392 738.1 20.7  46\n393 825.1 21.9  43\n394  25.6 15.2  19\n395  46.7  5.3  68\n396  56.9 10.1  62\n397 750.5 20.4  55\n398 613.0 24.3  33\n399 715.1 25.9  32\n400 297.7 28.0  34\n401 297.7 28.0  34\n402 731.7 22.8  46\n403 706.7 25.0  36\n404 643.0 21.3  41\n405 725.1 21.8  34\n406 680.9 27.9  27\n407 860.6 17.0  67\n408  55.0 14.2  46\n409 855.3 19.9  44\n410 450.2 23.4  31\n411  52.8 14.7  42\n412  43.6  8.2  53\n413 442.1 22.8  27\n414 715.1 26.4  33\n415 723.1 24.1  50\n416 698.6 27.5  27\n417 575.8 26.3  39\n418  28.3 13.8  24\n419 664.5 24.9  42\n420 613.0 24.8  36\n421 635.9 26.2  36\n422 690.0 30.8  19\n423 795.9 29.3  27\n424 744.4 22.3  48\n425 715.1 26.9  31\n426 753.8 20.4  56\n427 753.8 20.4  56\n428 672.6 27.9  33\n429 698.6 26.2  34\n430 613.0 24.6  44\n431 849.3 19.4  45\n432 605.3 23.3  40\n433 698.6 23.9  38\n434 723.1 20.9  66\n435 811.2 22.2  45\n436 376.6 23.8  51\n437 672.6 26.8  35\n438 768.4 14.2  73\n439 715.1 23.6  53\n440 738.1 19.1  46\n441 855.3 16.2  58\n442 672.6 25.5  29\n443  41.6 10.9  64\n444 368.3 14.8  78\n445 855.3 16.2  58\n446 100.7 17.3  80\n447 664.5 19.1  70\n448  28.3  8.9  35\n449 844.0 10.5  77\n450 613.0 19.3  61\n451 690.0 23.4  49\n452 649.9 11.8  88\n453 730.6 17.7  65\n454 803.3 17.4  54\n455 753.8 16.8  56\n456 567.2 17.9  48\n457 753.8 16.6  59\n458 635.9 19.9  50\n459 715.1 18.9  64\n460 819.1 15.5  72\n461 715.1 18.9  64\n462 715.1 18.9  64\n463 825.1 14.5  76\n464  16.2  4.6  82\n465  16.2  5.1  77\n466  15.3  4.6  59\n467  36.9 10.2  45\n468  41.1 11.2  41\n469  43.5 13.3  27\n470  25.6 13.7  33\n471  25.6 17.6  27\n472  73.7 18.0  40\n473 229.0 14.3  79\n474 252.6 24.5  50\n475 316.7 26.4  35\n476 350.2 22.7  40\n477 395.0 27.2  28\n478 423.4 26.1  45\n479 423.4 18.2  82\n480 431.6 22.6  57\n481 560.0 30.2  25\n482 560.0 30.2  22\n483 587.1 23.4  40\n484 587.1 31.0  27\n485 587.1 33.1  25\n486 596.3 30.6  28\n487 605.8 24.1  43\n488 605.8 26.4  34\n489 605.8 19.4  71\n490 605.8 20.6  58\n491 605.8 28.7  33\n492 624.1 32.4  21\n493 633.6 32.4  27\n494 633.6 27.5  29\n495 643.0 30.8  30\n496 661.8 23.9  42\n497 661.8 32.6  26\n498 671.2 32.3  27\n499 671.2 33.3  26\n500 671.2 27.3  63\n501 671.2 21.6  65\n502 671.2 21.6  65\n503 671.2 20.7  69\n504 689.1 29.2  30\n505 689.1 28.9  29\n506 744.4 26.7  35\n507 752.6 18.5  73\n508 752.6 25.9  41\n509 752.6 25.9  41\n510 752.6 21.1  71\n511 752.6 18.2  62\n512 665.6 27.8  35\n513 665.6 27.8  32\n514 665.6 21.9  71\n515 665.6 21.2  70\n516 614.7 25.6  42\n517 106.7 11.8  31\n\n\n\n\n3.2.2 PCA\nMétodes basats en correlacions ens permeten detectar outliers\n\n\n3.2.3 Distancia de Mahalanobis\nMedeix la distancia de un punt respecte a la mitjana considerant la covariança\n\ndistancia_mahalanobis &lt;- mahalanobis(dades, colMeans(dades), cov(dades))\n\nGrafiquem el plot de la densitat de les distancies\n\nplot(density(distancia_mahalanobis))\n\n\n\n\n\n\n\n\nEs mostren els valors de la bbdd que queden per sobre de el 99% de la distribució chi-cuadrat\n\ncutoff &lt;- qchisq(p = 0.99, df = ncol(dades))\ndades[distancia_mahalanobis&gt;cutoff, ]\n\n       DC temp  RH\n4    77.5  8.3  97\n5   102.2 11.4  99\n212 594.2  5.1  96\n277 349.7  4.6  21\n278 349.7  4.6  21\n279 349.7  4.6  21\n280 349.7  4.6  21\n282 349.7  5.1  24\n305 113.8 11.3  94\n380 171.4  5.2 100\n446 100.7 17.3  80\n\n\nOrdenamos de forma decreciente, según el score de Mahalanobis\n\ndades &lt;- dades[order(distancia_mahalanobis, decreasing = TRUE),]\n\nVisualitzem l’histograma de les distancies per veure on tallem els outliers\n\npar(mfrow=c(1,1))\nhist(distancia_mahalanobis)\n\n\n\n\n\n\n\n\nDescartamos los outliers según un umbral\n\numbral &lt;- 8\ndades[, \"outlier\"] &lt;- (distancia_mahalanobis &gt; umbral)\n\ndades[, \"color\"] &lt;- ifelse(dades[, \"outlier\"], \"red\", \"black\")\nscatterplot3d(dades[, \"DC\"], dades[, \"temp\"], dades[, \"RH\"], \n              color = dades[, \"color\"])\n\n\n\n\n\n\n\n(fig &lt;- plotly::plot_ly(dades, x = ~DC, y = ~temp, z = ~RH, \n                       color = ~color, colors = c('#0C4B8E', '#BF382A')) %&gt;% \n                        add_markers())\n\n\n\n\n(quienes &lt;- which(dades[, \"outlier\"] == TRUE))\n\n [1]   4   5  76 105 212 240 277 278 279 280 281 282 283 287 300 305 336 375 380\n[20] 446 464 465 479 500\n\n\n\n3.2.3.1 Mahalanobis Robusto\n\nlibrary(chemometrics)\n\ndis &lt;- chemometrics::Moutlier(dades[, c(\"DC\", \"temp\", \"RH\")], quantile = 0.99, plot = TRUE)\n\n\n\n\n\n\n\npar(mfrow = c(1, 1))\nplot(dis$md, dis$rd, type = \"n\")\ntext(dis$md, dis$rd, labels = rownames(dades))\n\n\n\n\n\n\n\na &lt;- which(dis$rd &gt; 7)\nprint(a)\n\n277 278 279 280   5 282   4 305 380 446 240 464 105  76 465 394 466 284 166 448 \n  1   2   3   4   5   6   7   8   9  10  12  15  17  18  20  31  35  37  39  40 \n395 418 112  98 391 119 177 197 203 285 242 469 205  62  78 471 117 443  73  92 \n 41  43  46  48  49  51  52  53  54  55  60  63  66  67  68  69  72  74  83  84 \n517 470  90 116 169 115 379 412  70  17 118 191 396 220  49 107 241 215 216 467 \n 85  86  87  88  90  91  92  94  95  96  97  99 100 103 105 107 108 109 110 113 \n468  71   1  20 472 161  77 388  59  97 183  60 127 190  19 163 106 411  61 187 \n118 119 121 122 123 126 128 131 132 133 137 139 140 144 145 146 147 150 151 152 \n408 132 135 111 202  40 133 147  50 214 223 131 189 \n154 157 158 159 163 164 165 169 170 171 172 173 177 \n\n\n\n\n\n3.2.4 Regresió Lineal i residus\nUn punt amb un residu gran pot considerar-se un outlier\n\n\n3.2.5 Distancia de Cook\nIdentifica punts amb gran influència en la regresió. Un valor de Cook D_i &gt; 1 és un outliers.\n\n\n3.2.6 K-Nearest Neighbors (KNN) Outlier Score\nBasats en la densitat local de les dades\n\nlibrary(adamethods)\n\ndo_knno(dades[, c(\"DC\", \"temp\", \"RH\")], k=1, top_n = 30)\n\n [1]   9 149  64  65   7  33  73  79 130  32  16  19 274 167 198  22 142  63 106\n[20]  35  11 273 245 227   5   8  13  27  10 279\n\n\n\n\n3.2.7 Local Outlier Factor (LOF)\nCompara la densidat de un punt amb la densidat dels seus veïns. Un valor LOF alt\n\nlibrary(DMwR2)\nlibrary(dplyr)\n\noutlier.scores &lt;- lofactor(dades[, c(\"DC\", \"temp\", \"RH\")], k = 5)\npar(mfrow=c(1,1))\nplot(density(outlier.scores))\noutlier.scores\noutliers &lt;- order(outlier.scores, decreasing=T)\noutliers &lt;- order(outlier.scores, decreasing=T)[1:5]\n\nAprofitarem el ACP per poder visualizar els outliers\n\nn &lt;- nrow(dades[, c(\"DC\", \"temp\", \"RH\")]); labels &lt;- 1:n; labels[-outliers] &lt;- \".\"\nbiplot(prcomp(dades[, c(\"DC\", \"temp\", \"RH\")]), cex = .8, xlabs = labels)\n\nGrafiquem les correlacions per veure els gráfics\n\npch &lt;- rep(\".\", n)\npch[outliers] &lt;- \"+\"\ncol &lt;- rep(\"black\", n)\ncol[outliers] &lt;- \"red\"\npairs(dades[, c(\"DC\", \"temp\", \"RH\")], pch = pch, col = col)\n\nHo visualitzem en 3D\n\nplot3d(dades[, \"DC\"], dades[, \"temp\"], dades[, \"RH\"], type = \"s\", col = col, size = 1)\n\n\n3.2.7.1 Nueva versión de LOF\n\nlibrary(Rlof)\noutliers.scores &lt;- Rlof::lof(dades[, c(\"DC\", \"temp\", \"RH\")], k = 5)\nplot(density(outliers.scores))\n\n\n\n\n\n\n\n#outlier.scores &lt;- lof(dades[, c(\"DC\", \"temp\", \"RH\")], k=c(5:10))\n\n\n\n\n3.2.8 Isolation Forest\n\n### Cargamos las librerias necesarias\nlibrary(R.matlab)   # Lectura de archivos .mat\nlibrary(solitude)   # Modelo isolation forest\nlibrary(tidyverse)  # Preparación de datos y gráficos\nlibrary(MLmetrics)\n\n# Carreguem les dades\ncardio_mat  &lt;- readMat(\"https://www.dropbox.com/s/galg3ihvxklf0qi/cardio.mat?dl=1\")\ndf_cardio   &lt;- as.data.frame(cardio_mat$X)\ndf_cardio$y &lt;- as.character(cardio_mat$y)\ndatos &lt;- df_cardio\n\n\nisoforest &lt;- isolationForest$new(\n  sample_size = as.integer(nrow(datos)/2),\n  num_trees   = 500, \n  replace     = TRUE,\n  seed        = 123\n)\nisoforest$fit(dataset = datos %&gt;% select(-y))\n\nAra anem a realitzar les prediccions.\n\npredicciones &lt;- isoforest$predict(\n  data = datos %&gt;% select(-y)\n)\nhead(predicciones)\n\n      id average_depth anomaly_score\n   &lt;int&gt;         &lt;num&gt;         &lt;num&gt;\n1:     1         9.816     0.5875006\n2:     2         9.738     0.5899888\n3:     3         9.528     0.5967405\n4:     4         9.728     0.5903086\n5:     5         9.860     0.5861016\n6:     6         9.796     0.5881376\n\n\n\nggplot(data = predicciones, aes(x = average_depth)) +\n  geom_histogram(color = \"gray40\") +\n  geom_vline(\n    xintercept = quantile(predicciones$average_depth, seq(0, 1, 0.1)),\n    color      = \"red\",\n    linetype   = \"dashed\") +\n  labs(\n    title = \"Distribución de las distancias medias del Isolation Forest\",\n    subtitle = \"Cuantiles marcados en rojo\"  ) +\n  theme_bw() +\n  theme(plot.title = element_text(size = 11))\n\n\n\n\n\n\n\ncuantiles &lt;- quantile(x = predicciones$average_depth, probs = seq(0, 1, 0.05))\ncuantiles\n\n   0%    5%   10%   15%   20%   25%   30%   35%   40%   45%   50%   55%   60% \n7.556 8.952 9.264 9.451 9.548 9.629 9.694 9.744 9.790 9.824 9.852 9.870 9.888 \n  65%   70%   75%   80%   85%   90%   95%  100% \n9.904 9.920 9.934 9.948 9.958 9.970 9.980 9.998 \n\n\n\n\n3.2.9 TIPS: Detección de anomalías\nUna vez que la distancia de separación ha sido calculado, se puede emplear como criterio para identificar anomalías. Asumiendo que las observaciones con valores atípicos en una o más de sus variables se separan del resto con mayor facilidad, aquellas observaciones con menor distancia promedio deberían ser las más atípicas.\nEn la práctica, si se está empleando esta estrategia de detección es porque no se dispone de datos etiquetados, es decir, no se conoce qué observaciones son realmente anomalías. Sin embargo, como en este ejemplo se dispone de la clasificación real, se puede verificar si realmente los datos anómalos tienen menores distancias.\n\ndatos &lt;- datos %&gt;%\n  bind_cols(predicciones)\n\nggplot(data = datos,\n       aes(x = y, y = average_depth)) +\n  geom_jitter(aes(color = y), width = 0.03, alpha = 0.3) + \n  geom_violin(alpha = 0) +\n  geom_boxplot(width = 0.2, outlier.shape = NA, alpha = 0) +\n  stat_summary(fun = \"mean\", colour = \"orangered2\", size = 3, geom = \"point\") +\n  labs(title = \"Distancia promedio en el modelo Isolation Forest\",\n       x = \"clasificación (0 = normal, 1 = anomalía)\",\n       y = \"Distancia promedio\") +\n  theme_bw() + \n  theme(legend.position = \"none\",\n        plot.title = element_text(size = 11)\n  )\n\n\n\n\n\n\n\n\nLa distancia promedio en el grupo de las anomalías (1) es claramente inferior. Sin embargo, al existir solapamiento, si se clasifican las n observaciones con menor distancia como anomalías, se incurriría en errores de falsos positivos.\nAcorde a la documentación, el set de datos Cardiotocogrpahy contiene 176 anomalías. Véase la matriz de confusión resultante si se clasifican como anomalías las 176 observaciones con menor distancia predicha.\n\nresultados &lt;- datos %&gt;%\n  select(y, average_depth) %&gt;%\n  arrange(average_depth) %&gt;%\n  mutate(clasificacion = if_else(average_depth &lt;= 8.5, \"1\", \"0\"))\n\nmat_confusion &lt;- MLmetrics::ConfusionMatrix(\n  y_pred = resultados$clasificacion,\n  y_true = resultados$y)\n\nmat_confusion\n\n      y_pred\ny_true    0    1\n     0 1638   17\n     1  158   18"
  },
  {
    "objectID": "material/Preprocessing/AdvancedPreprocessing.html#cas-general",
    "href": "material/Preprocessing/AdvancedPreprocessing.html#cas-general",
    "title": "Advance Preprocessing",
    "section": "2.3 Cas general",
    "text": "2.3 Cas general\n\nlibrary(mvoutlier)\ndades2 &lt;- dades; Y &lt;- as.matrix(dades2)\ndistances &lt;- dd.plot(Y,quan=1/2, alpha=0.025)\n\n\n\n\n\n\n\ndistances$md.cla\n\n  [1] 2.1512048 1.3379825 2.1172116 3.8246066 4.1575569 1.0214128 1.2694422\n  [8] 2.7595383 1.6355391 0.7435067 0.8384635 0.9832367 1.8368465 0.5815850\n [15] 1.5811197 0.8448810 2.2378066 0.8891432 2.0349346 2.1415420 0.9637216\n [22] 1.0652484 2.0207355 0.1516285 0.9211263 0.9991636 1.0865045 0.6181512\n [29] 1.9793545 0.8189627 1.3860142 2.3444645 0.8378744 1.2409837 1.1152316\n [36] 1.3406612 1.4257126 1.8975023 1.8395299 1.9442325 1.5246648 0.5432011\n [43] 0.8329662 0.6613403 2.1071830 1.4175912 1.7140652 1.1449734 2.2130392\n [50] 1.9026888 2.1219783 0.7044810 0.4459360 0.4459360 0.7512874 1.6047252\n [57] 1.3885461 1.5468641 2.1002115 2.0633162 2.0167931 2.4299533 0.7010824\n [64] 0.8008038 0.9015124 0.6025597 0.7374479 1.0558439 1.0558439 2.2428749\n [71] 2.1585402 1.2384809 2.3062269 0.6858624 1.5046259 2.9334983 2.1118951\n [78] 2.4255957 1.3308234 1.6266383 0.4465482 0.5872994 0.5867873 0.8654335\n [85] 0.4631097 2.1760160 1.2790069 0.8809271 1.1237454 2.2646990 1.1262982\n [92] 2.2986587 0.8051529 0.4352584 1.4074557 1.3971864 2.0931182 2.5313223\n [99] 2.6371026 0.4134452 0.4134452 1.4052058 0.4979006 1.0797643 2.9624567\n[106] 2.0237801 2.2067698 0.7659111 0.6120329 1.0306586 1.9840817 2.5398227\n[113] 0.7583594 1.2612702 2.2550289 2.2630901 2.3986605 2.2333581 2.5154080\n[120] 0.6164195 2.0776908 0.7059860 0.6907049 0.7447968 0.6961495 1.6092699\n[127] 2.0621329 1.2085780 1.1273125 1.8393167 1.8994420 1.9919912 1.9422520\n[134] 0.9727646 1.9913779 0.5248409 1.0284115 1.4808298 1.0140768 0.6869463\n[141] 0.6472623 1.0173741 1.0022887 1.2393227 1.0774778 1.7008576 1.9173406\n[148] 1.1253466 0.8016034 1.2873884 1.2708888 1.7937650 1.7535716 0.6123247\n[155] 1.8615127 0.8772978 1.6878607 0.5740120 0.7635284 1.0865045 2.1190046\n[162] 0.2598342 2.0279375 0.9019672 1.5828523 2.6275057 0.8654335 0.8812522\n[169] 2.2615419 0.4126382 0.8242294 0.5192159 0.3914657 1.1041732 1.1545360\n[176] 0.4488220 2.5007877 0.3735133 0.9842039 1.4052058 0.9619567 1.1900212\n[183] 2.0712298 1.0688091 0.6509067 0.8710078 2.0167931 0.5721858 1.8734132\n[190] 2.0455397 2.2223974 1.1169353 1.4160636 1.3115436 1.4160636 0.8402910\n[197] 2.5007877 2.1760160 1.4576505 0.6739897 0.9174610 1.9459779 2.4916974\n[204] 0.7477589 2.4314093 0.8863790 0.8707309 0.7715035 1.0558439 1.2380805\n[211] 1.8688756 3.4063203 0.6123247 1.9026888 2.1987798 2.1987798 1.1310435\n[218] 2.0474931 2.1657213 2.2160825 1.0026014 0.8037506 1.9026888 2.4507631\n[225] 1.3683752 0.8261645 1.2511671 0.6967559 1.6506914 0.5608519 1.5811197\n[232] 1.5634876 0.7688811 0.9334464 1.9579191 0.3124014 0.6834758 0.8748825\n[239] 1.2285607 3.1928408 2.1998017 2.4581678 1.3489065 2.4697383 1.2347779\n[246] 1.4312779 1.2250285 2.3230787 1.7396325 0.5648614 1.4690388 0.8842203\n[253] 0.6571482 0.4842651 0.4929435 0.7917183 1.0180851 0.5284808 1.2853576\n[260] 0.8651401 1.9285065 2.4490604 0.4759367 0.5746102 1.0670200 1.3483202\n[267] 0.8323413 1.3050887 0.9206251 0.7215715 1.2665824 1.1392373 0.5009500\n[274] 2.5915741 2.4562117 2.4533198 4.3204916 4.3204916 4.3204916 4.3204916\n[281] 3.1279535 4.0240805 2.9005429 2.6536444 2.4875865 1.4261032 2.9289704\n[288] 1.2602955 1.2582225 1.2913573 0.8064294 1.5973586 1.1013307 1.9199488\n[295] 1.6791747 1.3933199 1.0945785 1.0435959 0.9079748 3.1489768 1.6067994\n[302] 1.1813530 1.4983957 1.4983957 3.7793529 0.8403912 1.0098248 0.9263502\n[309] 1.5109206 1.3378955 1.3510446 1.2935832 2.2628304 1.3807921 1.0992508\n[316] 2.0043758 2.0723786 1.9693414 1.5822400 1.5224026 1.5667415 2.3115444\n[323] 2.3115444 2.7030650 1.6165510 1.3156217 1.3162506 1.2162273 1.2460084\n[330] 1.3295084 1.3156217 1.5777655 1.6136293 0.9647550 1.1032109 2.9002933\n[337] 1.0052333 1.0565707 1.0247083 0.9592401 1.0052333 1.4799833 1.2766433\n[344] 1.6119247 2.2102537 1.6489460 1.2846146 2.4626101 1.1856617 1.8722498\n[351] 2.4626101 1.4018743 0.8164050 0.8427082 0.9426092 1.0749215 1.0749215\n[358] 1.5076101 1.2309112 1.0752235 1.3174101 1.4108121 1.5390601 1.4262600\n[365] 1.1474900 0.9851802 2.0608206 2.5616952 1.2350960 2.3991992 2.3991992\n[372] 0.9636042 2.2156559 0.6405978 2.8472072 1.4027365 1.0146283 0.7065577\n[379] 2.2524889 3.6873905 0.5056023 1.2273610 1.6242278 1.2826760 1.1054025\n[386] 0.7217604 1.1213620 2.1008998 0.9016456 0.9016456 2.5306142 0.7783752\n[393] 1.1204441 2.7312638 2.6070900 2.2217968 1.1074819 0.9756355 1.2224428\n[400] 2.7386289 2.7386289 0.9215108 1.0612070 0.4646294 0.9617825 1.5893047\n[407] 1.9402502 2.0059941 1.3786885 1.2066435 2.0204233 2.2514669 1.2539158\n[414] 1.2942327 1.3207677 1.5193393 1.5408538 2.5836481 1.1581001 1.0629563\n[421] 1.3292082 2.1547964 1.8064456 0.9532013 1.3835120 1.1658931 1.1658931\n[428] 1.6023373 1.2601133 1.2353069 1.3882029 0.8093320 0.8875183 1.8162538\n[435] 1.0718102 2.1192709 1.3990278 2.0596422 1.3930155 0.8712025 1.7654778\n[442] 1.2064497 2.3516796 2.3766534 1.7654778 3.6831554 1.8781162 2.6231962\n[449] 2.7624742 1.2571769 1.1315694 2.7292269 1.4988128 1.3645685 1.2487224\n[456] 0.2760227 1.3354776 0.5711344 1.4684949 2.0850998 1.4684949 1.4684949\n[463] 2.3204115 3.1150249 2.9179968 2.6863547 2.1762693 2.1608622 2.4512056\n[470] 2.2700598 2.4203373 2.1340683 2.7610866 2.7243416 2.3384447 1.5379530\n[477] 2.1017212 2.1939930 2.9836682 1.9994740 2.2210057 2.2179387 0.8614322\n[484] 2.3467527 2.7820672 2.2514710 1.0877247 1.3927841 2.0228082 1.2372504\n[491] 1.8863196 2.5240480 2.5562991 1.5504217 2.2311298 0.9425752 2.5338080\n[498] 2.4657745 2.6692880 2.8486431 1.8598956 1.8598956 2.0117051 1.8215322\n[505] 1.7567235 1.3587946 2.0685362 1.3162443 1.3162443 2.1979471 1.3911676\n[512] 1.6202613 1.5794635 2.3509350 2.1643692 1.3910454 2.2803773\n\ndistances$md.rob\n\n  [1] 8.2157063 1.6301206 2.5927006 8.7074405 8.4305593 2.4131242 2.2677777\n  [8] 3.5465258 1.7618316 0.5640203 0.7009379 0.8879920 2.2292868 0.1551463\n [15] 1.6122568 0.8823011 7.8819613 1.0741592 7.6935630 8.1537779 0.9463274\n [22] 0.9930106 5.6883551 1.7830105 1.1389196 1.7054315 1.2689717 0.3929892\n [29] 2.2821365 0.8087114 1.3567522 2.5487554 0.6616880 1.2461023 1.0917446\n [36] 1.5038041 1.5930573 2.1694506 2.2059985 7.8152918 3.6288603 1.0804302\n [43] 1.1331551 0.4178624 2.2342008 1.3909220 1.9335813 2.7410709 8.3315088\n [50] 7.7723462 2.4649409 0.8600525 0.3581614 0.3581614 0.9090681 1.6896377\n [57] 1.5454964 1.7296222 8.2698337 8.2217135 7.8649425 8.4360084 2.8650931\n [64] 1.0618142 0.8732263 0.6670303 0.4821334 1.2010922 1.2010922 7.6603786\n [71] 7.5939776 1.3602058 7.9060422 1.3889142 1.6766843 9.0342033 7.9236794\n [78] 7.9552566 2.6860109 2.4573528 1.1848045 1.0624264 0.7875436 1.1490070\n [85] 0.3059888 2.5202367 1.2684147 0.7229385 1.3017097 7.6246013 1.6925358\n [92] 7.6522631 2.4586887 1.2914784 1.5606376 1.5774550 8.3501267 8.3858393\n [99] 3.4202364 1.1279366 1.1279366 2.0225251 0.5860946 0.9498146 9.3747753\n[106] 8.0042550 7.5080277 0.9509607 0.2662519 0.9717070 7.9816607 7.6885325\n[113] 0.5881634 1.2813352 7.9291458 7.9707388 8.3405097 7.8674589 8.4531841\n[120] 2.7190157 2.8323876 1.4897117 0.4801052 0.7783254 0.5601142 1.8191839\n[127] 8.0224812 1.1488887 1.3199856 2.1943586 7.4800868 7.9251227 7.8497015\n[134] 1.0134693 7.7967344 1.3976338 1.1000734 1.6510473 4.5524693 0.3744225\n[141] 0.2942177 1.9978997 1.9915017 4.8510143 1.9650389 1.7813716 7.7651241\n[148] 1.0128859 1.1389993 1.2677583 2.8594690 3.7699709 3.0964572 0.3419226\n[155] 2.1672916 1.2420105 1.9283729 0.7278137 0.8383210 1.2689717 7.6229208\n[162] 1.2436442 7.5391699 0.9694854 1.8389077 8.8702041 1.1490070 1.3377451\n[169] 7.8021768 1.2243510 0.6254693 1.2536495 1.0758740 1.2006624 1.7968090\n[176] 0.8097497 8.9738594 0.7656994 0.8372706 2.0225251 0.7872045 1.4281055\n[183] 8.1349516 1.0719965 1.3511167 0.7897428 7.8649425 0.1006548 7.6830528\n[190] 7.7041955 8.0468280 1.4253214 1.4455696 1.2942278 1.4455696 1.1315623\n[197] 8.9738594 2.5202367 1.5507476 0.3998531 0.7934616 7.7225329 8.6898201\n[204] 0.6729101 7.3401343 0.7313357 1.2005866 0.5753968 1.2010922 1.2191654\n[211] 2.2341036 4.3439349 0.3419226 7.7723462 7.6226334 7.6226334 1.2376389\n[218] 2.1997155 2.5641247 7.8035716 1.4484463 0.5987993 7.7723462 5.4377862\n[225] 1.2912976 0.8264241 1.3381767 2.5574766 1.9021290 2.8662752 1.6122568\n[232] 1.7948921 0.6174913 0.7730502 2.4187140 1.0869098 0.6461858 0.8002264\n[239] 1.1237513 8.6804322 7.9203264 7.8232250 2.1318851 3.1432113 1.4900889\n[246] 1.6284259 1.3516616 3.3785762 1.7661100 0.2235938 1.3926630 0.6973112\n[253] 0.3886249 1.3555949 1.0294052 0.7528347 1.3244015 1.1808389 1.5292418\n[260] 0.7777983 2.1524233 3.4065074 1.2438199 0.9814537 1.2219665 2.0478026\n[267] 1.3464787 1.3297142 0.9883371 0.8295569 1.5217910 1.3754096 0.4405954\n[274] 6.1191808 5.9436337 5.9674829 7.9240497 7.9240497 7.9240497 7.9240497\n[281] 6.6707462 7.6051455 6.4542151 9.3221740 9.2822318 3.1220294 4.7475686\n[288] 2.5612214 2.3893762 2.4146280 2.6506927 2.5009851 2.4807670 2.9676485\n[295] 3.5326586 3.0298057 5.2123227 5.0776808 3.6566395 6.7286092 4.7921000\n[302] 4.6577486 5.4328510 5.4328510 8.1145477 0.7352526 0.8870954 0.8683079\n[309] 1.4948587 1.5921329 1.6257463 1.5461321 2.4944242 1.7285717 1.2036855\n[316] 2.3519796 2.3716572 2.2385864 1.8657511 1.7978546 1.7387780 2.6272422\n[323] 2.6272422 2.9709197 1.6027909 1.4991915 1.4963063 1.2920596 1.2827263\n[330] 1.4920590 1.4991915 1.9104816 1.7320072 0.8777790 1.0878941 3.0972045\n[337] 1.0095365 1.2710524 1.2243430 1.0216210 1.0095365 1.4306690 1.6256760\n[344] 1.6944177 2.2387272 1.7907724 1.6777179 2.5400737 1.1772680 1.8565204\n[351] 2.5400737 1.3144652 0.7736309 0.7922019 0.8656220 1.0775349 1.0775349\n[358] 1.5327420 1.4134151 1.3608060 1.4303565 1.4745843 1.5632731 1.3756478\n[365] 1.2928977 1.0560837 2.2734361 2.9506793 1.1408481 2.7494115 2.7494115\n[372] 2.6585856 2.5915964 0.2859186 3.0384855 1.3264608 1.1576615 0.5548502\n[379] 8.8379406 8.0047752 2.7728449 1.5869296 1.7102709 1.6361218 1.6955705\n[386] 1.0860546 1.3977195 8.4811265 0.7680764 0.7680764 8.5500507 0.7492957\n[393] 1.8107260 8.7268752 8.9704335 8.2617904 1.3615027 0.9947617 1.3039687\n[400] 4.5322201 4.5322201 1.1056697 1.0836389 0.4212128 0.8857965 1.6000059\n[407] 2.6049474 7.9135709 2.0553305 2.6971923 7.9288514 8.7374402 2.9247081\n[414] 1.4293907 1.6915854 1.5751506 1.7459313 8.7440972 1.1963003 1.0766232\n[421] 1.3067784 2.2908143 2.6026554 1.2100382 1.5260517 1.4601269 1.4601269\n[428] 1.6726862 1.3123943 1.3896346 1.9752446 0.9759090 0.7983458 2.3001027\n[435] 1.7262508 3.8221951 1.4273152 2.2326354 1.7639182 0.7243175 2.1746276\n[442] 1.0879861 8.3518095 4.7687156 2.1746276 7.7082555 2.3184326 9.2029703\n[449] 2.9222732 1.7396517 1.3004394 3.2912419 1.7013126 1.5480203 1.2065761\n[456] 1.6118910 1.3402572 0.8055922 1.7284307 2.4764389 1.7284307 1.7284307\n[463] 2.7157733 9.4021626 9.3190081 9.5136889 8.6514786 8.5380896 8.5611742\n[470] 8.5336824 8.1756216 7.3768976 6.2780301 5.2036864 4.2458609 3.8429164\n[477] 3.3654341 3.3213773 4.6178209 3.4097509 2.2935763 2.2743959 1.1648570\n[484] 2.4055707 2.8867780 2.3051858 1.2552220 1.4103238 2.6423464 1.7033756\n[491] 1.9444397 2.5749001 2.7090937 1.4875413 2.3631233 0.8971715 2.7553091\n[498] 2.7178706 2.9674546 3.5872517 2.3123520 2.3123520 2.5043740 1.9885769\n[505] 1.8866382 1.7367897 2.5726948 1.8058369 1.8058369 2.8863013 1.6014070\n[512] 1.7055864 1.6057526 2.9520098 2.7068229 1.5144614 7.9984070\n\nres &lt;- aq.plot(Y,delta=qchisq(0.975,df=ncol(Y)),quan=1/2,alpha=0.05)\n\nProjection to the first and second robust principal components.\nProportion of total variation (explained variance): 0.9692463\n\n\n\n\n\n\n\n\nstr(res)\n\nList of 1\n $ outliers: logi [1:517] TRUE FALSE FALSE TRUE TRUE FALSE ...\n\nres$outliers\n\n  [1]  TRUE FALSE FALSE  TRUE  TRUE FALSE FALSE  TRUE FALSE FALSE FALSE FALSE\n [13] FALSE FALSE FALSE FALSE  TRUE FALSE  TRUE  TRUE FALSE FALSE  TRUE FALSE\n [25] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [37] FALSE FALSE FALSE  TRUE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [49]  TRUE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE  TRUE\n [61]  TRUE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE  TRUE FALSE\n [73]  TRUE FALSE FALSE  TRUE  TRUE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE\n [85] FALSE FALSE FALSE FALSE FALSE  TRUE FALSE  TRUE FALSE FALSE FALSE FALSE\n [97]  TRUE  TRUE  TRUE FALSE FALSE FALSE FALSE FALSE  TRUE  TRUE  TRUE FALSE\n[109] FALSE FALSE  TRUE  TRUE FALSE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE\n[121] FALSE FALSE FALSE FALSE FALSE FALSE  TRUE FALSE FALSE FALSE  TRUE  TRUE\n[133]  TRUE FALSE  TRUE FALSE FALSE FALSE  TRUE FALSE FALSE FALSE FALSE  TRUE\n[145] FALSE FALSE  TRUE FALSE FALSE FALSE FALSE  TRUE FALSE FALSE FALSE FALSE\n[157] FALSE FALSE FALSE FALSE  TRUE FALSE  TRUE FALSE FALSE  TRUE FALSE FALSE\n[169]  TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE FALSE FALSE FALSE\n[181] FALSE FALSE  TRUE FALSE FALSE FALSE  TRUE FALSE  TRUE  TRUE  TRUE FALSE\n[193] FALSE FALSE FALSE FALSE  TRUE FALSE FALSE FALSE FALSE  TRUE  TRUE FALSE\n[205]  TRUE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE FALSE  TRUE  TRUE  TRUE\n[217] FALSE FALSE FALSE  TRUE FALSE FALSE  TRUE  TRUE FALSE FALSE FALSE FALSE\n[229] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE\n[241]  TRUE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n[253] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE FALSE FALSE\n[265] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE  TRUE  TRUE\n[277]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE  TRUE FALSE\n[289] FALSE FALSE FALSE FALSE FALSE FALSE  TRUE FALSE  TRUE  TRUE  TRUE  TRUE\n[301]  TRUE  TRUE  TRUE  TRUE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n[313] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n[325] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n[337] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n[349] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n[361] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n[373] FALSE FALSE FALSE FALSE FALSE FALSE  TRUE  TRUE FALSE FALSE FALSE FALSE\n[385] FALSE FALSE FALSE  TRUE FALSE FALSE  TRUE FALSE FALSE  TRUE  TRUE  TRUE\n[397] FALSE FALSE FALSE  TRUE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE\n[409] FALSE FALSE  TRUE  TRUE FALSE FALSE FALSE FALSE FALSE  TRUE FALSE FALSE\n[421] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n[433] FALSE FALSE FALSE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE  TRUE\n[445] FALSE  TRUE FALSE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n[457] FALSE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE\n[469]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE FALSE  TRUE  TRUE\n[481] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n[493] FALSE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE FALSE FALSE FALSE FALSE\n[505] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n[517]  TRUE\n\ntable(res$outliers)\n\n\nFALSE  TRUE \n  388   129 \n\n#windows()\npar(mfrow=c(1, 1))\nlibrary(MVN)\n# mvnoutliers &lt;- mvn(dades, multivariateOutlierMethod = \"adj\", showOutliers = TRUE, \n#                   showNewData = TRUE)\nmvnoutliers &lt;- mvn(data = dades, mvn_test = \"royston\", \n                   univariate_test = \"AD\", \n              multivariate_outlier_method = \"adj\",\n              show_new_data = TRUE)\n\nVisualitzem tots els outliers detectats com a true\n\nsummary(mvnoutliers, select = \"outliers\")\n\n    Observation Mahalanobis.Distance\n1           466               95.620\n2           464               92.910\n3           105               92.347\n4           284               91.668\n5           465               91.312\n6           285               90.872\n7           448               89.349\n8            76               85.582\n9           177               85.110\n10          197               85.110\n11          395               84.820\n12          166               82.912\n13          379               82.172\n14          412               80.371\n15          418               80.180\n16          394               79.742\n17          203               79.121\n18            4               79.053\n19          467               78.658\n20          240               78.246\n21          469               76.910\n22          391               76.746\n23          468               76.551\n24          470               76.236\n25          119               75.377\n26          388               75.344\n27           62               75.271\n28            5               73.796\n29          117               73.244\n30           98               73.243\n31           49               72.952\n32          443               72.902\n33           97               72.889\n34          396               71.492\n35           59               71.469\n36            1               71.192\n37           60               70.684\n38           20               70.109\n39          471               69.553\n40          183               69.161\n41          305               68.417\n42          191               68.156\n43          127               67.840\n44          277               67.386\n45          278               67.386\n46          279               67.386\n47          280               67.386\n48          517               67.373\n49          380               67.118\n50          106               66.945\n51          111               66.582\n52          116               66.430\n53           78               66.389\n54          115               65.699\n55          132               65.676\n56          411               65.587\n57           73               65.387\n58           77               65.351\n59          408               65.339\n60          241               65.243\n61           61               65.055\n62          187               65.055\n63           17               65.028\n64          133               64.804\n65          118               64.776\n66          242               63.968\n67          169               63.822\n68           40               63.798\n69          220               63.737\n70           50               63.488\n71          214               63.488\n72          223               63.488\n73          135               63.471\n74          147               63.165\n75          202               62.350\n76          190               62.077\n77          282               62.062\n78          189               61.798\n79           19               61.726\n80          112               61.540\n81          446               61.359\n82           70               61.172\n83           92               61.051\n84          161               60.679\n85           90               60.596\n86          215               60.585\n87          216               60.585\n88           71               60.301\n89          163               59.452\n90          107               58.594\n91          131               58.351\n92          472               56.439\n93          205               55.917\n94          281               47.755\n95          300               47.138\n96          283               44.684\n97          473               40.862\n98          274               40.080\n99          276               38.049\n100         275               37.750\n101          23               33.308\n102         224               30.706\n103         303               30.572\n104         304               30.572\n105         297               28.501\n106         474               27.684\n107         298               26.969\n108         144               24.440\n109         301               23.821\n110         444               23.544\n111         287               23.359\n112         302               22.481\n113         479               21.975\n114         139               21.533\n115         400               20.893\n116         401               20.893\n117         212               19.743\n118         475               18.348\n119         476               15.110\n120         436               14.908\n121         152               14.508\n122         299               13.807\n123         500               13.600\n124          41               13.455\n125           8               13.137\n126         295               12.864\n127          99               12.083\n\n\nVisualitzem les variables originals quines han donat els que no son outliers\n\nsummary(mvnoutliers, select = \"new_data\")\n\n       DC temp RH\n2   669.1 18.0 33\n3   686.9 14.6 33\n6   488.0 22.2 29\n7   495.6 24.1 27\n9   692.6 13.1 63\n10  698.6 22.8 40\n11  698.6 17.8 51\n12  713.0 19.3 38\n13  665.3 17.0 72\n14  686.5 21.3 42\n15  699.6 26.4 21\n16  713.9 22.9 44\n18  664.2 16.7 47\n21  692.6 18.3 40\n22  724.3 19.1 38\n24  537.4 19.5 43\n25  594.2 23.7 32\n26  601.4 16.3 60\n27  668.0 19.0 34\n28  686.5 19.4 48\n29  721.4 30.2 24\n30  728.6 22.8 39\n31  692.3 25.4 24\n32  709.9 11.2 78\n33  706.8 20.6 37\n34  718.3 17.7 39\n35  724.3 21.2 32\n36  730.2 18.2 62\n37  669.1 21.7 24\n38  682.6 11.3 60\n39  686.9 17.8 27\n42  624.2 18.4 42\n43  647.1 16.6 54\n44  698.6 19.6 48\n45  735.7 12.9 74\n46  692.3 25.9 24\n47  686.5 14.7 70\n48  442.9 23.0 36\n51  706.4 20.8 17\n52  631.2 21.5 34\n53  654.1 20.4 42\n54  654.1 20.4 42\n55  661.3 17.6 45\n56  706.4 27.7 24\n57  730.2 17.8 63\n58  691.8 13.8 50\n63  466.6 18.8 35\n64  631.2 20.8 33\n65  638.8 23.1 31\n66  661.3 18.6 44\n67  668.0 23.0 37\n68  668.0 19.6 33\n69  668.0 19.6 33\n72  692.3 17.7 37\n74  614.5 17.3 43\n75  713.9 27.6 30\n79  529.8 14.7 66\n80  561.6 21.6 19\n81  601.4 19.5 39\n82  631.2 17.9 44\n83  647.1 18.6 51\n84  654.1 16.6 47\n85  661.3 20.2 45\n86  706.4 21.5 15\n87  706.4 25.4 27\n88  706.4 22.4 34\n89  728.6 25.3 36\n91  624.2 14.7 59\n93  488.0 20.8 32\n94  601.4 18.2 43\n95  638.8 23.4 22\n96  704.4 17.8 64\n100 601.4 19.8 39\n101 601.4 19.8 39\n102 614.5 14.4 66\n103 647.1 20.1 40\n104 674.4 24.1 29\n108 631.2 21.4 33\n109 698.6 20.3 45\n110 709.9 17.4 56\n113 704.4 22.8 39\n114 724.3 18.9 35\n120 466.6 19.6 36\n121 608.2 10.3 74\n122 608.2 17.1 43\n123 680.7 22.5 42\n124 671.9 17.9 45\n125 692.3 19.8 50\n126 691.8 20.6 24\n128 728.6 17.2 43\n129 673.8 15.9 46\n130 691.8 15.4 35\n134 685.2 17.6 42\n136 594.2 17.6 52\n137 680.7 17.2 58\n138 686.5 15.6 66\n140 692.6 21.7 38\n141 686.5 21.9 39\n142 513.3 23.3 31\n143 529.8 21.2 51\n145 513.3 23.8 32\n146 578.8 27.4 22\n148 671.9 24.2 28\n149 647.1 17.4 43\n150 685.2 23.7 25\n151 433.3 23.2 39\n153 424.1 24.6 43\n154 692.3 20.1 47\n155 721.4 29.6 27\n156 647.1 16.4 47\n157 721.4 28.6 27\n158 654.1 18.4 45\n159 654.1 20.5 35\n160 668.0 19.0 34\n162 578.8 20.3 41\n164 674.4 17.8 56\n165 704.4 17.8 67\n167 654.1 16.6 47\n168 570.5 23.4 33\n170 578.8 20.7 45\n171 699.6 21.9 35\n172 609.6 17.4 50\n173 601.4 20.1 39\n174 686.5 17.7 39\n175 624.2 14.2 53\n176 624.2 20.3 39\n178 631.2 19.2 44\n179 735.7 18.3 45\n180 614.5 14.4 66\n181 680.7 23.9 32\n182 664.2 19.1 32\n184 696.1 16.8 45\n185 586.7 20.8 34\n186 692.6 17.6 46\n188 686.5 21.0 42\n192 578.8 24.2 28\n193 647.1 24.6 22\n194 699.6 24.3 25\n195 647.1 24.6 22\n196 586.7 23.5 36\n198 706.4 21.5 15\n199 692.6 13.9 59\n200 665.3 22.6 38\n201 692.6 21.6 33\n204 673.8 20.2 37\n206 706.4 22.1 34\n207 594.2 22.9 31\n208 692.6 20.7 37\n209 668.0 19.6 33\n210 685.2 23.2 26\n211 686.9 18.4 25\n213 692.3 20.1 47\n217 680.7 16.9 60\n218 709.9 12.4 73\n219 699.6 19.4 19\n221 631.2 16.2 59\n222 713.9 18.6 49\n225 735.7 15.4 57\n226 728.6 22.9 39\n227 696.1 16.1 44\n228 480.8 20.1 34\n229 728.6 28.3 26\n230 480.8 16.4 43\n231 699.6 26.4 21\n232 728.6 27.8 27\n233 692.6 18.7 43\n234 671.9 24.3 36\n235 674.4 17.7 25\n236 601.4 19.6 41\n237 674.4 18.2 46\n238 692.6 18.8 40\n239 674.4 25.1 27\n243 589.9 15.4 66\n244 700.7 21.9 73\n245 700.7 22.4 54\n246 700.7 26.8 38\n247 700.7 25.7 39\n248 503.6 20.7 70\n249 666.7 28.7 28\n250 666.7 21.7 40\n251 666.7 26.8 25\n252 666.7 24.0 36\n253 666.7 22.1 37\n254 565.5 21.4 38\n255 621.7 18.9 41\n256 694.8 22.3 46\n257 581.1 23.9 41\n258 581.1 21.4 44\n259 692.3 20.6 59\n260 692.3 23.7 40\n261 542.0 28.3 32\n262 573.0 11.2 84\n263 573.0 21.4 42\n264 629.1 19.3 39\n265 684.4 21.8 53\n266 550.3 22.1 54\n267 607.1 19.4 55\n268 658.2 23.7 24\n269 658.2 21.0 32\n270 658.2 19.1 53\n271 658.2 21.8 56\n272 658.2 20.1 58\n273 658.2 20.2 47\n286 411.8 23.4 40\n288 474.9 22.1 49\n289 474.9 24.2 32\n290 474.9 24.3 30\n291 474.9 18.7 53\n292 474.9 25.3 39\n293 466.3 22.9 40\n294 430.8 26.9 28\n296 430.8 22.2 48\n306 714.3 19.0 52\n307 714.3 17.1 53\n308 714.3 23.8 35\n309 758.1 16.0 45\n310 758.1 24.9 27\n311 758.1 25.3 27\n312 758.1 24.8 28\n313 706.6 12.2 78\n314 777.1 24.3 27\n315 777.1 19.7 41\n316 817.5 18.5 30\n317 739.4 18.6 24\n318 739.4 19.2 24\n319 783.5 21.6 27\n320 783.5 21.6 28\n321 783.5 18.9 34\n322 783.5 16.8 28\n323 783.5 16.8 28\n324 822.8 12.9 39\n325 726.9 13.7 56\n326 751.5 24.2 27\n327 751.5 24.1 27\n328 751.5 21.2 32\n329 751.5 19.7 35\n330 751.5 23.5 27\n331 751.5 24.2 27\n332 795.3 21.5 28\n333 795.3 17.1 41\n334 721.1 18.1 54\n335 764.0 18.0 51\n336 764.0  9.8 86\n337 764.0 19.3 44\n338 764.0 23.0 34\n339 764.0 22.7 35\n340 764.0 20.4 41\n341 764.0 19.3 44\n342 770.3 15.7 51\n343 807.1 20.6 37\n344 807.1 15.9 51\n345 807.1 12.2 66\n346 807.1 16.8 43\n347 807.1 21.3 35\n348 745.3 10.1 75\n349 745.3 17.4 57\n350 745.3 12.8 64\n351 745.3 10.1 75\n352 745.3 15.4 53\n353 745.3 20.6 43\n354 745.3 19.8 47\n355 745.3 18.7 50\n356 745.3 20.8 35\n357 745.3 20.8 35\n358 789.7 15.9 55\n359 789.7 19.7 39\n360 789.7 21.1 39\n361 789.7 18.4 42\n362 789.7 17.3 45\n363 732.3 15.2 64\n364 770.3 15.9 53\n365 770.3 21.1 35\n366 770.3 19.6 45\n367 812.1 15.9 38\n368 812.1 16.4 27\n369 744.4 16.8 47\n370 825.1 13.8 77\n371 825.1 13.8 77\n372 520.5 14.2 58\n373 664.5 10.4 75\n374 698.6 20.3 42\n375 855.3 10.3 78\n376 744.4 15.4 57\n377 672.6 21.1 54\n378 715.1 21.9 42\n381 458.8 19.3 39\n382 643.0 16.2 63\n383 690.0 28.2 29\n384 753.8 20.5 58\n385 819.1 21.3 44\n386 613.0 20.9 50\n387 750.5 20.6 55\n389 706.7 23.3 34\n390 706.7 23.3 34\n392 738.1 20.7 46\n393 825.1 21.9 43\n397 750.5 20.4 55\n398 613.0 24.3 33\n399 715.1 25.9 32\n402 731.7 22.8 46\n403 706.7 25.0 36\n404 643.0 21.3 41\n405 725.1 21.8 34\n406 680.9 27.9 27\n407 860.6 17.0 67\n409 855.3 19.9 44\n410 450.2 23.4 31\n413 442.1 22.8 27\n414 715.1 26.4 33\n415 723.1 24.1 50\n416 698.6 27.5 27\n417 575.8 26.3 39\n419 664.5 24.9 42\n420 613.0 24.8 36\n421 635.9 26.2 36\n422 690.0 30.8 19\n423 795.9 29.3 27\n424 744.4 22.3 48\n425 715.1 26.9 31\n426 753.8 20.4 56\n427 753.8 20.4 56\n428 672.6 27.9 33\n429 698.6 26.2 34\n430 613.0 24.6 44\n431 849.3 19.4 45\n432 605.3 23.3 40\n433 698.6 23.9 38\n434 723.1 20.9 66\n435 811.2 22.2 45\n437 672.6 26.8 35\n438 768.4 14.2 73\n439 715.1 23.6 53\n440 738.1 19.1 46\n441 855.3 16.2 58\n442 672.6 25.5 29\n445 855.3 16.2 58\n447 664.5 19.1 70\n449 844.0 10.5 77\n450 613.0 19.3 61\n451 690.0 23.4 49\n452 649.9 11.8 88\n453 730.6 17.7 65\n454 803.3 17.4 54\n455 753.8 16.8 56\n456 567.2 17.9 48\n457 753.8 16.6 59\n458 635.9 19.9 50\n459 715.1 18.9 64\n460 819.1 15.5 72\n461 715.1 18.9 64\n462 715.1 18.9 64\n463 825.1 14.5 76\n477 395.0 27.2 28\n478 423.4 26.1 45\n480 431.6 22.6 57\n481 560.0 30.2 25\n482 560.0 30.2 22\n483 587.1 23.4 40\n484 587.1 31.0 27\n485 587.1 33.1 25\n486 596.3 30.6 28\n487 605.8 24.1 43\n488 605.8 26.4 34\n489 605.8 19.4 71\n490 605.8 20.6 58\n491 605.8 28.7 33\n492 624.1 32.4 21\n493 633.6 32.4 27\n494 633.6 27.5 29\n495 643.0 30.8 30\n496 661.8 23.9 42\n497 661.8 32.6 26\n498 671.2 32.3 27\n499 671.2 33.3 26\n501 671.2 21.6 65\n502 671.2 21.6 65\n503 671.2 20.7 69\n504 689.1 29.2 30\n505 689.1 28.9 29\n506 744.4 26.7 35\n507 752.6 18.5 73\n508 752.6 25.9 41\n509 752.6 25.9 41\n510 752.6 21.1 71\n511 752.6 18.2 62\n512 665.6 27.8 35\n513 665.6 27.8 32\n514 665.6 21.9 71\n515 665.6 21.2 70\n516 614.7 25.6 42\n\n\nVisualitzem els resultats del test de normalitat univariant\n\nsummary(mvnoutliers, select = \"mvn\")\n\n     Test Statistic p.value     Method          MVN\n1 Royston   162.918  &lt;0.001 asymptotic ✗ Not normal\n\n\ni multivariant\n\nsummary(mvnoutliers, select = \"univariate\")\n\n              Test Variable Statistic p.value    Normality\n1 Anderson-Darling       DC    43.098  &lt;0.001 ✗ Not normal\n2 Anderson-Darling     temp     1.812  &lt;0.001 ✗ Not normal\n3 Anderson-Darling       RH     7.871  &lt;0.001 ✗ Not normal"
  },
  {
    "objectID": "material/Preprocessing/AdvancedPreprocessing.html#pca",
    "href": "material/Preprocessing/AdvancedPreprocessing.html#pca",
    "title": "Advance Preprocessing",
    "section": "2.4 PCA",
    "text": "2.4 PCA\nMétodes basats en correlacions ens permeten detectar outliers"
  },
  {
    "objectID": "material/Preprocessing/AdvancedPreprocessing.html#distancia-de-mahalanobis",
    "href": "material/Preprocessing/AdvancedPreprocessing.html#distancia-de-mahalanobis",
    "title": "Advance Preprocessing",
    "section": "2.5 Distancia de Mahalanobis",
    "text": "2.5 Distancia de Mahalanobis\nMedeix la distancia de un punt respecte a la mitjana considerant la covariança\n\ndistancia_mahalanobis &lt;- mahalanobis(dades, colMeans(dades), cov(dades))\n\nGrafiquem el plot de la densitat de les distancies\n\nplot(density(distancia_mahalanobis))\n\n\n\n\n\n\n\n\nEs mostren els valors de la bbdd que queden per sobre de el 99% de la distribució chi-cuadrat\n\ncutoff &lt;- qchisq(p = 0.99, df = ncol(dades))\ndades[distancia_mahalanobis&gt;cutoff, ]\n\n       DC temp  RH\n4    77.5  8.3  97\n5   102.2 11.4  99\n212 594.2  5.1  96\n277 349.7  4.6  21\n278 349.7  4.6  21\n279 349.7  4.6  21\n280 349.7  4.6  21\n282 349.7  5.1  24\n305 113.8 11.3  94\n380 171.4  5.2 100\n446 100.7 17.3  80\n\n\nOrdenamos de forma decreciente, según el score de Mahalanobis\n\ndades &lt;- dades[order(distancia_mahalanobis, decreasing = TRUE),]\n\nVisualitzem l’histograma de les distancies per veure on tallem els outliers\n\npar(mfrow=c(1,1))\nhist(distancia_mahalanobis)\n\n\n\n\n\n\n\n\nDescartamos los outliers según un umbral\n\numbral &lt;- 8\ndades[, \"outlier\"] &lt;- (distancia_mahalanobis &gt; umbral)\n\ndades[, \"color\"] &lt;- ifelse(dades[, \"outlier\"], \"red\", \"black\")\nscatterplot3d(dades[, \"DC\"], dades[, \"temp\"], dades[, \"RH\"], \n              color = dades[, \"color\"])\n\n\n\n\n\n\n\n(fig &lt;- plotly::plot_ly(dades, x = ~DC, y = ~temp, z = ~RH, \n                       color = ~color, colors = c('#0C4B8E', '#BF382A')) %&gt;% \n                        add_markers())\n\n\n\n\n(quienes &lt;- which(dades[, \"outlier\"] == TRUE))\n\n [1]   4   5  76 105 212 240 277 278 279 280 281 282 283 287 300 305 336 375 380\n[20] 446 464 465 479 500\n\n\n\n2.5.1 Mahalanobis Robusto\n\nlibrary(chemometrics)\n\ndis &lt;- chemometrics::Moutlier(dades[, c(\"DC\", \"temp\", \"RH\")], quantile = 0.99, plot = TRUE)\n\n\n\n\n\n\n\npar(mfrow = c(1, 1))\nplot(dis$md, dis$rd, type = \"n\")\ntext(dis$md, dis$rd, labels = rownames(dades))\n\n\n\n\n\n\n\na &lt;- which(dis$rd &gt; 7)\nprint(a)\n\n277 278 279 280   5 282   4 305 380 446 240 464 105  76 465 394 466 284 166 448 \n  1   2   3   4   5   6   7   8   9  10  12  15  17  18  20  31  35  37  39  40 \n395 418 112  98 391 119 177 197 203 285 242 469 205  62  78 471 117 443  73  92 \n 41  43  46  48  49  51  52  53  54  55  60  63  66  67  68  69  72  74  83  84 \n517 470  90 116 169 115 379 412  70  17 118 191 396 220  49 107 241 215 216 467 \n 85  86  87  88  90  91  92  94  95  96  97  99 100 103 105 107 108 109 110 113 \n468  71   1  20 472 161  77 388  59  97 183  60 127 190  19 163 106 411  61 187 \n118 119 121 122 123 126 128 131 132 133 137 139 140 144 145 146 147 150 151 152 \n408 132 135 111 202  40 133 147  50 214 223 131 189 \n154 157 158 159 163 164 165 169 170 171 172 173 177"
  },
  {
    "objectID": "material/Preprocessing/AdvancedPreprocessing.html#regresió-lineal-i-residus",
    "href": "material/Preprocessing/AdvancedPreprocessing.html#regresió-lineal-i-residus",
    "title": "Advance Preprocessing",
    "section": "2.6 Regresió Lineal i residus",
    "text": "2.6 Regresió Lineal i residus\nUn punt amb un residu gran pot considerar-se un outlier"
  },
  {
    "objectID": "material/Preprocessing/AdvancedPreprocessing.html#distancia-de-cook",
    "href": "material/Preprocessing/AdvancedPreprocessing.html#distancia-de-cook",
    "title": "Advance Preprocessing",
    "section": "2.7 Distancia de Cook",
    "text": "2.7 Distancia de Cook\nIdentifica punts amb gran influència en la regresió. Un valor de Cook D_i &gt; 1 és un outliers."
  },
  {
    "objectID": "material/Preprocessing/AdvancedPreprocessing.html#k-nearest-neighbors-knn-outlier-score",
    "href": "material/Preprocessing/AdvancedPreprocessing.html#k-nearest-neighbors-knn-outlier-score",
    "title": "Advance Preprocessing",
    "section": "2.8 K-Nearest Neighbors (KNN) Outlier Score",
    "text": "2.8 K-Nearest Neighbors (KNN) Outlier Score\nBasats en la densitat local de les dades\n\nlibrary(adamethods)\n\ndo_knno(dades[, c(\"DC\", \"temp\", \"RH\")], k=1, top_n = 30)\n\n [1]   9 149  64  65   7  33  73  79 130  32  16  19 274 167 198  22 142  63 106\n[20]  35  11 273 245 227   5   8  13  27  10 279"
  },
  {
    "objectID": "material/Preprocessing/AdvancedPreprocessing.html#local-outlier-factor-lof",
    "href": "material/Preprocessing/AdvancedPreprocessing.html#local-outlier-factor-lof",
    "title": "Advance Preprocessing",
    "section": "2.9 Local Outlier Factor (LOF)",
    "text": "2.9 Local Outlier Factor (LOF)\nCompara la densidat de un punt amb la densidat dels seus veïns. Un valor LOF alt\n\nlibrary(DMwR2)\nlibrary(dplyr)\n\noutlier.scores &lt;- lofactor(dades[, c(\"DC\", \"temp\", \"RH\")], k = 5)\npar(mfrow=c(1,1))\nplot(density(outlier.scores))\noutlier.scores\noutliers &lt;- order(outlier.scores, decreasing=T)\noutliers &lt;- order(outlier.scores, decreasing=T)[1:5]\n\nAprofitarem el ACP per poder visualizar els outliers\n\nn &lt;- nrow(dades[, c(\"DC\", \"temp\", \"RH\")]); labels &lt;- 1:n; labels[-outliers] &lt;- \".\"\nbiplot(prcomp(dades[, c(\"DC\", \"temp\", \"RH\")]), cex = .8, xlabs = labels)\n\nGrafiquem les correlacions per veure els gráfics\n\npch &lt;- rep(\".\", n)\npch[outliers] &lt;- \"+\"\ncol &lt;- rep(\"black\", n)\ncol[outliers] &lt;- \"red\"\npairs(dades[, c(\"DC\", \"temp\", \"RH\")], pch = pch, col = col)\n\nHo visualitzem en 3D\n\nplot3d(dades[, \"DC\"], dades[, \"temp\"], dades[, \"RH\"], type = \"s\", col = col, size = 1)\n\n\n2.9.1 Nueva versión de LOF\n\nlibrary(Rlof)\noutliers.scores &lt;- Rlof::lof(dades[, c(\"DC\", \"temp\", \"RH\")], k = 5)\nplot(density(outliers.scores))\n\n\n\n\n\n\n\n#outlier.scores &lt;- lof(dades[, c(\"DC\", \"temp\", \"RH\")], k=c(5:10))"
  },
  {
    "objectID": "material/Preprocessing/AdvancedPreprocessing.html#isolation-forest",
    "href": "material/Preprocessing/AdvancedPreprocessing.html#isolation-forest",
    "title": "Advance Preprocessing",
    "section": "2.10 Isolation Forest",
    "text": "2.10 Isolation Forest\n\n### Cargamos las librerias necesarias\nlibrary(R.matlab)   # Lectura de archivos .mat\nlibrary(solitude)   # Modelo isolation forest\nlibrary(tidyverse)  # Preparación de datos y gráficos\nlibrary(MLmetrics)\n\n# Carreguem les dades\ncardio_mat  &lt;- readMat(\"https://www.dropbox.com/s/galg3ihvxklf0qi/cardio.mat?dl=1\")\ndf_cardio   &lt;- as.data.frame(cardio_mat$X)\ndf_cardio$y &lt;- as.character(cardio_mat$y)\ndatos &lt;- df_cardio\n\n\nisoforest &lt;- isolationForest$new(\n  sample_size = as.integer(nrow(datos)/2),\n  num_trees   = 500, \n  replace     = TRUE,\n  seed        = 123\n)\nisoforest$fit(dataset = datos %&gt;% select(-y))\n\nAra anem a realitzar les prediccions.\n\npredicciones &lt;- isoforest$predict(\n  data = datos %&gt;% select(-y)\n)\nhead(predicciones)\n\n      id average_depth anomaly_score\n   &lt;int&gt;         &lt;num&gt;         &lt;num&gt;\n1:     1         9.816     0.5875006\n2:     2         9.738     0.5899888\n3:     3         9.528     0.5967405\n4:     4         9.728     0.5903086\n5:     5         9.860     0.5861016\n6:     6         9.796     0.5881376\n\n\n\nggplot(data = predicciones, aes(x = average_depth)) +\n  geom_histogram(color = \"gray40\") +\n  geom_vline(\n    xintercept = quantile(predicciones$average_depth, seq(0, 1, 0.1)),\n    color      = \"red\",\n    linetype   = \"dashed\") +\n  labs(\n    title = \"Distribución de las distancias medias del Isolation Forest\",\n    subtitle = \"Cuantiles marcados en rojo\"  ) +\n  theme_bw() +\n  theme(plot.title = element_text(size = 11))\n\n\n\n\n\n\n\ncuantiles &lt;- quantile(x = predicciones$average_depth, probs = seq(0, 1, 0.05))\ncuantiles\n\n   0%    5%   10%   15%   20%   25%   30%   35%   40%   45%   50%   55%   60% \n7.556 8.952 9.264 9.451 9.548 9.629 9.694 9.744 9.790 9.824 9.852 9.870 9.888 \n  65%   70%   75%   80%   85%   90%   95%  100% \n9.904 9.920 9.934 9.948 9.958 9.970 9.980 9.998"
  },
  {
    "objectID": "material/Preprocessing/AdvancedPreprocessing.html#tips-detección-de-anomalías",
    "href": "material/Preprocessing/AdvancedPreprocessing.html#tips-detección-de-anomalías",
    "title": "Advance Preprocessing",
    "section": "2.11 TIPS: Detección de anomalías",
    "text": "2.11 TIPS: Detección de anomalías\nUna vez que la distancia de separación ha sido calculado, se puede emplear como criterio para identificar anomalías. Asumiendo que las observaciones con valores atípicos en una o más de sus variables se separan del resto con mayor facilidad, aquellas observaciones con menor distancia promedio deberían ser las más atípicas.\nEn la práctica, si se está empleando esta estrategia de detección es porque no se dispone de datos etiquetados, es decir, no se conoce qué observaciones son realmente anomalías. Sin embargo, como en este ejemplo se dispone de la clasificación real, se puede verificar si realmente los datos anómalos tienen menores distancias.\n\ndatos &lt;- datos %&gt;%\n  bind_cols(predicciones)\n\nggplot(data = datos,\n       aes(x = y, y = average_depth)) +\n  geom_jitter(aes(color = y), width = 0.03, alpha = 0.3) + \n  geom_violin(alpha = 0) +\n  geom_boxplot(width = 0.2, outlier.shape = NA, alpha = 0) +\n  stat_summary(fun = \"mean\", colour = \"orangered2\", size = 3, geom = \"point\") +\n  labs(title = \"Distancia promedio en el modelo Isolation Forest\",\n       x = \"clasificación (0 = normal, 1 = anomalía)\",\n       y = \"Distancia promedio\") +\n  theme_bw() + \n  theme(legend.position = \"none\",\n        plot.title = element_text(size = 11)\n  )\n\n\n\n\n\n\n\n\nLa distancia promedio en el grupo de las anomalías (1) es claramente inferior. Sin embargo, al existir solapamiento, si se clasifican las n observaciones con menor distancia como anomalías, se incurriría en errores de falsos positivos.\nAcorde a la documentación, el set de datos Cardiotocogrpahy contiene 176 anomalías. Véase la matriz de confusión resultante si se clasifican como anomalías las 176 observaciones con menor distancia predicha.\n\nresultados &lt;- datos %&gt;%\n  select(y, average_depth) %&gt;%\n  arrange(average_depth) %&gt;%\n  mutate(clasificacion = if_else(average_depth &lt;= 8.5, \"1\", \"0\"))\n\nmat_confusion &lt;- MLmetrics::ConfusionMatrix(\n  y_pred = resultados$clasificacion,\n  y_true = resultados$y)\n\nmat_confusion\n\n      y_pred\ny_true    0    1\n     0 1638   17\n     1  158   18"
  },
  {
    "objectID": "material/Preprocessing/AdvancedPreprocessing.html#generate-data-with-nas",
    "href": "material/Preprocessing/AdvancedPreprocessing.html#generate-data-with-nas",
    "title": "Advance Preprocessing",
    "section": "4.1 Generate data with NA’s",
    "text": "4.1 Generate data with NA’s\n\ncolSums(is.na(iris))\n\nSepal.Length  Sepal.Width Petal.Length  Petal.Width      Species \n           0            0            0            0            0 \n\niris.mis &lt;- missForest::prodNA(iris, noNA = 0.1)\ncolSums((is.na(iris.mis)))\n\nSepal.Length  Sepal.Width Petal.Length  Petal.Width      Species \n          12           15           16           16           16 \n\n\nOtra forma de crear missings en el dataframe\n\niris.mis &lt;- mi::create.missing(iris, pct.mis = 10)"
  },
  {
    "objectID": "material/Preprocessing/AdvancedPreprocessing.html#little-test",
    "href": "material/Preprocessing/AdvancedPreprocessing.html#little-test",
    "title": "Advance Preprocessing",
    "section": "4.2 Little Test",
    "text": "4.2 Little Test\nEs un test que nos permite detectar con que tipo de NA’s estamos enfrente:\n\nnaniar::mcar_test(iris.mis)\n\nSi el valor p de la prova és inferior a 0 això vol dir que les dades amb NAs s’han generat aleatòriament."
  },
  {
    "objectID": "material/Preprocessing/AdvancedPreprocessing.html#patrons-descriptius-de-na-en-una-base-de-dades",
    "href": "material/Preprocessing/AdvancedPreprocessing.html#patrons-descriptius-de-na-en-una-base-de-dades",
    "title": "Advance Preprocessing",
    "section": "4.3 Patrons descriptius de NA en una base de dades",
    "text": "4.3 Patrons descriptius de NA en una base de dades\n\n4.3.1 Explorar les relacions de NA’s\n\nlibrary(visdat)\nlibrary(ggplot2)\nlibrary(naniar)\n\nvis_dat(airquality);\n\n\n\n\n\n\n\nvis_dat(iris.mis)\n\n\n\n\n\n\n\nvis_miss(airquality);\n\n\n\n\n\n\n\nvis_miss(iris.mis)\n\n\n\n\n\n\n\nggplot(airquality, aes(x = Solar.R,y = Ozone)) + \n  geom_point()\n\n\n\n\n\n\n\nggplot(airquality, aes(x = Solar.R,  y = Ozone)) + \n  geom_miss_point()\n\n\n\n\n\n\n\nggplot(airquality, aes(x = Solar.R, y = Ozone)) + \n  geom_miss_point() + \n  facet_wrap(~Month)\n\n\n\n\n\n\n\nggplot(airquality, aes(x = Solar.R, y = Ozone)) + \n  geom_miss_point() + \n  facet_wrap(~Month) + \n  theme_dark()\n\n\n\n\n\n\n\n\n\n\n4.3.2 Visualització dels NA’s per variables\n\ngg_miss_var(airquality) + labs(y = \"Look at all the missing ones\")\n\n\n\n\n\n\n\n\n\n\n4.3.3 Detecció de NA’s en la base de dades\n\naq_shadow &lt;- bind_shadow(airquality)\n\nImprimeix el gràfic amb diferència a les NA i no a les NA,\n\nairquality %&gt;%\n  bind_shadow() %&gt;%\n  group_by(Ozone_NA) %&gt;%\n  summarise_at(.vars = \"Solar.R\",\n               .funs = c(\"mean\", \"sd\", \"var\", \"min\", \"max\"),\n               na.rm = TRUE)\n\n# A tibble: 2 × 6\n  Ozone_NA  mean    sd   var   min   max\n  &lt;fct&gt;    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt;\n1 !NA       185.  91.2 8309.     7   334\n2 NA        190.  87.7 7690.    31   332\n\nggplot(aq_shadow,\n       aes(x = Temp,\n           colour = Ozone_NA)) + \n  geom_density()\n\n\n\n\n\n\n\n\n\n\n4.3.4 Extreu estadístiques amb NAs de la base de dades\n\nprop_miss_case(airquality)\n\n[1] 0.2745098\n\npct_miss_case(airquality)\n\n[1] 27.45098\n\nmiss_case_summary(airquality)\n\n# A tibble: 153 × 3\n    case n_miss pct_miss\n   &lt;int&gt;  &lt;int&gt;    &lt;dbl&gt;\n 1     5      2     33.3\n 2    27      2     33.3\n 3     6      1     16.7\n 4    10      1     16.7\n 5    11      1     16.7\n 6    25      1     16.7\n 7    26      1     16.7\n 8    32      1     16.7\n 9    33      1     16.7\n10    34      1     16.7\n# ℹ 143 more rows\n\nmiss_case_table(airquality)\n\n# A tibble: 3 × 3\n  n_miss_in_case n_cases pct_cases\n           &lt;int&gt;   &lt;int&gt;     &lt;dbl&gt;\n1              0     111     72.5 \n2              1      40     26.1 \n3              2       2      1.31\n\nprop_miss_var(airquality)\n\n[1] 0.3333333\n\npct_miss_var(airquality)\n\n[1] 33.33333\n\nmiss_var_summary(airquality)\n\n# A tibble: 6 × 3\n  variable n_miss pct_miss\n  &lt;chr&gt;     &lt;int&gt;    &lt;num&gt;\n1 Ozone        37    24.2 \n2 Solar.R       7     4.58\n3 Wind          0     0   \n4 Temp          0     0   \n5 Month         0     0   \n6 Day           0     0   \n\nmiss_var_table(airquality)\n\n# A tibble: 3 × 3\n  n_miss_in_var n_vars pct_vars\n          &lt;int&gt;  &lt;int&gt;    &lt;dbl&gt;\n1             0      4     66.7\n2             7      1     16.7\n3            37      1     16.7"
  },
  {
    "objectID": "material/Preprocessing/AdvancedPreprocessing.html#imputació-bàsica",
    "href": "material/Preprocessing/AdvancedPreprocessing.html#imputació-bàsica",
    "title": "Advance Preprocessing",
    "section": "4.4 Imputació bàsica",
    "text": "4.4 Imputació bàsica\n\n4.4.1 Imputar con la media\n\niris.mis[, \"imputed_Sepal.Length\"] &lt;- with(iris.mis, Hmisc::impute(Sepal.Length, mean))\n\n\n\n4.4.2 Imputar con la mediana\n\npre_median &lt;- preProcess(dades, method = \"medianImpute\")\nimputed_median &lt;- predict(pre_median, dades)\ndiagnose(imputed_median)\n\n\n\n4.4.3 Imputar con un valor aleatorio\n\niris.mis[, \"imputed_Sepal.Length2\"] &lt;- with(iris.mis, Hmisc::impute(Sepal.Length, 'random'))\n\nDe manera similar podeu utilitzar la mediana min, max, per imputar el valor que manca.\n\n\n4.4.4 Representa la distribució a variables reals i d’imputació a través del gráfico de densidad con ggplot2\n\ndf_long &lt;- iris.mis %&gt;%\n  select(Sepal.Length, imputed_Sepal.Length, imputed_Sepal.Length2) %&gt;%\n  pivot_longer(cols = everything(), names_to = \"Variable\", values_to = \"Valor\")\n\nggplot(df_long, aes(x = Valor, fill = Variable)) +\n  geom_density(alpha = 0.3) +  # Transparencia para mejor visualización\n  labs(title = \"Densidad de las tres variables\",\n       x = \"Valor\",\n       y = \"Densidad\") +\n  theme_minimal() +\n  scale_fill_manual(values = c(\"blue\", \"red\", \"green\"))\n\n\n\n\n\n\n\niris.mis[, c(\"imputed_Sepal.Length\", \"imputed_Sepal.Length2\")] &lt;- NULL\n\nOtra forma es usando el paquete argImpute.\n\n(impute_arg &lt;- Hmisc::aregImpute(~ Sepal.Length + Sepal.Width + Petal.Length + Petal.Width +\n                           Species, data = iris.mis, n.impute = 5))\n\nIteration 1 \nIteration 2 \nIteration 3 \nIteration 4 \nIteration 5 \nIteration 6 \nIteration 7 \nIteration 8 \n\n\n\nMultiple Imputation using Bootstrap and PMM\n\nHmisc::aregImpute(formula = ~Sepal.Length + Sepal.Width + Petal.Length + \n    Petal.Width + Species, data = iris.mis, n.impute = 5)\n\nn: 150  p: 5    Imputations: 5      nk: 3 \n\nNumber of NAs:\nSepal.Length  Sepal.Width Petal.Length  Petal.Width      Species \n          12           15           16           16           16 \n\n             type d.f.\nSepal.Length    s    2\nSepal.Width     s    2\nPetal.Length    s    2\nPetal.Width     s    2\nSpecies         c    2\n\nTransformation of Target Variables Forced to be Linear\n\nR-squares for Predicting Non-Missing Values for Each Variable\nUsing Last Imputations of Predictors\nSepal.Length  Sepal.Width Petal.Length  Petal.Width      Species \n       0.825        0.632        0.982        0.974        0.990 \n\n\nRevisamos la variable Sepal.Length con la imputación realizada en cada una de las rondas.\n\nimpute_arg$imputed$Sepal.Length\n\n    [,1] [,2] [,3] [,4] [,5]\n6    5.8  5.1  4.6  5.1  5.0\n52   5.8  6.1  6.0  6.7  6.5\n61   5.1  5.1  5.7  5.0  5.4\n62   6.0  5.6  6.3  5.8  6.3\n74   6.7  5.6  5.8  6.4  5.6\n81   5.2  6.0  5.5  5.1  5.2\n116  6.1  6.1  6.0  6.3  6.3\n118  5.1  5.1  7.7  7.7  7.7\n132  7.6  7.6  7.2  7.7  7.3\n133  6.0  6.7  6.9  6.9  6.8\n136  6.3  7.2  6.9  6.8  6.7\n138  6.5  6.3  6.3  6.9  6.7\n\n\nCalculamos la media para las 5 simulaciones.\n\nimputed_Sepal.Length &lt;- rowMeans(impute_arg$imputed$Sepal.Length)\nnew_var_imputed &lt;- iris$Sepal.Length\nnew_var_imputed[as.numeric(names(imputed_Sepal.Length))] &lt;- imputed_Sepal.Length\n\nRevisamos la diferencia entre las dos imputaciones.\n\nnewBD &lt;- data.frame(real = iris[, \"Sepal.Length\"], imputed = new_var_imputed)\ndf_long &lt;- newBD %&gt;%\n  pivot_longer(cols = everything(), names_to = \"Variable\", values_to = \"Valor\")\n\nggplot(df_long, aes(x = Valor, fill = Variable)) +\n  geom_density(alpha = 0.3) +  # Transparencia para mejor visualización\n  labs(title = \"Densidad de las tres variables\",\n       x = \"Valor\",\n       y = \"Densidad\") +\n  theme_minimal() +\n  scale_fill_manual(values = c(\"blue\", \"red\"))"
  },
  {
    "objectID": "material/Preprocessing/AdvancedPreprocessing.html#multiple-iterative-regression-imputation-mi-method",
    "href": "material/Preprocessing/AdvancedPreprocessing.html#multiple-iterative-regression-imputation-mi-method",
    "title": "Advance Preprocessing",
    "section": "4.5 Multiple Iterative Regression Imputation (MI method)",
    "text": "4.5 Multiple Iterative Regression Imputation (MI method)\nImputamos los valores NA’s con mi\n\nmi_data &lt;- mi::mi(iris.mis, seed = 335)\n\nRevisamos la información de las imputaciones.\n\nsummary(mi_data)\nplot(mi_data)\npar(ask = FALSE)\n\nRevisamos las interaciones de la base de datos\n\nmi_data@data"
  },
  {
    "objectID": "material/Preprocessing/AdvancedPreprocessing.html#media-con-una-variable-target",
    "href": "material/Preprocessing/AdvancedPreprocessing.html#media-con-una-variable-target",
    "title": "Advance Preprocessing",
    "section": "4.6 Media con una variable target",
    "text": "4.6 Media con una variable target\nDefinimos la variable target\n\ntarget = \"Species\"\n\nDefinimos la base de datos con NA’s\n\ndata &lt;- subset(iris, select = -c(get(target)))\ndata &lt;- missForest::prodNA(data, noNA = 0.1)\ndata$Species &lt;- iris[, target]\n\nDefinir variables a imputar (excluyendo la variable target)\n\nvarImp &lt;- colnames(data)[which(!colnames(data) %in% target)]\n\nCalcular las medias por grupo\n\nmeans &lt;- aggregate(data[, varImp], list(data[, target]), mean, na.rm = TRUE)\n\nImputar valores faltantes\n\nfor (c in varImp) {\n  for (g in means[, \"Group.1\"]) {\n    cond &lt;- data[, target] == g  # Condición booleana en vez de which()\n    na_index &lt;- is.na(data[, c]) & cond  # Seleccionar NA dentro del grupo\n    \n    # Asignar valores imputados\n    data[na_index, c] &lt;- means[means[, \"Group.1\"] == g, c]\n  }\n}\nsummary(data)\n\n  Sepal.Length    Sepal.Width     Petal.Length    Petal.Width   \n Min.   :4.300   Min.   :2.000   Min.   :1.000   Min.   :0.100  \n 1st Qu.:5.100   1st Qu.:2.800   1st Qu.:1.500   1st Qu.:0.300  \n Median :5.800   Median :3.000   Median :4.300   Median :1.326  \n Mean   :5.833   Mean   :3.034   Mean   :3.757   Mean   :1.202  \n 3rd Qu.:6.400   3rd Qu.:3.300   3rd Qu.:5.100   3rd Qu.:1.800  \n Max.   :7.900   Max.   :4.100   Max.   :6.900   Max.   :2.500  \n       Species  \n setosa    :50  \n versicolor:50  \n virginica :50  \n                \n                \n                \n\n\nVisualizamos la diferencia entre las imputaciones\n\niris[, \"Tipo\"] &lt;- \"original\"\ndata[, \"Tipo\"] &lt;- \"imputed\"\n\nUnimos en un mismo dataframe\n\ndata_long &lt;- bind_rows(iris, data)\ncols_numeric &lt;- names(data_long)[sapply(data_long, is.numeric) & names(data_long) != \"Tipo\"]\n\n# Convert a large data\ndata_long &lt;- data_long %&gt;%\n  pivot_longer(cols = all_of(cols_numeric), names_to = \"Variable\", values_to = \"Valor\")\n\nCreamos el gráfico con ggplot\n\nggplot(data_long, aes(x = Valor, fill = Tipo)) +\n  geom_density(alpha = 0.3) +  # Transparencia para comparación\n  facet_wrap(~Variable, scales = \"free\") +  # Un gráfico por variable\n  labs(title = \"Comparación de Distribuciones: Original vs Imputado\",\n       x = \"Valor\",\n       y = \"Densidad\") +\n  theme_minimal() +\n  scale_fill_manual(values = c(\"blue\", \"red\"))\n\n\n\n\n\n\n\n\nRemovemos la tipologia de variable\n\niris.mis[, \"Tipo\"] &lt;- NULL"
  },
  {
    "objectID": "material/Preprocessing/AdvancedPreprocessing.html#multiple-imputation-by-chained-equations-mice",
    "href": "material/Preprocessing/AdvancedPreprocessing.html#multiple-imputation-by-chained-equations-mice",
    "title": "Advance Preprocessing",
    "section": "4.7 Multiple Imputation by Chained Equations (MICE)",
    "text": "4.7 Multiple Imputation by Chained Equations (MICE)\nEliminamos las variables categoricas\n\nquiCat &lt;- which(lapply(iris.mis, class) %in% c(\"character\", \"factor\"))\ncategories &lt;- names(iris.mis)[quiCat]\niris.mis2 &lt;- subset(iris.mis, select = -c(get(categories)))\nsummary(iris.mis2)\n\n  Sepal.Length    Sepal.Width     Petal.Length    Petal.Width   \n Min.   :4.300   Min.   :2.000   Min.   :1.000   Min.   :0.100  \n 1st Qu.:5.100   1st Qu.:2.800   1st Qu.:1.500   1st Qu.:0.300  \n Median :5.700   Median :3.000   Median :4.300   Median :1.300  \n Mean   :5.795   Mean   :3.032   Mean   :3.697   Mean   :1.208  \n 3rd Qu.:6.375   3rd Qu.:3.300   3rd Qu.:5.100   3rd Qu.:1.800  \n Max.   :7.700   Max.   :4.400   Max.   :6.700   Max.   :2.500  \n NA's   :12      NA's   :15      NA's   :16      NA's   :16     \n\n\nVisualizamos los patrones de NA’s de la base de datos\n\npar(mfrow = c(1, 1))\nmice::md.pattern(iris.mis2, rotate.names = TRUE)\n\n\n\n\n\n\n\n\n   Sepal.Length Sepal.Width Petal.Length Petal.Width   \n99            1           1            1           1  0\n12            1           1            1           0  1\n15            1           1            0           1  1\n9             1           0            1           1  1\n2             1           0            1           0  2\n1             1           0            0           1  2\n7             0           1            1           1  1\n2             0           1            1           0  2\n3             0           0            1           1  2\n             12          15           16          16 59\n\n\nTambién lo podemos visualizar con el paquete VIM\n\nmice_plot &lt;- VIM::aggr(iris.mis2, col=c('navyblue','yellow'),\n                  numbers=TRUE, sortVars=TRUE,\n                  labels=names(iris.mis), cex.axis=.7,\n                  gap=3, ylab=c(\"Missing data\",\"Pattern\"))\n\n\n\n\n\n\n\n\n\n Variables sorted by number of missings: \n     Variable     Count\n Petal.Length 0.1066667\n  Petal.Width 0.1066667\n  Sepal.Width 0.1000000\n Sepal.Length 0.0800000\n\n\nA continuación realizamos la imputación de los valores faltanes de manera multivariada\n\nimputed_Data &lt;- mice::mice(iris.mis2, m=5, maxit = 50, method = 'pmm', seed = 500)\n\n\n iter imp variable\n  1   1  Sepal.Length  Sepal.Width  Petal.Length  Petal.Width\n  1   2  Sepal.Length  Sepal.Width  Petal.Length  Petal.Width\n  1   3  Sepal.Length  Sepal.Width  Petal.Length  Petal.Width\n  1   4  Sepal.Length  Sepal.Width  Petal.Length  Petal.Width\n  1   5  Sepal.Length  Sepal.Width  Petal.Length  Petal.Width\n  2   1  Sepal.Length  Sepal.Width  Petal.Length  Petal.Width\n  2   2  Sepal.Length  Sepal.Width  Petal.Length  Petal.Width\n  2   3  Sepal.Length  Sepal.Width  Petal.Length  Petal.Width\n  2   4  Sepal.Length  Sepal.Width  Petal.Length  Petal.Width\n  2   5  Sepal.Length  Sepal.Width  Petal.Length  Petal.Width\n  3   1  Sepal.Length  Sepal.Width  Petal.Length  Petal.Width\n  3   2  Sepal.Length  Sepal.Width  Petal.Length  Petal.Width\n  3   3  Sepal.Length  Sepal.Width  Petal.Length  Petal.Width\n  3   4  Sepal.Length  Sepal.Width  Petal.Length  Petal.Width\n  3   5  Sepal.Length  Sepal.Width  Petal.Length  Petal.Width\n  4   1  Sepal.Length  Sepal.Width  Petal.Length  Petal.Width\n  4   2  Sepal.Length  Sepal.Width  Petal.Length  Petal.Width\n  4   3  Sepal.Length  Sepal.Width  Petal.Length  Petal.Width\n  4   4  Sepal.Length  Sepal.Width  Petal.Length  Petal.Width\n  4   5  Sepal.Length  Sepal.Width  Petal.Length  Petal.Width\n  5   1  Sepal.Length  Sepal.Width  Petal.Length  Petal.Width\n  5   2  Sepal.Length  Sepal.Width  Petal.Length  Petal.Width\n  5   3  Sepal.Length  Sepal.Width  Petal.Length  Petal.Width\n  5   4  Sepal.Length  Sepal.Width  Petal.Length  Petal.Width\n  5   5  Sepal.Length  Sepal.Width  Petal.Length  Petal.Width\n  6   1  Sepal.Length  Sepal.Width  Petal.Length  Petal.Width\n  6   2  Sepal.Length  Sepal.Width  Petal.Length  Petal.Width\n  6   3  Sepal.Length  Sepal.Width  Petal.Length  Petal.Width\n  6   4  Sepal.Length  Sepal.Width  Petal.Length  Petal.Width\n  6   5  Sepal.Length  Sepal.Width  Petal.Length  Petal.Width\n  7   1  Sepal.Length  Sepal.Width  Petal.Length  Petal.Width\n  7   2  Sepal.Length  Sepal.Width  Petal.Length  Petal.Width\n  7   3  Sepal.Length  Sepal.Width  Petal.Length  Petal.Width\n  7   4  Sepal.Length  Sepal.Width  Petal.Length  Petal.Width\n  7   5  Sepal.Length  Sepal.Width  Petal.Length  Petal.Width\n  8   1  Sepal.Length  Sepal.Width  Petal.Length  Petal.Width\n  8   2  Sepal.Length  Sepal.Width  Petal.Length  Petal.Width\n  8   3  Sepal.Length  Sepal.Width  Petal.Length  Petal.Width\n  8   4  Sepal.Length  Sepal.Width  Petal.Length  Petal.Width\n  8   5  Sepal.Length  Sepal.Width  Petal.Length  Petal.Width\n  9   1  Sepal.Length  Sepal.Width  Petal.Length  Petal.Width\n  9   2  Sepal.Length  Sepal.Width  Petal.Length  Petal.Width\n  9   3  Sepal.Length  Sepal.Width  Petal.Length  Petal.Width\n  9   4  Sepal.Length  Sepal.Width  Petal.Length  Petal.Width\n  9   5  Sepal.Length  Sepal.Width  Petal.Length  Petal.Width\n  10   1  Sepal.Length  Sepal.Width  Petal.Length  Petal.Width\n  10   2  Sepal.Length  Sepal.Width  Petal.Length  Petal.Width\n  10   3  Sepal.Length  Sepal.Width  Petal.Length  Petal.Width\n  10   4  Sepal.Length  Sepal.Width  Petal.Length  Petal.Width\n  10   5  Sepal.Length  Sepal.Width  Petal.Length  Petal.Width\n  11   1  Sepal.Length  Sepal.Width  Petal.Length  Petal.Width\n  11   2  Sepal.Length  Sepal.Width  Petal.Length  Petal.Width\n  11   3  Sepal.Length  Sepal.Width  Petal.Length  Petal.Width\n  11   4  Sepal.Length  Sepal.Width  Petal.Length  Petal.Width\n  11   5  Sepal.Length  Sepal.Width  Petal.Length  Petal.Width\n  12   1  Sepal.Length  Sepal.Width  Petal.Length  Petal.Width\n  12   2  Sepal.Length  Sepal.Width  Petal.Length  Petal.Width\n  12   3  Sepal.Length  Sepal.Width  Petal.Length  Petal.Width\n  12   4  Sepal.Length  Sepal.Width  Petal.Length  Petal.Width\n  12   5  Sepal.Length  Sepal.Width  Petal.Length  Petal.Width\n  13   1  Sepal.Length  Sepal.Width  Petal.Length  Petal.Width\n  13   2  Sepal.Length  Sepal.Width  Petal.Length  Petal.Width\n  13   3  Sepal.Length  Sepal.Width  Petal.Length  Petal.Width\n  13   4  Sepal.Length  Sepal.Width  Petal.Length  Petal.Width\n  13   5  Sepal.Length  Sepal.Width  Petal.Length  Petal.Width\n  14   1  Sepal.Length  Sepal.Width  Petal.Length  Petal.Width\n  14   2  Sepal.Length  Sepal.Width  Petal.Length  Petal.Width\n  14   3  Sepal.Length  Sepal.Width  Petal.Length  Petal.Width\n  14   4  Sepal.Length  Sepal.Width  Petal.Length  Petal.Width\n  14   5  Sepal.Length  Sepal.Width  Petal.Length  Petal.Width\n  15   1  Sepal.Length  Sepal.Width  Petal.Length  Petal.Width\n  15   2  Sepal.Length  Sepal.Width  Petal.Length  Petal.Width\n  15   3  Sepal.Length  Sepal.Width  Petal.Length  Petal.Width\n  15   4  Sepal.Length  Sepal.Width  Petal.Length  Petal.Width\n  15   5  Sepal.Length  Sepal.Width  Petal.Length  Petal.Width\n  16   1  Sepal.Length  Sepal.Width  Petal.Length  Petal.Width\n  16   2  Sepal.Length  Sepal.Width  Petal.Length  Petal.Width\n  16   3  Sepal.Length  Sepal.Width  Petal.Length  Petal.Width\n  16   4  Sepal.Length  Sepal.Width  Petal.Length  Petal.Width\n  16   5  Sepal.Length  Sepal.Width  Petal.Length  Petal.Width\n  17   1  Sepal.Length  Sepal.Width  Petal.Length  Petal.Width\n  17   2  Sepal.Length  Sepal.Width  Petal.Length  Petal.Width\n  17   3  Sepal.Length  Sepal.Width  Petal.Length  Petal.Width\n  17   4  Sepal.Length  Sepal.Width  Petal.Length  Petal.Width\n  17   5  Sepal.Length  Sepal.Width  Petal.Length  Petal.Width\n  18   1  Sepal.Length  Sepal.Width  Petal.Length  Petal.Width\n  18   2  Sepal.Length  Sepal.Width  Petal.Length  Petal.Width\n  18   3  Sepal.Length  Sepal.Width  Petal.Length  Petal.Width\n  18   4  Sepal.Length  Sepal.Width  Petal.Length  Petal.Width\n  18   5  Sepal.Length  Sepal.Width  Petal.Length  Petal.Width\n  19   1  Sepal.Length  Sepal.Width  Petal.Length  Petal.Width\n  19   2  Sepal.Length  Sepal.Width  Petal.Length  Petal.Width\n  19   3  Sepal.Length  Sepal.Width  Petal.Length  Petal.Width\n  19   4  Sepal.Length  Sepal.Width  Petal.Length  Petal.Width\n  19   5  Sepal.Length  Sepal.Width  Petal.Length  Petal.Width\n  20   1  Sepal.Length  Sepal.Width  Petal.Length  Petal.Width\n  20   2  Sepal.Length  Sepal.Width  Petal.Length  Petal.Width\n  20   3  Sepal.Length  Sepal.Width  Petal.Length  Petal.Width\n  20   4  Sepal.Length  Sepal.Width  Petal.Length  Petal.Width\n  20   5  Sepal.Length  Sepal.Width  Petal.Length  Petal.Width\n  21   1  Sepal.Length  Sepal.Width  Petal.Length  Petal.Width\n  21   2  Sepal.Length  Sepal.Width  Petal.Length  Petal.Width\n  21   3  Sepal.Length  Sepal.Width  Petal.Length  Petal.Width\n  21   4  Sepal.Length  Sepal.Width  Petal.Length  Petal.Width\n  21   5  Sepal.Length  Sepal.Width  Petal.Length  Petal.Width\n  22   1  Sepal.Length  Sepal.Width  Petal.Length  Petal.Width\n  22   2  Sepal.Length  Sepal.Width  Petal.Length  Petal.Width\n  22   3  Sepal.Length  Sepal.Width  Petal.Length  Petal.Width\n  22   4  Sepal.Length  Sepal.Width  Petal.Length  Petal.Width\n  22   5  Sepal.Length  Sepal.Width  Petal.Length  Petal.Width\n  23   1  Sepal.Length  Sepal.Width  Petal.Length  Petal.Width\n  23   2  Sepal.Length  Sepal.Width  Petal.Length  Petal.Width\n  23   3  Sepal.Length  Sepal.Width  Petal.Length  Petal.Width\n  23   4  Sepal.Length  Sepal.Width  Petal.Length  Petal.Width\n  23   5  Sepal.Length  Sepal.Width  Petal.Length  Petal.Width\n  24   1  Sepal.Length  Sepal.Width  Petal.Length  Petal.Width\n  24   2  Sepal.Length  Sepal.Width  Petal.Length  Petal.Width\n  24   3  Sepal.Length  Sepal.Width  Petal.Length  Petal.Width\n  24   4  Sepal.Length  Sepal.Width  Petal.Length  Petal.Width\n  24   5  Sepal.Length  Sepal.Width  Petal.Length  Petal.Width\n  25   1  Sepal.Length  Sepal.Width  Petal.Length  Petal.Width\n  25   2  Sepal.Length  Sepal.Width  Petal.Length  Petal.Width\n  25   3  Sepal.Length  Sepal.Width  Petal.Length  Petal.Width\n  25   4  Sepal.Length  Sepal.Width  Petal.Length  Petal.Width\n  25   5  Sepal.Length  Sepal.Width  Petal.Length  Petal.Width\n  26   1  Sepal.Length  Sepal.Width  Petal.Length  Petal.Width\n  26   2  Sepal.Length  Sepal.Width  Petal.Length  Petal.Width\n  26   3  Sepal.Length  Sepal.Width  Petal.Length  Petal.Width\n  26   4  Sepal.Length  Sepal.Width  Petal.Length  Petal.Width\n  26   5  Sepal.Length  Sepal.Width  Petal.Length  Petal.Width\n  27   1  Sepal.Length  Sepal.Width  Petal.Length  Petal.Width\n  27   2  Sepal.Length  Sepal.Width  Petal.Length  Petal.Width\n  27   3  Sepal.Length  Sepal.Width  Petal.Length  Petal.Width\n  27   4  Sepal.Length  Sepal.Width  Petal.Length  Petal.Width\n  27   5  Sepal.Length  Sepal.Width  Petal.Length  Petal.Width\n  28   1  Sepal.Length  Sepal.Width  Petal.Length  Petal.Width\n  28   2  Sepal.Length  Sepal.Width  Petal.Length  Petal.Width\n  28   3  Sepal.Length  Sepal.Width  Petal.Length  Petal.Width\n  28   4  Sepal.Length  Sepal.Width  Petal.Length  Petal.Width\n  28   5  Sepal.Length  Sepal.Width  Petal.Length  Petal.Width\n  29   1  Sepal.Length  Sepal.Width  Petal.Length  Petal.Width\n  29   2  Sepal.Length  Sepal.Width  Petal.Length  Petal.Width\n  29   3  Sepal.Length  Sepal.Width  Petal.Length  Petal.Width\n  29   4  Sepal.Length  Sepal.Width  Petal.Length  Petal.Width\n  29   5  Sepal.Length  Sepal.Width  Petal.Length  Petal.Width\n  30   1  Sepal.Length  Sepal.Width  Petal.Length  Petal.Width\n  30   2  Sepal.Length  Sepal.Width  Petal.Length  Petal.Width\n  30   3  Sepal.Length  Sepal.Width  Petal.Length  Petal.Width\n  30   4  Sepal.Length  Sepal.Width  Petal.Length  Petal.Width\n  30   5  Sepal.Length  Sepal.Width  Petal.Length  Petal.Width\n  31   1  Sepal.Length  Sepal.Width  Petal.Length  Petal.Width\n  31   2  Sepal.Length  Sepal.Width  Petal.Length  Petal.Width\n  31   3  Sepal.Length  Sepal.Width  Petal.Length  Petal.Width\n  31   4  Sepal.Length  Sepal.Width  Petal.Length  Petal.Width\n  31   5  Sepal.Length  Sepal.Width  Petal.Length  Petal.Width\n  32   1  Sepal.Length  Sepal.Width  Petal.Length  Petal.Width\n  32   2  Sepal.Length  Sepal.Width  Petal.Length  Petal.Width\n  32   3  Sepal.Length  Sepal.Width  Petal.Length  Petal.Width\n  32   4  Sepal.Length  Sepal.Width  Petal.Length  Petal.Width\n  32   5  Sepal.Length  Sepal.Width  Petal.Length  Petal.Width\n  33   1  Sepal.Length  Sepal.Width  Petal.Length  Petal.Width\n  33   2  Sepal.Length  Sepal.Width  Petal.Length  Petal.Width\n  33   3  Sepal.Length  Sepal.Width  Petal.Length  Petal.Width\n  33   4  Sepal.Length  Sepal.Width  Petal.Length  Petal.Width\n  33   5  Sepal.Length  Sepal.Width  Petal.Length  Petal.Width\n  34   1  Sepal.Length  Sepal.Width  Petal.Length  Petal.Width\n  34   2  Sepal.Length  Sepal.Width  Petal.Length  Petal.Width\n  34   3  Sepal.Length  Sepal.Width  Petal.Length  Petal.Width\n  34   4  Sepal.Length  Sepal.Width  Petal.Length  Petal.Width\n  34   5  Sepal.Length  Sepal.Width  Petal.Length  Petal.Width\n  35   1  Sepal.Length  Sepal.Width  Petal.Length  Petal.Width\n  35   2  Sepal.Length  Sepal.Width  Petal.Length  Petal.Width\n  35   3  Sepal.Length  Sepal.Width  Petal.Length  Petal.Width\n  35   4  Sepal.Length  Sepal.Width  Petal.Length  Petal.Width\n  35   5  Sepal.Length  Sepal.Width  Petal.Length  Petal.Width\n  36   1  Sepal.Length  Sepal.Width  Petal.Length  Petal.Width\n  36   2  Sepal.Length  Sepal.Width  Petal.Length  Petal.Width\n  36   3  Sepal.Length  Sepal.Width  Petal.Length  Petal.Width\n  36   4  Sepal.Length  Sepal.Width  Petal.Length  Petal.Width\n  36   5  Sepal.Length  Sepal.Width  Petal.Length  Petal.Width\n  37   1  Sepal.Length  Sepal.Width  Petal.Length  Petal.Width\n  37   2  Sepal.Length  Sepal.Width  Petal.Length  Petal.Width\n  37   3  Sepal.Length  Sepal.Width  Petal.Length  Petal.Width\n  37   4  Sepal.Length  Sepal.Width  Petal.Length  Petal.Width\n  37   5  Sepal.Length  Sepal.Width  Petal.Length  Petal.Width\n  38   1  Sepal.Length  Sepal.Width  Petal.Length  Petal.Width\n  38   2  Sepal.Length  Sepal.Width  Petal.Length  Petal.Width\n  38   3  Sepal.Length  Sepal.Width  Petal.Length  Petal.Width\n  38   4  Sepal.Length  Sepal.Width  Petal.Length  Petal.Width\n  38   5  Sepal.Length  Sepal.Width  Petal.Length  Petal.Width\n  39   1  Sepal.Length  Sepal.Width  Petal.Length  Petal.Width\n  39   2  Sepal.Length  Sepal.Width  Petal.Length  Petal.Width\n  39   3  Sepal.Length  Sepal.Width  Petal.Length  Petal.Width\n  39   4  Sepal.Length  Sepal.Width  Petal.Length  Petal.Width\n  39   5  Sepal.Length  Sepal.Width  Petal.Length  Petal.Width\n  40   1  Sepal.Length  Sepal.Width  Petal.Length  Petal.Width\n  40   2  Sepal.Length  Sepal.Width  Petal.Length  Petal.Width\n  40   3  Sepal.Length  Sepal.Width  Petal.Length  Petal.Width\n  40   4  Sepal.Length  Sepal.Width  Petal.Length  Petal.Width\n  40   5  Sepal.Length  Sepal.Width  Petal.Length  Petal.Width\n  41   1  Sepal.Length  Sepal.Width  Petal.Length  Petal.Width\n  41   2  Sepal.Length  Sepal.Width  Petal.Length  Petal.Width\n  41   3  Sepal.Length  Sepal.Width  Petal.Length  Petal.Width\n  41   4  Sepal.Length  Sepal.Width  Petal.Length  Petal.Width\n  41   5  Sepal.Length  Sepal.Width  Petal.Length  Petal.Width\n  42   1  Sepal.Length  Sepal.Width  Petal.Length  Petal.Width\n  42   2  Sepal.Length  Sepal.Width  Petal.Length  Petal.Width\n  42   3  Sepal.Length  Sepal.Width  Petal.Length  Petal.Width\n  42   4  Sepal.Length  Sepal.Width  Petal.Length  Petal.Width\n  42   5  Sepal.Length  Sepal.Width  Petal.Length  Petal.Width\n  43   1  Sepal.Length  Sepal.Width  Petal.Length  Petal.Width\n  43   2  Sepal.Length  Sepal.Width  Petal.Length  Petal.Width\n  43   3  Sepal.Length  Sepal.Width  Petal.Length  Petal.Width\n  43   4  Sepal.Length  Sepal.Width  Petal.Length  Petal.Width\n  43   5  Sepal.Length  Sepal.Width  Petal.Length  Petal.Width\n  44   1  Sepal.Length  Sepal.Width  Petal.Length  Petal.Width\n  44   2  Sepal.Length  Sepal.Width  Petal.Length  Petal.Width\n  44   3  Sepal.Length  Sepal.Width  Petal.Length  Petal.Width\n  44   4  Sepal.Length  Sepal.Width  Petal.Length  Petal.Width\n  44   5  Sepal.Length  Sepal.Width  Petal.Length  Petal.Width\n  45   1  Sepal.Length  Sepal.Width  Petal.Length  Petal.Width\n  45   2  Sepal.Length  Sepal.Width  Petal.Length  Petal.Width\n  45   3  Sepal.Length  Sepal.Width  Petal.Length  Petal.Width\n  45   4  Sepal.Length  Sepal.Width  Petal.Length  Petal.Width\n  45   5  Sepal.Length  Sepal.Width  Petal.Length  Petal.Width\n  46   1  Sepal.Length  Sepal.Width  Petal.Length  Petal.Width\n  46   2  Sepal.Length  Sepal.Width  Petal.Length  Petal.Width\n  46   3  Sepal.Length  Sepal.Width  Petal.Length  Petal.Width\n  46   4  Sepal.Length  Sepal.Width  Petal.Length  Petal.Width\n  46   5  Sepal.Length  Sepal.Width  Petal.Length  Petal.Width\n  47   1  Sepal.Length  Sepal.Width  Petal.Length  Petal.Width\n  47   2  Sepal.Length  Sepal.Width  Petal.Length  Petal.Width\n  47   3  Sepal.Length  Sepal.Width  Petal.Length  Petal.Width\n  47   4  Sepal.Length  Sepal.Width  Petal.Length  Petal.Width\n  47   5  Sepal.Length  Sepal.Width  Petal.Length  Petal.Width\n  48   1  Sepal.Length  Sepal.Width  Petal.Length  Petal.Width\n  48   2  Sepal.Length  Sepal.Width  Petal.Length  Petal.Width\n  48   3  Sepal.Length  Sepal.Width  Petal.Length  Petal.Width\n  48   4  Sepal.Length  Sepal.Width  Petal.Length  Petal.Width\n  48   5  Sepal.Length  Sepal.Width  Petal.Length  Petal.Width\n  49   1  Sepal.Length  Sepal.Width  Petal.Length  Petal.Width\n  49   2  Sepal.Length  Sepal.Width  Petal.Length  Petal.Width\n  49   3  Sepal.Length  Sepal.Width  Petal.Length  Petal.Width\n  49   4  Sepal.Length  Sepal.Width  Petal.Length  Petal.Width\n  49   5  Sepal.Length  Sepal.Width  Petal.Length  Petal.Width\n  50   1  Sepal.Length  Sepal.Width  Petal.Length  Petal.Width\n  50   2  Sepal.Length  Sepal.Width  Petal.Length  Petal.Width\n  50   3  Sepal.Length  Sepal.Width  Petal.Length  Petal.Width\n  50   4  Sepal.Length  Sepal.Width  Petal.Length  Petal.Width\n  50   5  Sepal.Length  Sepal.Width  Petal.Length  Petal.Width\n\nsummary(imputed_Data)\n\nClass: mids\nNumber of multiple imputations:  5 \nImputation methods:\nSepal.Length  Sepal.Width Petal.Length  Petal.Width \n       \"pmm\"        \"pmm\"        \"pmm\"        \"pmm\" \nPredictorMatrix:\n             Sepal.Length Sepal.Width Petal.Length Petal.Width\nSepal.Length            0           1            1           1\nSepal.Width             1           0            1           1\nPetal.Length            1           1            0           1\nPetal.Width             1           1            1           0\n\n\nInspeccionamos la calidad de las imputaciones\n\nmice::stripplot(imputed_Data, Sepal.Width, pch = 19, xlab = \"Imputation number\")\n\n\n\n\n\n\n\nimputed_Data$imp$Sepal.Width\n\n      1   2   3   4   5\n1   3.6 3.6 4.1 3.5 3.0\n5   3.4 3.0 2.6 3.0 3.4\n6   2.3 3.1 3.4 3.5 3.5\n17  4.4 3.8 4.2 3.5 3.7\n26  2.9 3.1 3.2 2.9 3.3\n31  2.8 2.8 2.8 2.8 3.1\n38  3.1 3.3 2.9 2.9 3.1\n70  3.0 2.9 2.3 2.4 2.4\n101 2.5 2.9 3.0 2.7 3.2\n112 2.9 3.2 3.0 3.0 2.7\n116 2.8 4.2 3.2 3.1 3.2\n121 3.4 2.8 2.8 3.8 3.2\n130 2.8 3.4 3.1 4.1 3.3\n132 2.7 2.7 2.7 2.5 2.9\n146 3.4 3.0 3.8 3.5 3.3\n\n\nAl final seleccionamos una de las iteracciones y la dejamos como imputación de los valores faltantes.\n\ncompleteData &lt;- mice::complete(imputed_Data, action = \"long\")\n\n\n4.7.1 Exercices:\n\nDeploy the multiple plot to compare te imputation and not imputation with all numeric vars in dataframe"
  },
  {
    "objectID": "material/Preprocessing/AdvancedPreprocessing.html#knn",
    "href": "material/Preprocessing/AdvancedPreprocessing.html#knn",
    "title": "Advance Preprocessing",
    "section": "4.8 KNN",
    "text": "4.8 KNN\n\ntipos &lt;- sapply(iris.mis, class)\nvarNum &lt;- names(tipos)[which(tipos %in% c(\"numeric\", \"integer\"))]\ndata_knn_imputation &lt;- multiUS::KNNimp(iris.mis[, varNum], k = 1)\nsummary(data_knn_imputation)\n\n  Sepal.Length    Sepal.Width     Petal.Length    Petal.Width   \n Min.   :4.300   Min.   :2.000   Min.   :1.000   Min.   :0.100  \n 1st Qu.:5.100   1st Qu.:2.800   1st Qu.:1.600   1st Qu.:0.300  \n Median :5.800   Median :3.000   Median :4.400   Median :1.300  \n Mean   :5.841   Mean   :3.045   Mean   :3.767   Mean   :1.191  \n 3rd Qu.:6.475   3rd Qu.:3.375   3rd Qu.:5.100   3rd Qu.:1.800  \n Max.   :7.700   Max.   :4.400   Max.   :6.700   Max.   :2.500  \n\n\nVisualizamos la diferencia entre las dos imputaciones.\n\nnewBD &lt;- data.frame(real = iris[, \"Sepal.Length\"], imputed = data_knn_imputation[, \"Sepal.Length\"])\ndf_long &lt;- newBD %&gt;%\n  pivot_longer(cols = everything(), names_to = \"Variable\", values_to = \"Valor\")\n\nggplot(df_long, aes(x = Valor, fill = Variable)) +\n  geom_density(alpha = 0.3) +  # Transparencia para mejor visualización\n  labs(title = \"Densidad de las tres variables\",\n       x = \"Valor\",\n       y = \"Densidad\") +\n  theme_minimal() +\n  scale_fill_manual(values = c(\"blue\", \"red\"))\n\n\n\n\n\n\n\n\nOtra forma de hacerlo en R seria la siguiente:\n\n#| label: graficamos_mi_iteration\n#| echo: true\n#| eval: false\n#| warning: false\n#| message: false\n#| error: false\n\nlibrary(\"caret\")\n\nCargando paquete requerido: lattice\n\n\n\nAdjuntando el paquete: 'caret'\n\n\nThe following objects are masked from 'package:MLmetrics':\n\n    MAE, RMSE\n\n\nThe following object is masked from 'package:purrr':\n\n    lift\n\npre_knn &lt;- preProcess(dades, method = \"knnImpute\", k = 2)\nimputed_knn &lt;- predict(pre_knn, dades)\ndiagnose(imputed_knn)\n\n# A tibble: 5 × 6\n  variables types     missing_count missing_percent unique_count unique_rate\n  &lt;chr&gt;     &lt;chr&gt;             &lt;int&gt;           &lt;dbl&gt;        &lt;int&gt;       &lt;dbl&gt;\n1 DC        numeric               0               0          219     0.424  \n2 temp      numeric               0               0          192     0.371  \n3 RH        numeric               0               0           75     0.145  \n4 outlier   logical               0               0            2     0.00387\n5 color     character             0               0            2     0.00387\n\n\n\n4.8.1 Exercices:\n\nDeploy the multiple plot to compare the imputation and not imputation with all numeric vars in dataframe"
  },
  {
    "objectID": "material/Preprocessing/AdvancedPreprocessing.html#missforest",
    "href": "material/Preprocessing/AdvancedPreprocessing.html#missforest",
    "title": "Advance Preprocessing",
    "section": "4.9 missForest",
    "text": "4.9 missForest\nImputamos los missings usando todos los parametros con los valores por defectos.\n\nlibrary(missForest)\n\nWarning: package 'missForest' was built under R version 4.4.3\n\niris.imp &lt;- missForest(iris.mis, variablewise = T, verbose = T) \n\n  missForest iteration 1 in progress...done!\n    estimated error(s): 0.1785301 0.09607199 0.1616973 0.032296 0.05223881 \n    difference(s): 0.005511064 0.08 \n    time: 0.05 seconds\n\n  missForest iteration 2 in progress...done!\n    estimated error(s): 0.1241168 0.08722486 0.06218984 0.02937162 0.04477612 \n    difference(s): 0.0001018209 0 \n    time: 0.04 seconds\n\n  missForest iteration 3 in progress...done!\n    estimated error(s): 0.1310795 0.08576933 0.0644648 0.02911535 0.03731343 \n    difference(s): 1.120453e-05 0 \n    time: 0.07 seconds\n\n  missForest iteration 4 in progress...done!\n    estimated error(s): 0.1240951 0.08841871 0.06278651 0.02825258 0.04477612 \n    difference(s): 1.038555e-05 0 \n    time: 0.06 seconds\n\n  missForest iteration 5 in progress...done!\n    estimated error(s): 0.1255279 0.09103837 0.06693245 0.0299239 0.04477612 \n    difference(s): 6.865365e-06 0 \n    time: 0.04 seconds\n\n  missForest iteration 6 in progress...done!\n    estimated error(s): 0.1239506 0.08867171 0.06091633 0.02916087 0.04477612 \n    difference(s): 6.449324e-06 0 \n    time: 0.04 seconds\n\n  missForest iteration 7 in progress...done!\n    estimated error(s): 0.1228071 0.08399709 0.06295863 0.03010765 0.05223881 \n    difference(s): 1.060512e-05 0 \n    time: 0.04 seconds\n\n\nVisualizamos los valores imputados\n\niris.imp$ximp\n\n    Sepal.Length Sepal.Width Petal.Length Petal.Width    Species\n1       5.100000    3.503897     1.504231   0.2000000     setosa\n2       4.900000    3.000000     1.400000   0.2000000     setosa\n3       4.700000    3.200000     1.300000   0.2000000     setosa\n4       4.600000    3.100000     1.500000   0.2000000     setosa\n5       5.000000    3.302965     1.400000   0.2057000     setosa\n6       5.209333    3.597583     1.700000   0.4000000     setosa\n7       4.600000    3.400000     1.400000   0.3000000     setosa\n8       5.000000    3.400000     1.500000   0.2000000     setosa\n9       4.400000    2.900000     1.400000   0.2000000     setosa\n10      4.900000    3.100000     1.500000   0.1000000     setosa\n11      5.400000    3.700000     1.500000   0.2667614     setosa\n12      4.800000    3.400000     1.600000   0.2000000     setosa\n13      4.800000    3.000000     1.400000   0.1000000     setosa\n14      4.300000    3.000000     1.100000   0.1000000     setosa\n15      5.800000    4.000000     1.200000   0.3362333     setosa\n16      5.700000    4.400000     1.500000   0.4000000     setosa\n17      5.400000    3.694333     1.300000   0.4000000     setosa\n18      5.100000    3.500000     1.442550   0.3000000     setosa\n19      5.700000    3.800000     1.700000   0.3000000     setosa\n20      5.100000    3.800000     1.500000   0.3000000     setosa\n21      5.400000    3.400000     1.700000   0.2000000     setosa\n22      5.100000    3.700000     1.500000   0.4000000     setosa\n23      4.600000    3.600000     1.000000   0.2000000     setosa\n24      5.100000    3.300000     1.700000   0.5000000     setosa\n25      4.800000    3.400000     1.900000   0.2938250     setosa\n26      5.000000    3.435417     1.600000   0.2000000     setosa\n27      5.000000    3.400000     1.600000   0.4000000     setosa\n28      5.200000    3.500000     1.500000   0.2000000     setosa\n29      5.200000    3.400000     1.400000   0.2000000     setosa\n30      4.700000    3.200000     1.600000   0.2000000     setosa\n31      4.800000    3.306800     1.600000   0.2000000     setosa\n32      5.400000    3.400000     1.500000   0.2217281     setosa\n33      5.200000    4.100000     1.500000   0.1000000     setosa\n34      5.500000    4.200000     1.400000   0.2000000     setosa\n35      4.900000    3.100000     1.500000   0.2000000     setosa\n36      5.000000    3.200000     1.200000   0.2000000     setosa\n37      5.500000    3.500000     1.300000   0.2000000     setosa\n38      4.900000    3.071380     1.400000   0.1000000     setosa\n39      4.400000    3.000000     1.300000   0.1940821     setosa\n40      5.100000    3.400000     1.500000   0.2000000     setosa\n41      5.000000    3.500000     1.300000   0.3000000     setosa\n42      4.500000    2.300000     1.300000   0.3000000     setosa\n43      4.400000    3.200000     1.300000   0.2000000     setosa\n44      5.000000    3.500000     1.600000   0.6000000     setosa\n45      5.100000    3.800000     1.900000   0.4000000     setosa\n46      4.800000    3.000000     1.400000   0.3000000     setosa\n47      5.100000    3.800000     1.600000   0.2000000     setosa\n48      4.600000    3.200000     1.400000   0.2000000     setosa\n49      5.300000    3.700000     1.500000   0.2000000     setosa\n50      5.000000    3.300000     1.400000   0.2000000     setosa\n51      7.000000    3.200000     4.657098   1.4000000 versicolor\n52      6.224267    3.200000     4.500000   1.5000000 versicolor\n53      6.900000    3.100000     4.900000   1.5000000 versicolor\n54      5.500000    2.300000     4.009955   1.3000000 versicolor\n55      6.500000    2.800000     4.785000   1.5000000 versicolor\n56      5.700000    2.800000     4.500000   1.3000000 versicolor\n57      6.300000    3.300000     4.723100   1.6000000 versicolor\n58      4.900000    2.400000     3.300000   1.0000000 versicolor\n59      6.600000    2.900000     4.600000   1.3000000 versicolor\n60      5.200000    2.700000     3.900000   1.4000000 versicolor\n61      5.415833    2.000000     3.500000   1.0000000 versicolor\n62      5.721783    3.000000     4.200000   1.5000000 versicolor\n63      6.000000    2.200000     4.000000   1.0000000 versicolor\n64      6.100000    2.900000     4.700000   1.4000000 versicolor\n65      5.600000    2.900000     4.224271   1.3000000 versicolor\n66      6.700000    3.100000     4.400000   1.4000000 versicolor\n67      5.600000    3.000000     4.500000   1.4176667 versicolor\n68      5.800000    2.700000     4.100000   1.0000000 versicolor\n69      6.200000    2.200000     4.500000   1.5000000 versicolor\n70      5.600000    2.473683     3.900000   1.1000000 versicolor\n71      5.900000    3.200000     4.800000   1.8000000 versicolor\n72      6.100000    2.800000     4.000000   1.3000000 versicolor\n73      6.300000    2.500000     4.900000   1.5000000 versicolor\n74      6.294995    2.800000     4.700000   1.4422000 versicolor\n75      6.400000    2.900000     4.300000   1.3419000 versicolor\n76      6.600000    3.000000     4.400000   1.4000000 versicolor\n77      6.800000    2.800000     4.800000   1.4000000 versicolor\n78      6.700000    3.000000     5.000000   1.7000000 versicolor\n79      6.000000    2.900000     4.647900   1.5000000 versicolor\n80      5.700000    2.600000     3.500000   1.0000000 versicolor\n81      5.440450    2.400000     3.800000   1.1000000 versicolor\n82      5.500000    2.400000     3.700000   1.0000000 versicolor\n83      5.800000    2.700000     3.900000   1.2000000 versicolor\n84      6.000000    2.700000     5.100000   1.6000000 versicolor\n85      5.400000    3.000000     4.500000   1.5000000 versicolor\n86      6.000000    3.400000     4.500000   1.6000000 versicolor\n87      6.700000    3.100000     4.700000   1.4577000 versicolor\n88      6.300000    2.300000     4.400000   1.3000000 versicolor\n89      5.600000    3.000000     4.100000   1.3000000 versicolor\n90      5.500000    2.500000     4.000000   1.3000000 versicolor\n91      5.500000    2.600000     4.400000   1.2000000 versicolor\n92      6.100000    3.000000     4.526114   1.4000000 versicolor\n93      5.800000    2.600000     4.000000   1.2000000 versicolor\n94      5.000000    2.300000     3.300000   1.0000000 versicolor\n95      5.600000    2.700000     4.200000   1.3000000 versicolor\n96      5.700000    3.000000     4.139935   1.2000000 versicolor\n97      5.700000    2.900000     4.200000   1.3000000 versicolor\n98      6.200000    2.900000     4.300000   1.3000000 versicolor\n99      5.100000    2.500000     3.725717   1.1000000 versicolor\n100     5.700000    2.800000     4.287783   1.3000000 versicolor\n101     6.300000    3.327167     6.000000   2.5000000  virginica\n102     5.800000    2.700000     5.100000   1.9000000  virginica\n103     7.100000    3.000000     5.900000   2.1000000  virginica\n104     6.300000    2.900000     5.600000   1.8000000  virginica\n105     6.500000    3.000000     5.800000   2.2000000  virginica\n106     7.600000    3.000000     6.600000   2.1000000  virginica\n107     4.900000    2.500000     4.500000   1.7000000  virginica\n108     7.300000    2.900000     6.300000   1.8000000  virginica\n109     6.700000    2.500000     5.800000   1.8000000  virginica\n110     7.200000    3.600000     6.100000   2.5000000  virginica\n111     6.500000    3.200000     5.100000   2.0000000  virginica\n112     6.400000    2.756000     5.300000   1.9000000  virginica\n113     6.800000    3.000000     5.500000   2.1000000  virginica\n114     5.700000    2.500000     5.000000   2.0000000  virginica\n115     5.800000    2.800000     5.100000   2.4000000  virginica\n116     6.736750    3.133500     5.300000   2.3000000  virginica\n117     6.500000    3.000000     5.500000   1.8000000  virginica\n118     7.345200    3.800000     6.700000   2.2000000  virginica\n119     7.700000    2.600000     6.322667   2.3000000  virginica\n120     6.000000    2.200000     5.000000   1.5000000  virginica\n121     6.900000    3.186500     5.700000   2.3000000  virginica\n122     5.600000    2.800000     4.900000   1.8892000  virginica\n123     7.700000    2.800000     6.700000   2.0000000  virginica\n124     6.300000    2.700000     5.410267   1.8000000  virginica\n125     6.700000    3.300000     5.700000   2.1000000  virginica\n126     7.200000    3.200000     6.000000   1.8000000  virginica\n127     6.200000    2.800000     4.800000   1.7237000  virginica\n128     6.100000    3.000000     4.900000   1.8000000  virginica\n129     6.400000    2.800000     5.600000   2.1000000  virginica\n130     7.200000    3.045971     5.800000   2.0216667  virginica\n131     7.400000    2.800000     6.326700   1.9000000  virginica\n132     7.477000    2.862000     6.400000   2.0000000  virginica\n133     6.409000    2.800000     5.600000   2.2000000  virginica\n134     6.300000    2.800000     5.100000   1.5000000  virginica\n135     6.100000    2.600000     5.600000   1.9045667  virginica\n136     7.186500    3.000000     6.100000   2.0358333  virginica\n137     6.300000    3.400000     5.600000   2.4000000  virginica\n138     6.586857    3.100000     5.500000   1.8000000  virginica\n139     6.000000    3.000000     4.800000   1.8000000  virginica\n140     6.900000    3.100000     5.400000   2.1000000  virginica\n141     6.700000    3.100000     5.317300   2.4000000  virginica\n142     6.900000    3.100000     5.100000   2.3000000  virginica\n143     5.800000    2.700000     5.100000   1.9000000  virginica\n144     6.800000    3.200000     5.900000   2.3000000  virginica\n145     6.700000    3.300000     5.700000   2.5000000  virginica\n146     6.700000    3.118500     5.200000   2.3000000  virginica\n147     6.300000    2.500000     5.000000   1.9000000  virginica\n148     6.500000    3.000000     5.200000   1.9456667  virginica\n149     6.200000    3.400000     5.400000   2.3000000  virginica\n150     5.900000    3.000000     5.100000   1.8000000  virginica\n\n\nVisualizamos el error cometido en las imputaciones.\n\niris.imp$OOBerror\n\n       MSE        MSE        MSE        MSE        PFC \n0.12395058 0.08867171 0.06091633 0.02916087 0.04477612 \n\n\nNRMSE és un error normalitzat mitjà al quadrat. S’utilitza per representar l’error derivat d’imputar valors continus. El PFC (proporció de falsament classificada) s’utilitza per representar l’error derivat d’imputar valors categòrics.\nComparamos el accuracy actual,\n\n(iris.err &lt;- mixError(iris.imp$ximp, iris.mis, iris))\n\n    NRMSE       PFC \n0.1403915 0.0000000 \n\n\nMiramos la diferencia entre las dos imputaciones\n\nnewBD &lt;- data.frame(real = iris[, \"Sepal.Length\"], imputed =  iris.imp$ximp[, \"Sepal.Length\"])\ndf_long &lt;- newBD %&gt;%\n  pivot_longer(cols = everything(), names_to = \"Variable\", values_to = \"Valor\")\n\nggplot(df_long, aes(x = Valor, fill = Variable)) +\n  geom_density(alpha = 0.3) +  # Transparencia para mejor visualización\n  labs(title = \"Densidad de las tres variables\",\n       x = \"Valor\",\n       y = \"Densidad\") +\n  theme_minimal() +\n  scale_fill_manual(values = c(\"blue\", \"red\"))\n\n\n\n\n\n\n\n\n\n4.9.1 Exercises:\n\nDeploy the multiple plot to compare te imputation and not imputation with all numeric vars in dataframe\n\n\n\n4.9.2 Extra\nQuan es tracta de valors que manquen, és possible que vulgueu reemplaçar valors per valors que manquen (NA). Això és útil en els casos en què es coneix l’origen de les dades i es pot estar segur de quins valors han de faltar. Per exemple, podríeu saber que tots els valors de «N/A», «N A» i «No disponible», o -99 o -1 se suposa que falten.\nnaniar proporciona funcions per treballar específicament en aquest tipus de problemes utilitzant la funció replace.with.na. Aquesta funció és el compliment a tidyr::replace els NA’s, que reemplaça un valor NA per un valor especificat, mentre que naniar::replace,with_na reemplaça un valor per un NA:\n\ntidyr::replace_na: Missing values turns into a value (NA –&gt; -99)\nnaniar::replace_with_na: Value becomes a missing value (-99 –&gt; NA)"
  },
  {
    "objectID": "material/Preprocessing/AdvancedPreprocessing.html#outliers-1",
    "href": "material/Preprocessing/AdvancedPreprocessing.html#outliers-1",
    "title": "Advance Preprocessing",
    "section": "8.1 Outliers",
    "text": "8.1 Outliers\n\nhttps://statsandr.com/blog/outliers-detection-in-r/#z-scores\nhttps://m-mburu.github.io/datacamp/anomaly_detection_R/anomaly_detection.html\nhttps://github.com/pridiltal/ctv-AnomalyDetection"
  },
  {
    "objectID": "material/Preprocessing/AdvancedPreprocessing.html#imputación",
    "href": "material/Preprocessing/AdvancedPreprocessing.html#imputación",
    "title": "Advance Preprocessing",
    "section": "8.2 Imputación",
    "text": "8.2 Imputación\n\nhttp://naniar.njtierney.com/articles/replace-with-na.html\nhttps://cran.r-project.org/web/packages/visdat/vignettes/using_visdat.html\nhttps://cran.r-project.org/web/packages/DMwR2/DMwR2.pdf\nhttps://ltorgo.github.io/DMwR2/RintroDM.html#data_pre-processing\nhttps://www.rdocumentation.org/packages/DMwR/versions/0.4.1/topics/knnImputation\nhttps://amices.org/mice/"
  },
  {
    "objectID": "material/Preprocessing/AdvancedPreprocessing.html#mimmi",
    "href": "material/Preprocessing/AdvancedPreprocessing.html#mimmi",
    "title": "Advance Preprocessing",
    "section": "4.10 MIMMI",
    "text": "4.10 MIMMI\nPuedes descargar el fichero de MIMMI pinchando aquí"
  },
  {
    "objectID": "material/FactorialMethods/ACP.html#elección-de-número-de-componentes",
    "href": "material/FactorialMethods/ACP.html#elección-de-número-de-componentes",
    "title": "Anàlisis de Components Principals (ACP)",
    "section": "8.1 Elección de número de componentes",
    "text": "8.1 Elección de número de componentes\nCriterios habituales:\n\nCodo del scree plot.\nUmbral de varianza acumulada (p. ej., 90–95%).\nKaiser (autovalores &gt; 1) cuando se usa matriz de correlaciones.\nValidación en tareas posteriores (clasificación/regresión).\n\n\nRPython\n\n\n\n# Elegir k para alcanzar &gt;= 95% varianza\n(k &lt;- which(cum_exp_ratio &gt;= 0.95)[1])\n\n[1] 2\n\n\n\n\n\nk = int(np.argmax(cum_exp_ratio &gt;= 0.95)) + 1\nk\n\n2"
  },
  {
    "objectID": "material/FactorialMethods/ACP.html#interpretación-de-cargas-y-biplot",
    "href": "material/FactorialMethods/ACP.html#interpretación-de-cargas-y-biplot",
    "title": "Anàlisis de Components Principals (ACP)",
    "section": "10.1 Interpretación de cargas y biplot",
    "text": "10.1 Interpretación de cargas y biplot\n\nVectores largos: variables con alta contribución al componente.\nÁngulos pequeños entre vectores: variables correlacionadas.\nPuntos (observaciones) próximos: perfiles similares en variables originales.\nSigno de la carga: dirección de la relación con el componente."
  },
  {
    "objectID": "material/Preprocessing/AdvancedPreprocessing.html#preselección-de-variables",
    "href": "material/Preprocessing/AdvancedPreprocessing.html#preselección-de-variables",
    "title": "Advance Preprocessing",
    "section": "5.1 Preselección de variables",
    "text": "5.1 Preselección de variables\n\n5.1.1 Varianza nula\nUno de los aspectos fundamentales en la selección de variables es comprobar si su varianza es cero o cercana a cero porque, si es así, sus valores son iguales o similares, respectivamente, y, por tanto, esas variables estarán perfectamente o cuasiperfectamente correlacionadas con el término independiente del modelo, con lo cual, en el mejor de los casos, solo añadirán ruido al modelo. Este tipo de efecto acaba afectando en la división de los conjuntos de entrenamiento y validación de los datos.\nPara visualizarlo en R, podemos hacer lo siguiente:\n\nlibrary(caret)\nlibrary(idealista18)\nlibrary(tidyverse)\n\nMadrid_Sale &lt;- as.data.frame(Madrid_Sale)\nnumeric_cols &lt;- sapply(Madrid_Sale, is.numeric)\nMadrid_Sale_num &lt;- Madrid_Sale[, numeric_cols]\n\nvarianza &lt;- nearZeroVar(Madrid_Sale_num, saveMetrics = T)\nhead(varianza, 2)\n\n       freqRatio percentUnique zeroVar   nzv\nPERIOD  2.019617   0.004218742   FALSE FALSE\nPRICE   1.076923   2.911986500   FALSE FALSE\n\n\n\n\n5.1.2 Correlación entre variables\nUna de las cuestiones a tener en cuenta en el proceso de selección de variables es la magnitud de las correlaciones entre variables ya que esto puede afectar a la fiabilidad de las predicciones al tener variables muy correlacionadas. En el caso extreno el modelo tendrá problemas de colinealidad o multicolinealidad.\nPara detectar las variables con muy elevada correlación entre ellas, se le pasa la función findCorrelation() de caret, con valor 0,9, a la matriz de correlaciones lineales entre las variables susceptibles de ser seleccionadas.\n\nmadrid_cor &lt;- cor(Madrid_Sale_num[, 1:20])\n(alta_corr &lt;- findCorrelation(madrid_cor, cutoff = .9))\n\n[1] 11\n\n\nPara visualizar, podemos ver el corrplot:\n\nlibrary(\"corrplot\")\n\nmatriz_corr &lt;- cor(Madrid_Sale_num[, 1:8])\ncorrplot(matriz_corr, method = \"circle\")\n\n\n\n\n\n\n\n\n\n\n5.1.3 Combinaciones lineales\nEn la mayoría de los casos las variables que se utilizan como predictoras no son ortogonales, sino que tienen cierto grado de dependencia lineal entre ellas. Si dicho grado es moderado, las consecuencias de la no ortogonalidad en la predicción no son graves, pero en los casos de dependencia lineal cuasiperfecta las inferencias resultantes del modelo estimado distan mucho de la realidad. Dichas consecuencias son aún más graves en el caso de que las combinaciones lineales sean perfectas. Por ello, la existencia de colinealidad o combinaciones lineales entre las variables seleccionables también es una circunstancia a evitar.\nLas principales fuentes de multicolinealidad son:\n\nEl método utilizado en la recogida de datos (subespacios)\nRestricciones en el modelo o en la población (existencia de variables correlacionadas)\nEspecificación del modelo (polinomios)\nMás variables que observaciones\n\nLos efectos de la multicolinealidad en los modelos son los siguientes:\n\nLos estimadores tendrán grandes varianzas y covarianzas\nLas estimaciones de los coeficientes del modelo serán demasiado grandes\nLos signos de los coeficientes estimados suelen ser distintos a los esperados\nPequeñas variaciones en los datos, o en las especificaciones del modelo, provocarán grandes cambios en los coeficientes\n\nUtilizando la función findLinearCombos() del paquete caret permite encontrar combinaciones lineales de las variables predictoras.\n\nMadrid_Sale_num_na &lt;- tidyr::drop_na(Madrid_Sale_num) # Es necesario eliminar los NA.\n(combos &lt;- findLinearCombos(Madrid_Sale_num_na))\n\n$linearCombos\n$linearCombos[[1]]\n[1] 12 11\n\n$linearCombos[[2]]\n[1] 33\n\n\n$remove\n[1] 12 33\n\n\nEn caso de encontrarse con problemas de multicolinealidad se deberia de realizar:\n\nEliminación de variables predictoras que se encuentren altamente relacionadas con otras que permanecen en el modelo\nSustituir las variables predictoras por componentes principales\nIncluir información externa a los datos originales. Esta alternativa implica utilizar estimadores contraídos (de Stein o ridge) o bayesianos.\n\nSi quisieramos eliminar variables que son combinaciones lineales deberiamos de hacer lo siguiente:\n\nhead(Madrid_Sale_num_na[, -combos$remove])\n\n  PERIOD   PRICE UNITPRICE CONSTRUCTEDAREA ROOMNUMBER BATHNUMBER HASTERRACE\n1 201803  126000  2680.851              47          1          1          0\n2 201803  228000  4560.000              50          0          1          0\n3 201803  425000  6071.429              70          1          1          0\n4 201803 2349000  4333.948             542          1          1          0\n5 201803  236000  4720.000              50          2          1          0\n6 201803 1131000  5463.768             207          5          3          0\n  HASLIFT HASAIRCONDITIONING AMENITYID HASPARKINGSPACE PARKINGSPACEPRICE\n1       1                  1         3               0                 1\n2       0                  0         3               0                 1\n3       1                  0         1               1                 1\n4       1                  1         3               0                 1\n5       0                  0         3               0                 1\n6       1                  0         3               1                 1\n  HASNORTHORIENTATION HASSOUTHORIENTATION HASEASTORIENTATION HASWESTORIENTATION\n1                   0                   0                  0                  0\n2                   0                   0                  0                  0\n3                   0                   0                  0                  1\n4                   0                   0                  0                  0\n5                   0                   0                  1                  0\n6                   0                   1                  1                  0\n  HASBOXROOM HASWARDROBE HASSWIMMINGPOOL HASDOORMAN HASGARDEN ISDUPLEX ISSTUDIO\n1          1           1               1          1         1        0        0\n2          0           0               0          0         0        0        1\n3          0           0               0          0         0        0        0\n4          0           0               0          0         0        0        0\n5          0           0               0          0         0        0        0\n6          1           0               0          1         0        0        0\n  ISINTOPFLOOR CONSTRUCTIONYEAR FLOORCLEAN FLATLOCATIONID CADCONSTRUCTIONYEAR\n1            0             2005          1              1                2005\n2            0             1930          0              1                1930\n3            0             1900          2              1                1900\n4            0             1890          1              1                1890\n5            0             1900          3              1                1900\n6            0             1940          0              1                1940\n  CADMAXBUILDINGFLOOR CADDWELLINGCOUNT CADASTRALQUALITYID BUILTTYPEID_2\n1                   7              319                  3             1\n2                   5               19                  7             0\n3                   5               16                  1             0\n4                   4               11                  2             1\n5                   5                6                  6             1\n6                   5               29                  4             1\n  BUILTTYPEID_3 DISTANCE_TO_CITY_CENTER DISTANCE_TO_METRO\n1             0               8.0584293         0.8720746\n2             1               1.2502313         0.3370982\n3             1               0.7535746         0.4371906\n4             0               0.6227413         0.1622422\n5             0               1.0341080         0.3147058\n6             0               0.6328356         0.1954949\n  DISTANCE_TO_CASTELLANA LONGITUDE LATITUDE\n1               6.868677 -3.766933 40.36248\n2               1.794136 -3.714340 40.40874\n3               1.548310 -3.712390 40.41487\n4               1.453994 -3.711072 40.41734\n5               1.622230 -3.712053 40.40976\n6               1.400989 -3.709651 40.42011"
  },
  {
    "objectID": "material/Preprocessing/AdvancedPreprocessing.html#métodos-de-selección-de-variables",
    "href": "material/Preprocessing/AdvancedPreprocessing.html#métodos-de-selección-de-variables",
    "title": "Advance Preprocessing",
    "section": "5.2 Métodos de selección de variables",
    "text": "5.2 Métodos de selección de variables\nEn el subset de variables que sobrevivien al proceso anterior, es necesario detectar cuales de ellas han de entrar en el modelo. Esta fase se realiza porque:\n\nQueremos simplificar los modelos para hacerlos más interpretables\nMejorar la precisión del modelo\nReducir el tiempo de computación. Entrenar algoritmos con mayor velocidad\nEvitar la maldición de la dimensionalidad (efecto Huges), que se refiere a las consecuencias no deseadas que tiene lugar cuando la dimensionalidad de un problema es muy elevada.\nReducir la probabilidad de sobreajuste\n\n\n5.2.1 Filters\nLos métodos de selección de variables tipo filtro usan técnicas estadísticas para evaluar la relación entre cada variable predictora y la variable objetivo. Generalmente, consideran la influencia de cada variable predictora sobre la variable objetivo por separado. Las puntuaciones obtenidas se utilizan como base para clasificar y elegir las variables predictoras que se utilizarán en el modelo.\nSi la variable predictora es numérica, entonces se usa el coeficiente de correlación de Pearson o el de Spearman (si es o no lineal). Si las variables fueran todas categóricas se podria usar medidas de asociación para tablas de contingencia. Por lo contrario si la entregada es categorica y la salida es numérica se podrian usar técnicas de ANOVA para analizar que variables son influeyentes.\nPodemos usar diferentes paquetes como FSelector, caret para implementar dicha técnica. En este caso, usaremos el paquete FSinR. A continuación se muestra un ejemplo para variables predictoras numéricas. Para ello, se toma una muestra del conjunto de datos. Una vez en disposición de la muestra, primeramente se transforma la variable objetivo en categórica, siendo las categorías (intervalos) cuatro cortes de la distribución de sus valores; dicha categorización se lleva a cabo mediante binning También se eliminan los registros con datos faltantes.\n\nlibrary(\"rsample\")\n\n# Se toma una muestra con el paquete rsample\nset.seed(7)\nMadrid_Sale_num_sample &lt;- sample(1:nrow(Madrid_Sale_num), size = 5000, replace = FALSE)\nMadrid_Sale_num_sample &lt;- Madrid_Sale_num[Madrid_Sale_num_sample, ]\n# Se realiza binning con cuatro bins\nMadrid_Sale_num_sample_bin &lt;- Madrid_Sale_num_sample |&gt;\n  mutate(price_bin = cut(PRICE, breaks = c(0, 250000, 500000, 750000, 10000000), labels = c(\"primerQ\", \"segundoQ\", \"tercerQ\", \"c\"), include.lowest = TRUE)) |&gt;\n  select(price_bin, CONSTRUCTEDAREA, ROOMNUMBER, BATHNUMBER, HASTERRACE, HASLIFT)\n# Se eliminan los registros con valores missing\nMadrid_Sale_sample_na &lt;- drop_na(Madrid_Sale_num_sample_bin)\n\nUna vez discretizada la variable objetivo, se selecciona el conjunto de variables predictoras de la variable objetivo price_bin, que es la variable PRICE transformada mediante binning. Como método tipo filtro se utiliza minimum description length (MDLM), que es un método de selección de variables que se basa en una medida de la complejidad del modelo denominada “longitud mínima de la descripción” (de ahí el nombre del modelo), por lo que su objetivo es encontrar el modelo más sencillo que proporcione una explicación aceptable de los datos. Como algoritmo de búsqueda se utiliza sequential forward selection.\n\nlibrary(\"FSinR\")\n\n# Método tipo filtro MDLC (Minimum-Description_Length-Criterion)\nevaluador &lt;- filterEvaluator(\"MDLC\")\n\n# Se genera el algoritmo de búsqueda\nbuscador &lt;- searchAlgorithm(\"sequentialForwardSelection\")\n\n# Se implementa el proceso, pasando a la función los dos parámetros anteriores\nresultados &lt;- featureSelection(Madrid_Sale_sample_na, \"price_bin\", buscador, evaluador)\n\n# Se muestran los resultados\nresultados$bestFeatures\nresultados$bestValue\n\n\n\n5.2.2 Wrappers\nEste enfoque realiza una búsqueda a través de diferentes combinaciones o subconjuntos de variables predictoras/clasificadoras para comprobar el efecto que tienen en la precisión del modelo.\nLos métodos wrapper son de gran eficacia a la hora de eliminar variables irrelevantes y/o redundantes (cosa que no ocurre en los de tipo filtro porque se centran en el poder predictor de cada variable de forma aislada).\ntienen en cuenta la circunstancia de que dos o más variables, aparentemente irrelevantes en cuanto a su capacidad predictiva o clasificatoria cuando se consideran una por una, pueden ser relevantes cuando se consideran conjuntamente. Sin embargo, son muy lentos, ya que tienen que aplicar muchísimas veces el algoritmo de búsqueda, cambiando cada vez el número de variables, siguiendo cada vez algún criterio tanto de búsqueda como de paro.\n\n# Se fijan los parámetros\nevaluador &lt;- wrapperEvaluator(\"rpart1SE\")\nbuscador &lt;- searchAlgorithm(\"sequentialForwardSelection\")\n# Se evalúan sobre Madrid_Sale_sample_na\nresults &lt;- featureSelection(Madrid_Sale_sample_na, \"price_bin\", buscador, evaluador)\nresultados$bestFeatures\nresultados$bestValue\n\n\n\n5.2.3 Embeddings\nHay algunos algoritmos de aprendizaje automático que realizan la selección automática de variables como parte del aprendizaje del modelo. Estos son los métodos de selección de tipo intrínseco, que aglutinan las ventajas de los métodos de filtro y envoltura.\n\nlibrary(\"randomForest\")\n\n# Usar random forest para la selección de variables\nrf_modelo &lt;- randomForest(price_bin ~ ., data = Madrid_Sale_num_sample_bin)\n\n# Listar las variables más importantes\nvarImp(rf_modelo)"
  },
  {
    "objectID": "material/Preprocessing/AdvancedPreprocessing.html#limpieza-de-los-datos-a-nivel-de-variable",
    "href": "material/Preprocessing/AdvancedPreprocessing.html#limpieza-de-los-datos-a-nivel-de-variable",
    "title": "Advance Preprocessing",
    "section": "2.1 Limpieza de los datos a nivel de variable",
    "text": "2.1 Limpieza de los datos a nivel de variable\nLos errores estructurales a nivel de variable se centran fundamentalmente en el tipo de dato de las variables. En primer lugar, se visualizan los datos con la función diagnose() de dlookr.\n\nlibrary(dlookr)\ndlookr::diagnose(dades)\n\n# A tibble: 10 × 6\n   variables        types missing_count missing_percent unique_count unique_rate\n   &lt;chr&gt;            &lt;chr&gt;         &lt;int&gt;           &lt;dbl&gt;        &lt;int&gt;       &lt;dbl&gt;\n 1 Name             char…             0               0        17795     0.890  \n 2 Age              inte…             0               0           23     0.00115\n 3 Gender           char…             0               0            2     0.0001 \n 4 Income           inte…             0               0        17035     0.852  \n 5 Appearance_Score nume…             0               0         8642     0.432  \n 6 Interests_Score  nume…             0               0         8656     0.433  \n 7 Confidence_Score nume…             0               0         8649     0.432  \n 8 Educational_Sta… char…             0               0            4     0.0002 \n 9 Job_Type         char…             0               0            2     0.0001 \n10 Valentine_Date   inte…             0               0            2     0.0001"
  },
  {
    "objectID": "material/Preprocessing/AdvancedPreprocessing.html#eliminación-de-observaciones-duplicadas-o-irrelevantes",
    "href": "material/Preprocessing/AdvancedPreprocessing.html#eliminación-de-observaciones-duplicadas-o-irrelevantes",
    "title": "Advance Preprocessing",
    "section": "2.2 Eliminación de observaciones duplicadas o irrelevantes",
    "text": "2.2 Eliminación de observaciones duplicadas o irrelevantes\nLas observaciones duplicadas aparecen frecuentemente durante la recogida de datos e integración de las bases de datos, por lo que dichas duplicidades deben ser eliminadas en esta fase de limpieza.\nA continuación, se usa la función overview() del paquete dlookr.\n\nhead(overview(dades), n = 9)\n\n    division               metrics   value\n1       size          observations   20000\n2       size             variables      10\n3       size                values  200000\n4       size           memory size 2544080\n5 duplicated duplicate observation       0\n6    missing  complete observation   20000\n7    missing   missing observation       0\n8    missing     missing variables       0\n9    missing        missing values       0"
  }
]
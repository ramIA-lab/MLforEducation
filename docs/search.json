[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Métodos Estadísticos para la mineria de datos",
    "section": "",
    "text": "La filosofía de la minería de datos trata de la conversión de datos en conocimiento para la toma de decisiones, y como tal constituye la fase central del proceso de extracción de conocimiento a partir de bases de datos. La minería de datos es un punto de encuentro de diferentes disciplinas:\nJuntas permiten afrontar muchos problemas actuales en cuanto al tratamiento de la información.\nLa asignatura introduce las técnicas más usuales para la resolución de tres tipos de problemas fundamentales: el análisis de datos binarios (transacciones), el análisis de datos científicos (por ejemplo, de genómica) y el análisis de datos de empresas; los cuales configuran buena parte de los problemas actuales que trata la minería de datos.\nComo objetivo paralelo hay utilizar la R, un potente en torno a programación libre."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "material/trees_ensambleMethods/DecisionsTree_RandomForest_XGBoost.html",
    "href": "material/trees_ensambleMethods/DecisionsTree_RandomForest_XGBoost.html",
    "title": "Árboles de Decisión, Random Forest y XGBoost",
    "section": "",
    "text": "En este ejemplo se entrena un árbol de regresión para predecir el precio unitario de la vivienda en Madrid. Para ello se utilizan los datos de viviendas a la venta en Madrid publicados en Idealista durante el año 2018. Estos datos están incluidos en el paquete idealista18. Las variables que contienen nuestra base de datos son las siguientes:\n\n“ASSETID” : Identificador único del activo\n“PERIOD” : Fecha AAAAMM, indica el trimestre en el que se extrajo el anuncio, utilizamos AAAA03 para el 1.er trimestre, AAAA06 para el 2.º, AAAA09 para el 3.er y AAAA12 para el 4.º\n“PRICE” : Precio de venta del anuncio en idealista expresado en euros\n“UNITPRICE” : Precio en euros por metro cuadrado\n“CONSTRUCTEDAREA” : Superficie construida de la casa en metros cuadrados\n“ROOMNUMBER” : Número de habitaciones\n“BATHNUMBER” : Número de baños\n“HASTERRACE” : Variable ficticia para terraza (toma 1 si hay una terraza, 0 en caso contrario)\n“HASLIFT” : Variable ficticia para ascensor (toma 1 si hay ascensor en el edificio, 0 en caso contrario)\n“HASAIRCONDITIONING” : Variable ficticia para Aire Acondicionado (toma 1 si hay una Aire Acondicionado, 0 en caso contrario)\n“AMENITYID” : Indica las comodidades incluidas (1 - sin muebles, sin comodidades de cocina, 2 - comodidades de cocina, sin muebles, 3 - comodidades de cocina, muebles)\n“HASPARKINGSPACE” : Variable ficticia para estacionamiento (toma 1 si el estacionamiento está incluido en el anuncio, 0 en caso contrario)\n“ISPARKINGSPACEINCLUDEDINPRICE” : Variable ficticia para estacionamiento (toma 1 si el estacionamiento está incluido en el anuncio, 0 en caso contrario)\n“PARKINGSPACEPRICE” : Precio de plaza de parking en euros\n“HASNORTHORIENTATION” : Variable ficticia para orientación (toma 1 si la orientación es Norte en el anuncio, 0 en caso contrario) - Nota importante: las características de orientación no son características ortogonales, una casa orientada al norte también puede estar orientada al este\n“HASSOUTHORIENTATION” : Variable ficticia para orientación (toma 1 si la orientación es Sur en el anuncio, 0 en caso contrario) - Nota importante: las características de orientación no son características ortogonales, una casa orientada al norte también puede estar orientada al este\n“HASEASTORIENTATION” : Variable ficticia para orientación (toma 1 si la orientación es Este en el anuncio, 0 en caso contrario) - Nota importante: las características de orientación no son características ortogonales, una casa orientada al norte también puede estar orientada al este\n“HASWESTORIENTATION” : Variable ficticia para orientación (toma 1 si la orientación es Oeste en el anuncio, 0 en caso contrario) - Nota importante: las características de orientación no son características ortogonales, una casa orientada al norte también puede estar orientada al este\n“HASBOXROOM” : Variable ficticia para boxroom (toma 1 si boxroom está incluido en el anuncio, 0 en caso contrario)\n“HASWARDROBE” : Variable ficticia para vestuario (toma 1 si el vestuario está incluido en el anuncio, 0 en caso contrario)\n“HASSWIMMINGPOOL” : Variable ficticia para piscina (toma 1 si la piscina está incluida en el anuncio, 0 en caso contrario)\n“HASDOORMAN” : Variable ficticia para portero (toma 1 si hay un portero en el edificio, 0 en caso contrario)\n“HASGARDEN” : Variable ficticia para jardín (toma 1 si hay un jardín en el edificio, 0 en caso contrario)\n“ISDUPLEX” : Variable ficticia para dúplex (toma 1 si es un dúplex, 0 en caso contrario)\n“ISSTUDIO” : Variable ficticia para piso de soltero (estudio en español) (toma 1 si es un piso para una sola persona, 0 en caso contrario)\n“ISINTOPFLOOR” : Variable ficticia que indica si el apartamento está ubicado en el piso superior (toma 1 en el piso superior, 0 en caso contrario)\n“CONSTRUCTIONYEAR” : Año de construcción (fuente: anunciante)\n“FLOORCLEAN” : Indica el número de piso del apartamento comenzando desde el valor 0 para la planta baja (fuente: anunciante)\n“FLATLOCATIONID” : Indica el tipo de vistas que tiene el piso (1 - exterior, 2 - interior)\n“CADCONSTRUCTIONYEAR” : Año de construcción según fuente catastral (fuente: catastro), tenga en cuenta que esta cifra puede diferir de la proporcionada por el anunciante\n“CADMAXBUILDINGFLOOR” : Superficie máxima del edificio (fuente: catastro)\n“CADDWELLINGCOUNT” : Recuento de viviendas en el edificio (fuente: catastro)\n“CADASTRALQUALITYID” : Calidad catastral (fuente: catastro)\n“BUILTTYPEID_1” : Valor ficticio para estado del piso: 1 obra nueva 0 en caso contrario (fuente: anunciante)\n“BUILTTYPEID_2” : Valor ficticio para condición plana: 1 segundero a restaurar 0 en caso contrario (fuente: anunciante)\n“BUILTTYPEID_3” : Valor ficticio para estado plano: 1 de segunda mano en buen estado 0 en caso contrario (fuente: anunciante)\n“DISTANCE_TO_CITY_CENTER” : Distancia al centro de la ciudad en km\n“DISTANCE_TO_METRO” : Distancia istancia a una parada de metro en km.\n“DISTANCE_TO_DIAGONAL” : Distancia a la Avenida Diagonal en km; Diagonal es una calle principal que corta la ciudad en diagonal a la cuadrícula de calles.\n“LONGITUDE” : Longitud del activo\n“LATITUDE” : Latitud del activo\n“geometry” : Geometría de características simples en latitud y longitud.\n\nFuente: Idealista\n\nlibrary(\"idealista18\")\nBCN &lt;- get(data(\"Barcelona_Sale\"))\n\n\n# Filtramos la epoca a Navidad\nBCN &lt;- BCN[which(BCN$PERIOD == \"201812\"), ]\n\npisos_sf_BCN &lt;- st_as_sf(BCN, coords = c(\"LONGITUDE\", \"LATITUDE\"), crs = 4326)\n\n# Leer shapefile de secciones censales\nsecciones &lt;- st_read(\"C:/Users/sergi/Downloads/Shapefile/seccionado_2024/SECC_CE_20240101.shp\")\n\n# Transformar pisos al sistema de referencia de las secciones censales\npisos_sf_BCN &lt;- st_transform(pisos_sf_BCN, crs = st_crs(secciones))\n\n# Hacer el match entre pisos y secciones censales\npisos_con_seccion &lt;- st_join(pisos_sf_BCN, secciones, join = st_within)\n\n# Convertir a dataframe para exportar\nBCN &lt;- as.data.frame(pisos_con_seccion)\n\nrm(Barcelona_Sale, Barcelona_Polygons, Barcelona_POIS, pisos_con_seccion, pisos_sf_BCN, secciones); gc()\n\n\nrentaMedia &lt;- read.csv(\"https://raw.githubusercontent.com/miguel-angel-monjas/spain-datasets/refs/heads/master/data/Renta%20media%20en%20Espa%C3%B1a.csv\")\n# NOs quedamos con los datos que nos interesa de Barcelona\nrentaMedia &lt;- rentaMedia[which(rentaMedia$Provincia == \"Barcelona\" & rentaMedia$Tipo.de.elemento == \"sección\"), ]\nrentaMedia$Código.de.territorio &lt;- paste0(\"0\", rentaMedia$Código.de.territorio)\n\n\ncols &lt;- c(\"Renta.media.por.persona\", \"Renta.media.por.hogar\")\n\nm &lt;- match(BCN$CUSEC, rentaMedia$Código.de.territorio)\nBCN[, cols] &lt;- rentaMedia[m, cols]"
  },
  {
    "objectID": "material/trees_ensambleMethods/DecisionsTree_RandomForest_XGBoost.html#análisi-descriptivo-de-los-datos",
    "href": "material/trees_ensambleMethods/DecisionsTree_RandomForest_XGBoost.html#análisi-descriptivo-de-los-datos",
    "title": "Árboles de Decisión, Random Forest y XGBoost",
    "section": "2.1 Análisi descriptivo de los datos",
    "text": "2.1 Análisi descriptivo de los datos\n\n## Descriptiva de los datos\nlibrary(DataExplorer)\nlibrary(lubridate)\nlibrary(dplyr)\n\n## Data Manipulation\nlibrary(reshape2)\n\n## Plotting\nlibrary(ggplot2)\n\n## Descripción completa\nDataExplorer::introduce(BCN)\n\n   rows columns discrete_columns continuous_columns all_missing_columns\n1 23334      36               24                 12                   0\n  total_missing_values complete_rows total_observations memory_usage\n1                    0         23334             840024      6078888\n\n## Descripción de la bbdd\nplot_intro(BCN)\n\n\n\n\n\n\n\n## Descripción de los missings\nplot_missing(BCN)\n\n\n\n\n\n\n\n## Descripción de las varaibles categoricas\nplot_bar(BCN)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n## Descripción variables numéricas\nplot_histogram(BCN)\n\n\n\n\n\n\n\nplot_density(BCN)\n\n\n\n\n\n\n\nplot_qq(BCN)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nplot_correlation(BCN)"
  },
  {
    "objectID": "material/trees_ensambleMethods/DecisionsTree_RandomForest_XGBoost.html#creación-del-árbol",
    "href": "material/trees_ensambleMethods/DecisionsTree_RandomForest_XGBoost.html#creación-del-árbol",
    "title": "Árboles de Decisión, Random Forest y XGBoost",
    "section": "4.1 Creación del árbol",
    "text": "4.1 Creación del árbol\n\nRPython\n\n\n\nlibrary(rpart)\nlibrary(rpart.plot)\n\nset.seed(1994)\n\narbol &lt;- rpart(RENTA ~ ., data = rtrain)\nsummary(arbol)\n\nCall:\nrpart(formula = RENTA ~ ., data = rtrain)\n  n= 18669 \n\n          CP nsplit rel error    xerror        xstd\n1 0.38211183      0 1.0000000 1.0000000 0.008293935\n2 0.06753946      1 0.6178882 0.6178882 0.007426365\n3 0.04447571      2 0.5503487 0.5503487 0.007149373\n4 0.02214609      4 0.4613973 0.4643338 0.006727872\n5 0.01884253      5 0.4392512 0.4425548 0.006607381\n6 0.01186835      6 0.4204087 0.4267711 0.006516230\n7 0.01162364      8 0.3966720 0.4034014 0.006375036\n8 0.01015539     10 0.3734247 0.3792977 0.006221129\n9 0.01000000     12 0.3531139 0.3676740 0.006143722\n\nVariable importance\n                   CDIS DISTANCE_TO_CITY_CENTER    DISTANCE_TO_DIAGONAL \n                     40                      24                      17 \n    CADCONSTRUCTIONYEAR         CONSTRUCTEDAREA               UNITPRICE \n                      8                       3                       3 \n      DISTANCE_TO_METRO              BATHNUMBER              ROOMNUMBER \n                      2                       1                       1 \n\nNode number 1: 18669 observations,    complexity param=0.3821118\n  predicted class=Media  expected loss=0.4377846  P(node) =1\n    class counts:  1941  6232 10496\n   probabilities: 0.104 0.334 0.562 \n  left son=2 (4541 obs) right son=3 (14128 obs)\n  Primary splits:\n      CDIS                    splits as  LRRRRLRRRR, improve=2615.4350, (0 missing)\n      DISTANCE_TO_DIAGONAL    &lt; 1.697581   to the right, improve=1710.4180, (0 missing)\n      HASLIFT                 splits as  LR, improve= 961.7684, (0 missing)\n      DISTANCE_TO_CITY_CENTER &lt; 1.146692   to the left,  improve= 885.2803, (0 missing)\n      CADCONSTRUCTIONYEAR     &lt; 1900.5     to the left,  improve= 800.8061, (0 missing)\n  Surrogate splits:\n      DISTANCE_TO_CITY_CENTER &lt; 1.146692   to the left,  agree=0.853, adj=0.396, (0 split)\n      CADCONSTRUCTIONYEAR     &lt; 1900.5     to the left,  agree=0.823, adj=0.273, (0 split)\n      DISTANCE_TO_DIAGONAL    &lt; 4.28574    to the right, agree=0.775, adj=0.076, (0 split)\n      CONSTRUCTEDAREA         &lt; 46.5       to the left,  agree=0.769, adj=0.052, (0 split)\n      ROOMNUMBER              &lt; 11         to the right, agree=0.757, adj=0.001, (0 split)\n\nNode number 2: 4541 observations,    complexity param=0.02214609\n  predicted class=Baja   expected loss=0.156133  P(node) =0.2432375\n    class counts:     0  3832   709\n   probabilities: 0.000 0.844 0.156 \n  left son=4 (4294 obs) right son=5 (247 obs)\n  Primary splits:\n      DISTANCE_TO_CITY_CENTER &lt; 0.4629643  to the right, improve=263.54560, (0 missing)\n      DISTANCE_TO_DIAGONAL    &lt; 1.293729   to the right, improve= 85.60347, (0 missing)\n      CDIS                    splits as  L----R----, improve= 66.85504, (0 missing)\n      HASLIFT                 splits as  LR, improve= 65.14028, (0 missing)\n      CADCONSTRUCTIONYEAR     &lt; 1950.5     to the left,  improve= 50.10187, (0 missing)\n\nNode number 3: 14128 observations,    complexity param=0.06753946\n  predicted class=Media  expected loss=0.3072622  P(node) =0.7567625\n    class counts:  1941  2400  9787\n   probabilities: 0.137 0.170 0.693 \n  left son=6 (1990 obs) right son=7 (12138 obs)\n  Primary splits:\n      CDIS                 splits as  -RRRL-RRRL, improve=903.3696, (0 missing)\n      DISTANCE_TO_DIAGONAL &lt; 2.370657   to the right, improve=788.5626, (0 missing)\n      CONSTRUCTEDAREA      &lt; 137.5      to the right, improve=434.3222, (0 missing)\n      UNITPRICE            &lt; 2898.227   to the left,  improve=369.9583, (0 missing)\n      BATHNUMBER           &lt; 2.5        to the right, improve=353.8536, (0 missing)\n  Surrogate splits:\n      CONSTRUCTEDAREA   &lt; 223.5      to the right, agree=0.866, adj=0.052, (0 split)\n      BATHNUMBER        &lt; 3.5        to the right, agree=0.864, adj=0.038, (0 split)\n      DISTANCE_TO_METRO &lt; 1.634495   to the right, agree=0.860, adj=0.006, (0 split)\n      ROOMNUMBER        &lt; 6.5        to the right, agree=0.859, adj=0.003, (0 split)\n      UNITPRICE         &lt; 6992.805   to the right, agree=0.859, adj=0.001, (0 split)\n\nNode number 4: 4294 observations,    complexity param=0.01015539\n  predicted class=Baja   expected loss=0.1152771  P(node) =0.230007\n    class counts:     0  3799   495\n   probabilities: 0.000 0.885 0.115 \n  left son=8 (3278 obs) right son=9 (1016 obs)\n  Primary splits:\n      DISTANCE_TO_CITY_CENTER &lt; 3.316192   to the left,  improve=107.18450, (0 missing)\n      CDIS                    splits as  L----R----, improve=107.18450, (0 missing)\n      DISTANCE_TO_DIAGONAL    &lt; 2.818623   to the left,  improve= 91.99937, (0 missing)\n      CADCONSTRUCTIONYEAR     &lt; 1950.5     to the left,  improve= 66.10102, (0 missing)\n      HASLIFT                 splits as  LR, improve= 49.70642, (0 missing)\n  Surrogate splits:\n      DISTANCE_TO_DIAGONAL &lt; 2.826385   to the left,  agree=0.989, adj=0.955, (0 split)\n      CADCONSTRUCTIONYEAR  &lt; 1950.5     to the left,  agree=0.886, adj=0.517, (0 split)\n      UNITPRICE            &lt; 2840.561   to the right, agree=0.876, adj=0.475, (0 split)\n      CADDWELLINGCOUNT     &lt; 30.5       to the left,  agree=0.789, adj=0.106, (0 split)\n      CADMAXBUILDINGFLOOR  &lt; 7.5        to the left,  agree=0.776, adj=0.053, (0 split)\n\nNode number 5: 247 observations\n  predicted class=Media  expected loss=0.1336032  P(node) =0.01323049\n    class counts:     0    33   214\n   probabilities: 0.000 0.134 0.866 \n\nNode number 6: 1990 observations,    complexity param=0.01162364\n  predicted class=Alta   expected loss=0.3613065  P(node) =0.1065938\n    class counts:  1271     0   719\n   probabilities: 0.639 0.000 0.361 \n  left son=12 (1077 obs) right son=13 (913 obs)\n  Primary splits:\n      CONSTRUCTEDAREA      &lt; 119.5      to the right, improve=121.31850, (0 missing)\n      BATHNUMBER           &lt; 2.5        to the right, improve= 87.03903, (0 missing)\n      DISTANCE_TO_DIAGONAL &lt; 0.5534786  to the left,  improve= 85.36516, (0 missing)\n      CDIS                 splits as  ----R----L, improve= 68.18003, (0 missing)\n      UNITPRICE            &lt; 5287.749   to the right, improve= 60.24694, (0 missing)\n  Surrogate splits:\n      ROOMNUMBER                    &lt; 3.5        to the right, agree=0.790, adj=0.542, (0 split)\n      BATHNUMBER                    &lt; 1.5        to the right, agree=0.747, adj=0.448, (0 split)\n      HASTERRACE                    splits as  RL, agree=0.647, adj=0.230, (0 split)\n      HASPARKINGSPACE               splits as  RL, agree=0.641, adj=0.217, (0 split)\n      ISPARKINGSPACEINCLUDEDINPRICE splits as  RL, agree=0.641, adj=0.217, (0 split)\n\nNode number 7: 12138 observations,    complexity param=0.04447571\n  predicted class=Media  expected loss=0.2529247  P(node) =0.6501687\n    class counts:   670  2400  9068\n   probabilities: 0.055 0.198 0.747 \n  left son=14 (2820 obs) right son=15 (9318 obs)\n  Primary splits:\n      DISTANCE_TO_DIAGONAL    &lt; 2.370657   to the right, improve=832.9688, (0 missing)\n      CDIS                    splits as  -RRL--RLL-, improve=615.9082, (0 missing)\n      UNITPRICE               &lt; 2898.227   to the left,  improve=380.3717, (0 missing)\n      DISTANCE_TO_CITY_CENTER &lt; 3.940257   to the right, improve=338.4368, (0 missing)\n      CONSTRUCTEDAREA         &lt; 80.5       to the left,  improve=178.0100, (0 missing)\n  Surrogate splits:\n      DISTANCE_TO_CITY_CENTER &lt; 4.058752   to the right, agree=0.897, adj=0.558, (0 split)\n      CDIS                    splits as  -RRL--LRR-, agree=0.816, adj=0.206, (0 split)\n      UNITPRICE               &lt; 2753.972   to the left,  agree=0.811, adj=0.184, (0 split)\n      DISTANCE_TO_METRO       &lt; 0.7218963  to the right, agree=0.780, adj=0.051, (0 split)\n      CADMAXBUILDINGFLOOR     &lt; 26.5       to the right, agree=0.769, adj=0.005, (0 split)\n\nNode number 8: 3278 observations\n  predicted class=Baja   expected loss=0.05308115  P(node) =0.1755852\n    class counts:     0  3104   174\n   probabilities: 0.000 0.947 0.053 \n\nNode number 9: 1016 observations,    complexity param=0.01015539\n  predicted class=Baja   expected loss=0.3159449  P(node) =0.05442177\n    class counts:     0   695   321\n   probabilities: 0.000 0.684 0.316 \n  left son=18 (712 obs) right son=19 (304 obs)\n  Primary splits:\n      DISTANCE_TO_DIAGONAL    &lt; 3.420441   to the right, improve=181.26100, (0 missing)\n      DISTANCE_TO_CITY_CENTER &lt; 4.8151     to the right, improve=162.58570, (0 missing)\n      UNITPRICE               &lt; 2586.19    to the left,  improve= 62.53053, (0 missing)\n      HASLIFT                 splits as  LR, improve= 54.20484, (0 missing)\n      CONSTRUCTEDAREA         &lt; 74.5       to the left,  improve= 31.09944, (0 missing)\n  Surrogate splits:\n      DISTANCE_TO_CITY_CENTER &lt; 5.091757   to the right, agree=0.958, adj=0.859, (0 split)\n      UNITPRICE               &lt; 3251.667   to the left,  agree=0.775, adj=0.247, (0 split)\n      CADCONSTRUCTIONYEAR     &lt; 1954.5     to the right, agree=0.708, adj=0.023, (0 split)\n      BUILTTYPEID_1           splits as  LR, agree=0.707, adj=0.020, (0 split)\n      DISTANCE_TO_METRO       &lt; 0.02808259 to the right, agree=0.707, adj=0.020, (0 split)\n\nNode number 12: 1077 observations\n  predicted class=Alta   expected loss=0.2005571  P(node) =0.05768922\n    class counts:   861     0   216\n   probabilities: 0.799 0.000 0.201 \n\nNode number 13: 913 observations,    complexity param=0.01162364\n  predicted class=Media  expected loss=0.449069  P(node) =0.0489046\n    class counts:   410     0   503\n   probabilities: 0.449 0.000 0.551 \n  left son=26 (495 obs) right son=27 (418 obs)\n  Primary splits:\n      CDIS                    splits as  ----R----L, improve=47.94928, (0 missing)\n      DISTANCE_TO_DIAGONAL    &lt; 0.5540544  to the left,  improve=44.18423, (0 missing)\n      UNITPRICE               &lt; 5289.352   to the right, improve=37.28880, (0 missing)\n      DISTANCE_TO_METRO       &lt; 0.2951583  to the right, improve=24.68798, (0 missing)\n      DISTANCE_TO_CITY_CENTER &lt; 2.14678    to the left,  improve=21.64891, (0 missing)\n  Surrogate splits:\n      CADDWELLINGCOUNT        &lt; 30.5       to the left,  agree=0.729, adj=0.409, (0 split)\n      DISTANCE_TO_DIAGONAL    &lt; 1.062684   to the right, agree=0.705, adj=0.356, (0 split)\n      UNITPRICE               &lt; 4821.591   to the right, agree=0.677, adj=0.294, (0 split)\n      CADMAXBUILDINGFLOOR     &lt; 8.5        to the left,  agree=0.662, adj=0.261, (0 split)\n      DISTANCE_TO_CITY_CENTER &lt; 3.575557   to the left,  agree=0.623, adj=0.177, (0 split)\n\nNode number 14: 2820 observations,    complexity param=0.04447571\n  predicted class=Baja   expected loss=0.4553191  P(node) =0.1510525\n    class counts:    95  1536  1189\n   probabilities: 0.034 0.545 0.422 \n  left son=28 (2010 obs) right son=29 (810 obs)\n  Primary splits:\n      CDIS                    splits as  -RRL--RLL-, improve=202.04190, (0 missing)\n      DISTANCE_TO_CITY_CENTER &lt; 2.380396   to the left,  improve=170.89780, (0 missing)\n      DISTANCE_TO_DIAGONAL    &lt; 3.446593   to the left,  improve=103.80040, (0 missing)\n      CADCONSTRUCTIONYEAR     &lt; 1971.5     to the left,  improve= 77.57975, (0 missing)\n      CONSTRUCTEDAREA         &lt; 86.5       to the left,  improve= 62.41583, (0 missing)\n  Surrogate splits:\n      DISTANCE_TO_CITY_CENTER &lt; 5.730574   to the left,  agree=0.819, adj=0.370, (0 split)\n      DISTANCE_TO_DIAGONAL    &lt; 3.716013   to the left,  agree=0.759, adj=0.160, (0 split)\n      CONSTRUCTEDAREA         &lt; 130.5      to the left,  agree=0.716, adj=0.012, (0 split)\n      BATHNUMBER              &lt; 2.5        to the left,  agree=0.715, adj=0.006, (0 split)\n      DISTANCE_TO_METRO       &lt; 0.01826087 to the right, agree=0.714, adj=0.005, (0 split)\n\nNode number 15: 9318 observations,    complexity param=0.01186835\n  predicted class=Media  expected loss=0.1544323  P(node) =0.4991162\n    class counts:   575   864  7879\n   probabilities: 0.062 0.093 0.846 \n  left son=30 (523 obs) right son=31 (8795 obs)\n  Primary splits:\n      DISTANCE_TO_CITY_CENTER &lt; 0.871618   to the left,  improve=157.24670, (0 missing)\n      CDIS                    splits as  -RRL--RLL-, improve=145.36290, (0 missing)\n      CADDWELLINGCOUNT        &lt; 167.5      to the right, improve= 73.55115, (0 missing)\n      CONSTRUCTEDAREA         &lt; 135.5      to the right, improve= 56.59289, (0 missing)\n      DISTANCE_TO_DIAGONAL    &lt; 1.820647   to the right, improve= 55.50302, (0 missing)\n  Surrogate splits:\n      BATHNUMBER &lt; 0.5        to the left,  agree=0.944, adj=0.002, (0 split)\n\nNode number 18: 712 observations\n  predicted class=Baja   expected loss=0.1207865  P(node) =0.03813809\n    class counts:     0   626    86\n   probabilities: 0.000 0.879 0.121 \n\nNode number 19: 304 observations\n  predicted class=Media  expected loss=0.2269737  P(node) =0.01628368\n    class counts:     0    69   235\n   probabilities: 0.000 0.227 0.773 \n\nNode number 26: 495 observations\n  predicted class=Alta   expected loss=0.4020202  P(node) =0.02651454\n    class counts:   296     0   199\n   probabilities: 0.598 0.000 0.402 \n\nNode number 27: 418 observations\n  predicted class=Media  expected loss=0.2727273  P(node) =0.02239006\n    class counts:   114     0   304\n   probabilities: 0.273 0.000 0.727 \n\nNode number 28: 2010 observations,    complexity param=0.01884253\n  predicted class=Baja   expected loss=0.3278607  P(node) =0.1076651\n    class counts:    35  1351   624\n   probabilities: 0.017 0.672 0.310 \n  left son=56 (1748 obs) right son=57 (262 obs)\n  Primary splits:\n      DISTANCE_TO_DIAGONAL    &lt; 3.446593   to the left,  improve=135.93570, (0 missing)\n      DISTANCE_TO_CITY_CENTER &lt; 2.380396   to the left,  improve=112.73140, (0 missing)\n      DISTANCE_TO_METRO       &lt; 0.3243732  to the left,  improve= 83.82446, (0 missing)\n      CADCONSTRUCTIONYEAR     &lt; 1967.5     to the left,  improve= 70.86276, (0 missing)\n      CADMAXBUILDINGFLOOR     &lt; 7.5        to the left,  improve= 37.15447, (0 missing)\n  Surrogate splits:\n      DISTANCE_TO_CITY_CENTER &lt; 5.208402   to the left,  agree=0.949, adj=0.611, (0 split)\n      DISTANCE_TO_METRO       &lt; 0.7342067  to the left,  agree=0.904, adj=0.267, (0 split)\n      PARKINGSPACEPRICE       &lt; 28451      to the left,  agree=0.872, adj=0.015, (0 split)\n      CADMAXBUILDINGFLOOR     &lt; 1.5        to the right, agree=0.872, adj=0.015, (0 split)\n\nNode number 29: 810 observations\n  predicted class=Media  expected loss=0.3024691  P(node) =0.04338743\n    class counts:    60   185   565\n   probabilities: 0.074 0.228 0.698 \n\nNode number 30: 523 observations,    complexity param=0.01186835\n  predicted class=Media  expected loss=0.4760994  P(node) =0.02801436\n    class counts:   249     0   274\n   probabilities: 0.476 0.000 0.524 \n  left son=60 (284 obs) right son=61 (239 obs)\n  Primary splits:\n      DISTANCE_TO_DIAGONAL    &lt; 0.990107   to the left,  improve=165.999900, (0 missing)\n      DISTANCE_TO_METRO       &lt; 0.1705538  to the right, improve= 38.560880, (0 missing)\n      DISTANCE_TO_CITY_CENTER &lt; 0.44858    to the right, improve= 36.859370, (0 missing)\n      UNITPRICE               &lt; 4976.906   to the right, improve= 12.669910, (0 missing)\n      CONSTRUCTEDAREA         &lt; 124.5      to the right, improve=  9.569507, (0 missing)\n  Surrogate splits:\n      DISTANCE_TO_METRO       &lt; 0.1699124  to the right, agree=0.744, adj=0.439, (0 split)\n      DISTANCE_TO_CITY_CENTER &lt; 0.4805939  to the right, agree=0.728, adj=0.406, (0 split)\n      UNITPRICE               &lt; 4976.906   to the right, agree=0.642, adj=0.218, (0 split)\n      CADCONSTRUCTIONYEAR     &lt; 1941.5     to the left,  agree=0.587, adj=0.096, (0 split)\n      CADDWELLINGCOUNT        &lt; 24.5       to the left,  agree=0.583, adj=0.088, (0 split)\n\nNode number 31: 8795 observations\n  predicted class=Media  expected loss=0.1353042  P(node) =0.4711018\n    class counts:   326   864  7605\n   probabilities: 0.037 0.098 0.865 \n\nNode number 56: 1748 observations\n  predicted class=Baja   expected loss=0.2580092  P(node) =0.09363115\n    class counts:    35  1297   416\n   probabilities: 0.020 0.742 0.238 \n\nNode number 57: 262 observations\n  predicted class=Media  expected loss=0.2061069  P(node) =0.01403396\n    class counts:     0    54   208\n   probabilities: 0.000 0.206 0.794 \n\nNode number 60: 284 observations\n  predicted class=Alta   expected loss=0.1584507  P(node) =0.01521238\n    class counts:   239     0    45\n   probabilities: 0.842 0.000 0.158 \n\nNode number 61: 239 observations\n  predicted class=Media  expected loss=0.041841  P(node) =0.01280197\n    class counts:    10     0   229\n   probabilities: 0.042 0.000 0.958 \n\n\n\nrpart.plot(arbol)\n\n\n\n\n\n\n\n\n\n\n\n# Decision Tree Classification\nfrom sklearn.tree import DecisionTreeClassifier\n\nclassifier = DecisionTreeClassifier(criterion = 'entropy', random_state = 1994)\nclf = classifier.fit(pyX_train, pyy_train)\n\n\nfrom sklearn import tree\n\ntree.plot_tree(clf)"
  },
  {
    "objectID": "material/trees_ensambleMethods/DecisionsTree_RandomForest_XGBoost.html#creamos-las-predicciones",
    "href": "material/trees_ensambleMethods/DecisionsTree_RandomForest_XGBoost.html#creamos-las-predicciones",
    "title": "Árboles de Decisión, Random Forest y XGBoost",
    "section": "4.2 Creamos las predicciones",
    "text": "4.2 Creamos las predicciones\n\nRPython\n\n\nAplicamos el modelo a nuestros valores de test.\n\npredict(arbol, rtest[1:10, ])\n\n   Alta      Baja      Media\n1     0 0.9469189 0.05308115\n2     0 0.9469189 0.05308115\n3     0 0.9469189 0.05308115\n4     0 0.9469189 0.05308115\n5     0 0.9469189 0.05308115\n6     0 0.1336032 0.86639676\n7     0 0.9469189 0.05308115\n8     0 0.9469189 0.05308115\n9     0 0.9469189 0.05308115\n10    0 0.1336032 0.86639676\n\n\n\npredicciones &lt;- predict(arbol, rtest, type = \"class\")\ncaret::confusionMatrix(predicciones, as.factor(rtest$RENTA))\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction Alta Baja Media\n     Alta   311    1   148\n     Baja     5 1250   171\n     Media  151  266  2362\n\nOverall Statistics\n                                          \n               Accuracy : 0.8409          \n                 95% CI : (0.8301, 0.8513)\n    No Information Rate : 0.5747          \n    P-Value [Acc &gt; NIR] : &lt; 2.2e-16       \n                                          \n                  Kappa : 0.7099          \n                                          \n Mcnemar's Test P-Value : 3.415e-05       \n\nStatistics by Class:\n\n                     Class: Alta Class: Baja Class: Media\nSensitivity              0.66595      0.8240       0.8810\nSpecificity              0.96451      0.9441       0.7898\nPos Pred Value           0.67609      0.8766       0.8499\nNeg Pred Value           0.96290      0.9176       0.8309\nPrevalence               0.10011      0.3252       0.5747\nDetection Rate           0.06667      0.2680       0.5063\nDetection Prevalence     0.09861      0.3057       0.5957\nBalanced Accuracy        0.81523      0.8840       0.8354\n\n\n\nCM &lt;- caret::confusionMatrix(predicciones, as.factor(rtest$RENTA)); CM &lt;- data.frame(CM$table)\n\ngrafico &lt;- ggplot(CM, aes(Prediction,Reference, fill= Freq)) +\n        geom_tile() + geom_text(aes(label=Freq)) +\n        scale_fill_gradient(low=\"white\", high=\"#009194\") +\n        labs(x = \"Reference\",y = \"Prediction\")\n\nplot(grafico)\n\n\n\n\n\n\n\n\n\n\n\nimport pandas as pd\n\n# Prediction\ny_pred = classifier.predict(pyX_test)\n\nresults = pd.DataFrame({\n    'Real': pyy_test,  # Valores reales\n    'Predicho': y_pred  # Valores predichos\n})\n\n# Muestra los primeros 5 registros\nprint(results.head())  \n\n       Real  Predicho\n2399      2         2\n18420     0         0\n22515     2         2\n20265     1         1\n10432     1         1\n\n\n\nfrom sklearn.metrics import classification_report\n\nprint(f'Classification Report: \\n{classification_report(pyy_test, y_pred)}')\n\nClassification Report: \n              precision    recall  f1-score   support\n\n           0       0.85      0.82      0.84       488\n           1       0.92      0.91      0.91      1522\n           2       0.92      0.93      0.92      2657\n\n    accuracy                           0.91      4667\n   macro avg       0.89      0.89      0.89      4667\nweighted avg       0.91      0.91      0.91      4667\n\n\n\nfrom sklearn.metrics import confusion_matrix\nimport seaborn as sns\n\n# Confusion matrix\ncf_matrix = confusion_matrix(pyy_test, y_pred)\nsns.heatmap(cf_matrix, annot=True, fmt='d', cmap='Blues', cbar=False)"
  },
  {
    "objectID": "material/trees_ensambleMethods/DecisionsTree_RandomForest_XGBoost.html#modelo-de-classificación-con-cross-evaluación",
    "href": "material/trees_ensambleMethods/DecisionsTree_RandomForest_XGBoost.html#modelo-de-classificación-con-cross-evaluación",
    "title": "Árboles de Decisión, Random Forest y XGBoost",
    "section": "4.3 Modelo de classificación con cross-evaluación",
    "text": "4.3 Modelo de classificación con cross-evaluación\n\nRPython\n\n\n\n## Generamos los parámetros de control\ntrControl &lt;- trainControl(method = \"cv\", number = 10, classProbs = TRUE,\n  summaryFunction = multiClassSummary)\n## En este caso, se realiza una cros-validación de 10 etapas\n\n# se fija una semilla aleatoria\nset.seed(1994)\n\n# se entrena el modelo\nmodel &lt;- train(RENTA ~ .,  # . equivale a incluir todas las variables\n               data = rtrain,\n               method = \"rpart\",\n               metric = \"Accuracy\",\n               trControl = trControl)\n\n# Generamos el gráfico del árbol\nrpart.plot(model$finalModel)\n\n\n\n\n\n\n\n\n\n# A continuación generamos un gráfico que nos permite ver la variabilidad de los estadísticos\n# calculados\nggplot(melt(model$resample[,c(2:5, 7:9, 12:13)]), aes(x = variable, y = value, fill=variable)) +\n  geom_boxplot(show.legend=FALSE) +\n  xlab(NULL) + ylab(NULL)\n\n\n\n\n\n\n\n\n\n\n\nfrom sklearn.tree import DecisionTreeClassifier\n\n# Modelo de árbol de decisión\nmodel = DecisionTreeClassifier(random_state=1994)\n\nfrom sklearn.model_selection import cross_val_score\n\n# Realizar validación cruzada con 5 folds\nscores = cross_val_score(model, pyX_train, pyy_train, cv=10, scoring = 'accuracy')  # Métrica: accuracy\n\n# Mostrar resultados\nprint(f\"Accuracy por fold: {scores}\")\n\nAccuracy por fold: [0.90626674 0.90680236 0.8993037  0.90305303 0.91215854 0.90787359\n 0.89769684 0.90836013 0.914791   0.91425509]\n\nprint(f\"Accuracy promedio: {scores.mean():.4f}\")\n\nAccuracy promedio: 0.9071"
  },
  {
    "objectID": "material/trees_ensambleMethods/DecisionsTree_RandomForest_XGBoost.html#realizando-hiperparámetro-tunning",
    "href": "material/trees_ensambleMethods/DecisionsTree_RandomForest_XGBoost.html#realizando-hiperparámetro-tunning",
    "title": "Árboles de Decisión, Random Forest y XGBoost",
    "section": "4.4 Realizando hiperparámetro tunning",
    "text": "4.4 Realizando hiperparámetro tunning\n\nRPython\n\n\n\n# Detectamos cuales son los parámetros del modelo que podemos realizar hiperparámeter tunning\nmodelLookup(\"rpart\")\n\n  model parameter                label forReg forClass probModel\n1 rpart        cp Complexity Parameter   TRUE     TRUE      TRUE\n\n\n\n# Se especifica un rango de valores típicos para el hiperparámetro\ntuneGrid &lt;- expand.grid(cp = seq(0.01,0.05,0.01))\n\n\n# se entrena el modelo\nset.seed(1994)\n\nmodel &lt;- train(RENTA ~ .,\n               data = rtrain,\n               method = \"rpart\",\n               metric = \"Accuracy\",\n               trControl = trControl,\n               tuneGrid = tuneGrid)\n\n# Gráfico del árbol obtenido\nrpart.plot(model$finalModel)\n\n\n\n\n\n\n\n\n\n\n\nfrom sklearn.model_selection import GridSearchCV\n\n# Definir rejilla de hiperparámetros\nparam_grid = {\n    'max_depth': [None, 5, 10],\n    'min_samples_split': [2, 5, 10],\n    'min_samples_leaf': [1, 2, 4]\n}\n\n# Declaramos el modelo\nmodel = DecisionTreeClassifier(random_state=1994)\n\n# Configurar GridSearch con validación cruzada\ngrid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=10, scoring='accuracy', n_jobs=-1)\n\n# Ajustar modelo\ngrid_search.fit(pyX_train, pyy_train)\n\nGridSearchCV(cv=10, estimator=DecisionTreeClassifier(random_state=1994),\n             n_jobs=-1,\n             param_grid={'max_depth': [None, 5, 10],\n                         'min_samples_leaf': [1, 2, 4],\n                         'min_samples_split': [2, 5, 10]},\n             scoring='accuracy')In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  GridSearchCV?Documentation for GridSearchCViFittedGridSearchCV(cv=10, estimator=DecisionTreeClassifier(random_state=1994),\n             n_jobs=-1,\n             param_grid={'max_depth': [None, 5, 10],\n                         'min_samples_leaf': [1, 2, 4],\n                         'min_samples_split': [2, 5, 10]},\n             scoring='accuracy') best_estimator_: DecisionTreeClassifierDecisionTreeClassifier(random_state=1994)  DecisionTreeClassifier?Documentation for DecisionTreeClassifierDecisionTreeClassifier(random_state=1994) \n\n# Mostrar mejores parámetros\nprint(f\"Mejores parámetros: {grid_search.best_params_}\")\n\nMejores parámetros: {'max_depth': None, 'min_samples_leaf': 1, 'min_samples_split': 2}\n\nprint(f\"Mejor accuracy: {grid_search.best_score_:.4f}\")\n\nMejor accuracy: 0.9071\n\n\n\nfrom sklearn import tree\ntree.plot_tree(grid_search.best_estimator_)"
  },
  {
    "objectID": "material/trees_ensambleMethods/DecisionsTree_RandomForest_XGBoost.html#como-realizar-poda-de-nuestro-árbol",
    "href": "material/trees_ensambleMethods/DecisionsTree_RandomForest_XGBoost.html#como-realizar-poda-de-nuestro-árbol",
    "title": "Árboles de Decisión, Random Forest y XGBoost",
    "section": "4.5 Como realizar poda de nuestro árbol",
    "text": "4.5 Como realizar poda de nuestro árbol\n\nRPython\n\n\n\n# Con el objetivo de aumentar la generalidad del árbol y facilitar su interpretación, \n# se procede a reducir su tamaño podándolo. Para ello se establece el criterio de \n# que un nodo terminal tiene que tener, como mínimo, 50 observaciones.\nset.seed(1994)\nprunedtree &lt;- rpart(RENTA ~ ., data = rtrain,\n                    cp= 0.01, control = rpart.control(minbucket = 50))\n\nrpart.plot(prunedtree)\n\n\n\n\n\n\n\n\n\n\nEn Python, la poda de un árbol de decisión se puede realizar ajustando los hiperparámetros del árbol durante su creación. Estos hiperparámetros controlan el crecimiento del árbol y, por lo tanto, actúan como técnicas de poda preventiva o postpoda.\nscikit-learn no implementa poda dinámica directa (como ocurre en algunos otros frameworks), pero puedes limitar el tamaño del árbol y evitar sobreajuste mediante los siguientes métodos.\n\n4.5.1 Poda Preventiva (Pre-pruning)\nPoda preventiva consiste en detener el crecimiento del árbol antes de que se haga demasiado grande. Esto se logra ajustando hiperparámetros como:\n\nmax_depth: Profundidad máxima del árbol\nmin_samples_split: Número mínimo de muestras necesarias para dividir un nodo.\nmin_samples_leaf: Número mínimo de muestras necesarias en una hoja.\nmax_leaf_nodes: Número máximo de nodos hoja en el árbol.\n\n\n# Crear un árbol con poda preventiva\nmodel = DecisionTreeClassifier(\n    max_depth=3,              # Limitar la profundidad\n    min_samples_split=10,     # Mínimo 10 muestras para dividir un nodo\n    min_samples_leaf=5,       # Mínimo 5 muestras por hoja\n    random_state=42\n)\n\n# Entrenar el modelo\nmodel.fit(pyX_train, pyy_train)\n\nDecisionTreeClassifier(max_depth=3, min_samples_leaf=5, min_samples_split=10,\n                       random_state=42)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  DecisionTreeClassifier?Documentation for DecisionTreeClassifieriFittedDecisionTreeClassifier(max_depth=3, min_samples_leaf=5, min_samples_split=10,\n                       random_state=42) \n\n# Evaluar\nprint(f\"Accuracy en entrenamiento: {model.score(pyX_train, pyy_train):.4f}\")\n\nAccuracy en entrenamiento: 0.7919\n\nprint(f\"Accuracy en prueba: {model.score(pyX_test, pyy_test):.4f}\")\n\nAccuracy en prueba: 0.7932\n\n\n# Graficamos el árbol podado\ntree.plot_tree(model)\n\n\n\n\n\n\n\n\n\n\n4.5.2 Poda Posterior (Post-Pruning) con ccp_alpha\nSe puedes realizar poda posterior usando cost complexity pruning. Esto implica ajustar el parámetro ccp_alpha (el parámetro de complejidad de coste).\nEl árbol generará múltiples subárboles podados para diferentes valores de ccp_alpha, y tú puedes elegir el más adecuado evaluando su desempeño.\n\nimport matplotlib.pyplot as plt\n\n# Crear un árbol sin poda\nmodel = DecisionTreeClassifier(random_state=1994)\nmodel.fit(pyX_train, pyy_train)\n\nDecisionTreeClassifier(random_state=1994)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  DecisionTreeClassifier?Documentation for DecisionTreeClassifieriFittedDecisionTreeClassifier(random_state=1994) \n\n# Obtener valores de ccp_alpha\npath = model.cost_complexity_pruning_path(pyX_train, pyy_train)\nccp_alphas = path.ccp_alphas\nimpurities = path.impurities\n\n# Entrenar árboles para cada valor de ccp_alpha\nmodels = []\nfor ccp_alpha in ccp_alphas:\n    clf = DecisionTreeClassifier(random_state=42, ccp_alpha=ccp_alpha)\n    clf.fit(pyX_train, pyy_train)\n    models.append(clf)\n\nDecisionTreeClassifier(ccp_alpha=np.float64(0.12113075107966081),\n                       random_state=42)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  DecisionTreeClassifier?Documentation for DecisionTreeClassifieriFittedDecisionTreeClassifier(ccp_alpha=np.float64(0.12113075107966081),\n                       random_state=42) \n\n# Evaluar desempeño\ntrain_scores = [clf.score(pyX_train, pyy_train) for clf in models]\ntest_scores = [clf.score(pyX_test, pyy_test) for clf in models]\n\n# Graficar resultados\nplt.figure(figsize=(8, 6))\nplt.plot(ccp_alphas, train_scores, marker='o', label=\"Train Accuracy\", drawstyle=\"steps-post\")\nplt.plot(ccp_alphas, test_scores, marker='o', label=\"Test Accuracy\", drawstyle=\"steps-post\")\nplt.xlabel(\"ccp_alpha\")\nplt.ylabel(\"Accuracy\")\nplt.title(\"Accuracy vs ccp_alpha\")\nplt.legend()\nplt.grid()\nplt.show()"
  },
  {
    "objectID": "material/trees_ensambleMethods/DecisionsTree_RandomForest_XGBoost.html#aplicación-del-modelo",
    "href": "material/trees_ensambleMethods/DecisionsTree_RandomForest_XGBoost.html#aplicación-del-modelo",
    "title": "Árboles de Decisión, Random Forest y XGBoost",
    "section": "5.1 Aplicación del modelo",
    "text": "5.1 Aplicación del modelo\n\nRPython\n\n\n\n# Random Forest \nlibrary(randomForest)\n## devtools::install_github('araastat/reprtree') # Se instala 1 vez para poder printar graficos\nlibrary(reprtree)\n\nset.seed(1994)\narbol_rf &lt;- randomForest(as.factor(RENTA) ~ .,  data = rtrain, ntree = 25)\n\n\n# se observa el árbol número 20\ntree20 &lt;- getTree(arbol_rf, 20, labelVar = TRUE)\nhead(tree20)\n\n  left daughter right daughter               split var split point status\n1             2              3         CONSTRUCTEDAREA  74.5000000      1\n2             4              5       DISTANCE_TO_METRO   0.7023313      1\n3             6              7              BATHNUMBER   2.5000000      1\n4             8              9             HASWARDROBE   1.5000000      1\n5            10             11       DISTANCE_TO_METRO   1.6344953      1\n6            12             13 DISTANCE_TO_CITY_CENTER   1.1452018      1\n  prediction\n1       &lt;NA&gt;\n2       &lt;NA&gt;\n3       &lt;NA&gt;\n4       &lt;NA&gt;\n5       &lt;NA&gt;\n6       &lt;NA&gt;\n\n## Sin embargo, el método por el que se representa gráficamente no es muy claro y\n## puede llevar a confusión o dificultar la interpretación del árbol. \n## Si se desea estudiar el árbol, hasta un cierto nivel, se puede incluir el argumento depth.\n## El árbol, ahora con una profundidad de 5 ramas.\nplot.getTree(arbol_rf, k = 20, depth = 5)\n\n\n\n\n\n\n\n\n\nlibrary(vip)\nvip(arbol_rf)\n\n\n\n\n\n\n\n\n\n\n\nfrom sklearn.ensemble import RandomForestClassifier\n\nclf = RandomForestClassifier()\nclf.fit(pyX_train, pyy_train)\n\nRandomForestClassifier()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  RandomForestClassifier?Documentation for RandomForestClassifieriFittedRandomForestClassifier() \n\n\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nresults = pd.DataFrame(clf.feature_importances_, index=pyBCN.columns[:-1]).sort_values(by=0, ascending=False)\n\n# Crear gráfico de barras horizontales\nplt.figure(figsize=(10, 8))\nplt.barh(results.index, results[0], color='skyblue')\n\n# Añadir etiquetas y título\nplt.xlabel('Importancia')\nplt.ylabel('Características')\nplt.title('Importancia de las Características')\nplt.grid(axis='x', linestyle='--', alpha=0.7)\nplt.show()"
  },
  {
    "objectID": "material/trees_ensambleMethods/DecisionsTree_RandomForest_XGBoost.html#hiperparameter-tunning-de-random-forest",
    "href": "material/trees_ensambleMethods/DecisionsTree_RandomForest_XGBoost.html#hiperparameter-tunning-de-random-forest",
    "title": "Árboles de Decisión, Random Forest y XGBoost",
    "section": "5.2 Hiperparameter tunning de Random Forest",
    "text": "5.2 Hiperparameter tunning de Random Forest\n\nRPython\n\n\n\n# Identificamos los parámetros que podemos tunnerar\nmodelLookup(\"rf\")\n\n  model parameter                         label forReg forClass probModel\n1    rf      mtry #Randomly Selected Predictors   TRUE     TRUE      TRUE\n\n\n\n# Se especifica un rango de valores posibles de mtry\ntuneGrid &lt;- expand.grid(mtry = 1:10)\ntuneGrid\n\n   mtry\n1     1\n2     2\n3     3\n4     4\n5     5\n6     6\n7     7\n8     8\n9     9\n10   10\n\n\n\n# se fija la semilla aleatoria\nset.seed(1994)\n\n# se entrena el modelo\nmodel &lt;- train(RENTA ~ ., data = rtrain, \n               ntree = 20,\n               method = \"rf\", metric = \"Accuracy\",\n               tuneGrid = tuneGrid,\n               trControl = trainControl(classProbs = TRUE))\n\n\n\n\n5.2.1 Ajuste de hiperparámetros con GridSearchCV\nEl GridSearchCV realiza una búsqueda exhaustiva sobre un conjunto de parámetros especificados. Probará todas las combinaciones posibles de hiperparámetros.\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n\n# Definir los parámetros para la búsqueda\nparam_dist = {\n    'n_estimators': [150, 200],        # Número de árboles en el bosque\n    'max_depth': [None, 10, 20],            # Profundidad máxima del árbol\n    'min_samples_split': [2, 5, 10],            # Número mínimo de muestras para dividir un nodo\n    'min_samples_leaf': [1, 2, 4],               # Número mínimo de muestras en una hoja\n    'max_features': ['auto'],      # Número de características a considerar para dividir un nodo\n    'bootstrap': [True]                      # Si usar bootstrap para los árboles\n}\n\n# Crear el modelo RandomForest\nrf = RandomForestClassifier(random_state = 1994)\n\n# Usar GridSearchCV para encontrar el mejor conjunto de parámetros\ngrid_search = GridSearchCV(estimator = rf, param_grid = param_grid, cv = 10, n_jobs = -1, verbose = 0)\n\n# Ajustar el modelo con los datos de entrenamiento\ngrid_search.fit(pyX_train, pyy_train)\n\n# Mostrar los mejores parámetros encontrados\nprint(\"Mejores parámetros encontrados:\", grid_search.best_params_)\n\ntree.plot_tree(grid_search.best_estimator_)\n\n\n\n5.2.2 Ajuste de Hiperparámetros con RandomizedSearchCV\nRandomizedSearchCV es una técnica más eficiente que GridSearchCV, ya que no prueba todas las combinaciones posibles, sino un número limitado de combinaciones aleatorias dentro de un rango definido. Esto es útil si el espacio de búsqueda es grande y quieres evitar un tiempo de cómputo muy largo.\n\n# Definir los parámetros para la búsqueda aleatoria\nparam_dist = {\n    'n_estimators': [150, 200],        # Número de árboles en el bosque\n    'max_depth': [None, 10, 20],            # Profundidad máxima del árbol\n    'min_samples_split': [2, 5, 10],            # Número mínimo de muestras para dividir un nodo\n    'min_samples_leaf': [1, 2, 4],               # Número mínimo de muestras en una hoja\n    'max_features': ['auto'],      # Número de características a considerar para dividir un nodo\n    'bootstrap': [True]                      # Si usar bootstrap para los árboles\n}\n\n# Usar RandomizedSearchCV para búsqueda aleatoria\nrandom_search = RandomizedSearchCV(estimator=rf, param_distributions=param_dist, \n                                   n_iter=50, cv=10, n_jobs=-1, random_state=1994)\n\n# Ajustar el modelo con los datos de entrenamiento\nrandom_search.fit(X_train, y_train)\n\n# Mostrar los mejores parámetros encontrados\nprint(\"Mejores parámetros encontrados:\", random_search.best_params_)\n\ntree.plot_tree(random_search.best_estimator_)"
  },
  {
    "objectID": "material/trees_ensambleMethods/DecisionsTree_RandomForest_XGBoost.html#predicciones-del-algoritmo",
    "href": "material/trees_ensambleMethods/DecisionsTree_RandomForest_XGBoost.html#predicciones-del-algoritmo",
    "title": "Árboles de Decisión, Random Forest y XGBoost",
    "section": "5.3 Predicciones del algoritmo",
    "text": "5.3 Predicciones del algoritmo\n\nRPython\n\n\n\n# Realizamos las predicciones de este ultimo arbol para la predicción de test\n## Si no decimos nada en type (type = prob), nos devolvera la probabilidad de \n## pertenecer a cada clase. \nprediccion &lt;- predict(arbol_rf, rtest, type = \"class\")\n\n## Para ver la performance, realizaremos la matriz de confusión \ncaret::confusionMatrix(prediccion, as.factor(rtest$RENTA))\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction Alta Baja Media\n     Alta   346    2    52\n     Baja     1 1381    78\n     Media  120  134  2551\n\nOverall Statistics\n                                          \n               Accuracy : 0.917           \n                 95% CI : (0.9088, 0.9248)\n    No Information Rate : 0.5747          \n    P-Value [Acc &gt; NIR] : &lt; 2.2e-16       \n                                          \n                  Kappa : 0.8475          \n                                          \n Mcnemar's Test P-Value : 3.994e-09       \n\nStatistics by Class:\n\n                     Class: Alta Class: Baja Class: Media\nSensitivity              0.74090      0.9103       0.9515\nSpecificity              0.98714      0.9749       0.8720\nPos Pred Value           0.86500      0.9459       0.9094\nNeg Pred Value           0.97163      0.9576       0.9301\nPrevalence               0.10011      0.3252       0.5747\nDetection Rate           0.07417      0.2960       0.5468\nDetection Prevalence     0.08574      0.3130       0.6013\nBalanced Accuracy        0.86402      0.9426       0.9117\n\n\n\nCM &lt;- caret::confusionMatrix(prediccion, as.factor(rtest$RENTA)); CM &lt;- data.frame(CM$table)\n\ngrafico &lt;- ggplot(CM, aes(Prediction,Reference, fill= Freq)) +\n        geom_tile() + geom_text(aes(label=Freq)) +\n        scale_fill_gradient(low=\"white\", high=\"#009194\") +\n        labs(x = \"Reference\",y = \"Prediction\")\n\nplot(grafico)\n\n\n\n\n\n\n\n\n\n\n\npreds = clf.predict(pyX_test)\nprint(f'Classification Report: \\n{classification_report(pyy_test, preds)}')\n\nClassification Report: \n              precision    recall  f1-score   support\n\n           0       0.89      0.78      0.83       488\n           1       0.95      0.90      0.93      1522\n           2       0.91      0.96      0.93      2657\n\n    accuracy                           0.92      4667\n   macro avg       0.92      0.88      0.90      4667\nweighted avg       0.92      0.92      0.92      4667\n\n\n\n# Confusion matrix\ncf_matrix = confusion_matrix(pyy_test, preds)\nsns.heatmap(cf_matrix, annot=True, fmt='d', cmap='Blues', cbar=False)"
  },
  {
    "objectID": "material/trees_ensambleMethods/DecisionsTree_RandomForest_XGBoost.html#aplicamos-el-algoritmo-con-cross-validation-e-hiperparameter-tunning",
    "href": "material/trees_ensambleMethods/DecisionsTree_RandomForest_XGBoost.html#aplicamos-el-algoritmo-con-cross-validation-e-hiperparameter-tunning",
    "title": "Árboles de Decisión, Random Forest y XGBoost",
    "section": "6.1 Aplicamos el algoritmo con cross-validation e hiperparameter tunning",
    "text": "6.1 Aplicamos el algoritmo con cross-validation e hiperparameter tunning\n\nRPython\n\n\n\nrtrain &lt;- rtrain %&gt;%\n  mutate(across(where(is.character), ~ as.factor(.)))\nrtest &lt;- rtest %&gt;%\n  mutate(across(where(is.character), ~ as.factor(.)))\n\n\n# Miramos cuales son los parámetros de este algoritmo \nhead(modelLookup(\"xgbTree\"),4)\n\n# se determina la semilla aleatoria\nset.seed(1994)\n\ntuneGrid &lt;- expand.grid(nrounds = c(50, 100),  # Menor cantidad de rondas \n                        max_depth = c(3, 5),   # Profundidad más baja para los árboles\n                        eta = c(0.1, 0.3),     # Tasa de aprendizaje más alta\n                        gamma = c(0),          # Regularización\n                        colsample_bytree = c(0.7),  # Submuestreo de características\n                        min_child_weight = c(1),    # Regularización adicional\n                        subsample = c(0.7))          # Submuestreo de datos\n\n\n## se determina la semilla aleatoria\nset.seed(1994)\n\n## se entrena el modelo\nmodel &lt;- train(as.factor(RENTA) ~. , \n               data = rtrain, \n               method = \"xgbTree\", \n               metric = \"Accuracy\",\n               trControl = trainControl(classProbs = TRUE, method = \"cv\", number = 10, \n                                        verboseIter = FALSE), \n               tuneGrid = tuneGrid)\n\n# se muestra la salida del modelo\nmodel$bestTune[,1:4]\n\n\nlibrary(vip)\nvip(model$finalModel)\n\n\n\n\nimport xgboost as xgb\n\n# Convertir los datos a DMatrix\ndtrain = xgb.DMatrix(pyX_train, label = pyy_train)\ndtest = xgb.DMatrix(pyX_test, label = pyy_test)\n\n\nparams = {\n    'objective': 'multi:softmax',  # Problema de clasificación multiclase\n    'num_class': 3,                # Número de clases (para el conjunto de datos Iris)\n    'max_depth': 3,                # Profundidad máxima de los árboles\n    'eta': 0.1,                    # Tasa de aprendizaje\n    'eval_metric': 'merror'        # Métrica de evaluación: error en clasificación\n}\n\n\n# Entrenar el modelo\nnum_round = 50  # Número de iteraciones (rounds) de boosting\nmodel = xgb.train(params, dtrain, num_round)\n\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nresults = pd.DataFrame(model.feature_importances_, index=pyBCN.columns[:-1]).sort_values(by=0, ascending=False)\n\n# Crear gráfico de barras horizontales\nplt.figure(figsize=(10, 8))\nplt.barh(results.index, results[0], color='skyblue')\n\n# Añadir etiquetas y título\nplt.xlabel('Importancia')\nplt.ylabel('Características')\nplt.title('Importancia de las Características')\nplt.grid(axis='x', linestyle='--', alpha=0.7)\nplt.show()\n\nSi quisieramos hacerlo en cross validación hariamos lo siguiente:\n\nfrom sklearn.model_selection import GridSearchCV\nfrom xgboost import XGBClassifier\n\n# Definir el modelo\nxgb_model = XGBClassifier(objective='multi:softmax', num_class=3)\n\n# Definir los hiperparámetros a ajustar\nparam_grid = {\n    'max_depth': [3, 5, 7],\n    'eta': [0.01, 0.1, 0.3],\n    'n_estimators': [50, 100, 200]\n}\n\n# Configurar GridSearchCV\ngrid_search = GridSearchCV(estimator=xgb_model, param_grid=param_grid, cv=3, scoring='accuracy')\n\n# Entrenar el modelo con ajuste de hiperparámetros\ngrid_search.fit(X_train, y_train)\n\n# Mostrar los mejores parámetros y resultados\nprint(f\"Mejores parámetros: {grid_search.best_params_}\")\nprint(f\"Mejor precisión: {grid_search.best_score_:.4f}\")"
  },
  {
    "objectID": "material/trees_ensambleMethods/DecisionsTree_RandomForest_XGBoost.html#predicciones",
    "href": "material/trees_ensambleMethods/DecisionsTree_RandomForest_XGBoost.html#predicciones",
    "title": "Árboles de Decisión, Random Forest y XGBoost",
    "section": "6.2 Predicciones",
    "text": "6.2 Predicciones\n\nRPython\n\n\n\n# Realizamos las predicciones de este ultimo arbol para la predicción de test\n## Si no decimos nada en type (type = prob), nos devolvera la probabilidad de \n## pertenecer a cada clase. \nrtest_matrix &lt;- rtest %&gt;% select(-RENTA) %&gt;% as.matrix()\nprediccion &lt;- predict(model$finalModel, rtest_matrix, type = \"class\")\n\n## Para ver la performance, realizaremos la matriz de confusión \ncaret::confusionMatrix(prediccion, as.factor(rtest$RENTA))\n\n\nCM &lt;- caret::confusionMatrix(prediccion, as.factor(rtest$RENTA)); CM &lt;- data.frame(CM$table)\n\ngrafico &lt;- ggplot(CM, aes(Prediction,Reference, fill= Freq)) +\n        geom_tile() + geom_text(aes(label=Freq)) +\n        scale_fill_gradient(low=\"white\", high=\"#009194\") +\n        labs(x = \"Reference\",y = \"Prediction\")\n\nplot(grafico)\n\n\n\n\n# Realizar predicciones\ny_pred = model.predict(dtest)\n\n# Convertir las predicciones de flotante a enteros (si se usa 'multi:softmax')\ny_pred = np.round(y_pred).astype(int)\n\n\n# Confusion matrix\ncf_matrix = confusion_matrix(pyy_test, y_pred)\nsns.heatmap(cf_matrix, annot=True, fmt='d', cmap='Blues', cbar=False)"
  },
  {
    "objectID": "index.html#introducción-a-la-mineria-de-datos",
    "href": "index.html#introducción-a-la-mineria-de-datos",
    "title": "Métodos Estadísticos para la mineria de datos",
    "section": "1 Introducción a la mineria de datos",
    "text": "1 Introducción a la mineria de datos\nLa minería de datos es el proceso de extraer patrones, tendencias y conocimientos útiles a partir de grandes volúmenes de datos. Combina estadística, aprendizaje automático y bases de datos para ayudar a resolver problemas en diversas áreas, como negocios, ciencia y tecnología.\nTeoria\nLaboratorio"
  },
  {
    "objectID": "index.html#introducción-a-la-mineria-de-datos-1",
    "href": "index.html#introducción-a-la-mineria-de-datos-1",
    "title": "Métodos Estadísticos para la mineria de datos",
    "section": "2 [Introducción a la mineria de datos] ()",
    "text": "2 [Introducción a la mineria de datos] ()"
  },
  {
    "objectID": "index.html#introducción-a-la-mineria-de-datos-2",
    "href": "index.html#introducción-a-la-mineria-de-datos-2",
    "title": "Métodos Estadísticos para la mineria de datos",
    "section": "3 [Introducción a la mineria de datos] ()",
    "text": "3 [Introducción a la mineria de datos] ()"
  },
  {
    "objectID": "index.html#introducción-a-la-mineria-de-datos-3",
    "href": "index.html#introducción-a-la-mineria-de-datos-3",
    "title": "Métodos Estadísticos para la mineria de datos",
    "section": "4 [Introducción a la mineria de datos] ()",
    "text": "4 [Introducción a la mineria de datos] ()"
  },
  {
    "objectID": "index.html#introducción-a-la-mineria-de-datos-4",
    "href": "index.html#introducción-a-la-mineria-de-datos-4",
    "title": "Métodos Estadísticos para la mineria de datos",
    "section": "5 [Introducción a la mineria de datos] ()",
    "text": "5 [Introducción a la mineria de datos] ()"
  },
  {
    "objectID": "index.html#introducción-a-la-mineria-de-datos-5",
    "href": "index.html#introducción-a-la-mineria-de-datos-5",
    "title": "Métodos Estadísticos para la mineria de datos",
    "section": "6 [Introducción a la mineria de datos] ()",
    "text": "6 [Introducción a la mineria de datos] ()"
  },
  {
    "objectID": "index.html#introducción-a-la-mineria-de-datos-6",
    "href": "index.html#introducción-a-la-mineria-de-datos-6",
    "title": "Métodos Estadísticos para la mineria de datos",
    "section": "7 [Introducción a la mineria de datos] ()",
    "text": "7 [Introducción a la mineria de datos] ()"
  },
  {
    "objectID": "index.html#introducción-a-la-mineria-de-datos-7",
    "href": "index.html#introducción-a-la-mineria-de-datos-7",
    "title": "Métodos Estadísticos para la mineria de datos",
    "section": "8 [Introducción a la mineria de datos] ()",
    "text": "8 [Introducción a la mineria de datos] ()"
  },
  {
    "objectID": "index.html#introducción-a-la-mineria-de-datos-8",
    "href": "index.html#introducción-a-la-mineria-de-datos-8",
    "title": "Métodos Estadísticos para la mineria de datos",
    "section": "9 [Introducción a la mineria de datos] ()",
    "text": "9 [Introducción a la mineria de datos] ()"
  },
  {
    "objectID": "index.html#introducción-a-la-mineria-de-datos-9",
    "href": "index.html#introducción-a-la-mineria-de-datos-9",
    "title": "Métodos Estadísticos para la mineria de datos",
    "section": "10 [Introducción a la mineria de datos] ()",
    "text": "10 [Introducción a la mineria de datos] ()"
  },
  {
    "objectID": "index.html#introducción-a-la-mineria-de-datos-material",
    "href": "index.html#introducción-a-la-mineria-de-datos-material",
    "title": "Métodos Estadísticos para la mineria de datos",
    "section": "11 [Introducción a la mineria de datos] (material/)",
    "text": "11 [Introducción a la mineria de datos] (material/)"
  },
  {
    "objectID": "index.html#árboles-de-decisión-y-métodos-de-ensamblado",
    "href": "index.html#árboles-de-decisión-y-métodos-de-ensamblado",
    "title": "Métodos Estadísticos para la mineria de datos",
    "section": "12 Árboles de decisión y métodos de ensamblado",
    "text": "12 Árboles de decisión y métodos de ensamblado\npráctica"
  },
  {
    "objectID": "index.html#clustering",
    "href": "index.html#clustering",
    "title": "Métodos Estadísticos para la mineria de datos",
    "section": "2 Clustering",
    "text": "2 Clustering\nEl clustering agrupa datos similares en clústeres basados en características compartidas. Es útil para descubrir patrones ocultos y segmentar conjuntos de datos, comúnmente aplicado en marketing, biología y análisis de redes.\nTeoria\nLaboratorio"
  },
  {
    "objectID": "index.html#visualización-de-datos",
    "href": "index.html#visualización-de-datos",
    "title": "Métodos Estadísticos para la mineria de datos",
    "section": "3 Visualización de datos",
    "text": "3 Visualización de datos\nLa visualización de datos convierte información compleja en gráficos y representaciones visuales claras, facilitando la interpretación y comunicación de resultados. Herramientas como gráficos de dispersión, histogramas y mapas de calor son fundamentales.\n\n3.1 Analisis de componentes principales (ACP)\nEl ACP reduce la dimensionalidad de los datos al identificar las combinaciones lineales más relevantes de las variables originales, conservando la mayor parte de la variación. Se usa para simplificar datos y facilitar su interpretación.\nTeoria\nLaboratorio\n\n\n3.2 Analisis de correspondiencias múltiples (ACM)\nEl ACM analiza tablas de datos categóricos para identificar relaciones entre categorías, visualizando patrones en mapas bidimensionales que facilitan la interpretación.\nTeoria\nLaboratorio"
  },
  {
    "objectID": "index.html#reglas-de-asociación",
    "href": "index.html#reglas-de-asociación",
    "title": "Métodos Estadísticos para la mineria de datos",
    "section": "4 Reglas de asociación",
    "text": "4 Reglas de asociación\nEste método identifica relaciones significativas entre variables en grandes bases de datos. Es clave en aplicaciones como los sistemas de recomendación y análisis de cestas de mercado.\nTeoria\nLaboratorio"
  },
  {
    "objectID": "index.html#reglas-de-clasificación",
    "href": "index.html#reglas-de-clasificación",
    "title": "Métodos Estadísticos para la mineria de datos",
    "section": "5 Reglas de clasificación",
    "text": "5 Reglas de clasificación\nLos modelos de clasificación asignan datos a categorías predefinidas basándose en patrones aprendidos. Es ampliamente usado en diagnóstico médico, detección de fraudes y análisis de texto.\n\n5.1 Lineal Discriminant Analysis (LDA) y Quadratic Discriminant Analysis (QDA)\nAmbos métodos buscan separar categorías utilizando fronteras de decisión basadas en estadísticas. LDA asume varianzas iguales entre clases, mientras que QDA permite varianzas diferentes.\nTeoria\nLaboratorio\n\n\n5.2 Naives Bayes\nUn clasificador basado en probabilidad que asume independencia entre las características. Es eficiente y se aplica en problemas como clasificación de texto y detección de spam.\nTeoria\nLaboratorio"
  },
  {
    "objectID": "index.html#métodos-particionales",
    "href": "index.html#métodos-particionales",
    "title": "Métodos Estadísticos para la mineria de datos",
    "section": "6 Métodos particionales",
    "text": "6 Métodos particionales\nDividen datos en subconjuntos o particiones, a menudo mediante árboles de decisión y técnicas relacionadas.\n\n6.1 Decisions Tree\nModelo gráfico que toma decisiones en base a condiciones secuenciales. Es intuitivo y útil en clasificación y regresión.\n\n\n6.2 Random Forest\nCombina múltiples árboles de decisión para mejorar precisión y reducir sobreajuste. Es robusto y adecuado para tareas de clasificación y regresión.\n\n\n6.3 Bagging & Boosting\nMétodos de ensamblado que mejoran el rendimiento combinando múltiples modelos. Bagging reduce la variabilidad, mientras que Boosting optimiza errores iterativamente.\nTeoria\nLaboratorio"
  },
  {
    "objectID": "index.html#métodos-flexibles-de-discriminación",
    "href": "index.html#métodos-flexibles-de-discriminación",
    "title": "Métodos Estadísticos para la mineria de datos",
    "section": "7 Métodos flexibles de discriminación",
    "text": "7 Métodos flexibles de discriminación\n\n7.1 Support Vectors Machines (SVM)\nSeparan clases usando hiperplanos óptimos en un espacio de alta dimensionalidad. Son efectivas en problemas no lineales y clasificación compleja.\nTeoria\nLaboratorio"
  },
  {
    "objectID": "index.html#deep-learning",
    "href": "index.html#deep-learning",
    "title": "Métodos Estadísticos para la mineria de datos",
    "section": "8 Deep Learning",
    "text": "8 Deep Learning\nEl aprendizaje profundo utiliza redes neuronales para modelar datos complejos. Es ampliamente aplicado en reconocimiento de imágenes, procesamiento de lenguaje natural y más.\n\n8.1 Redes neuronales: Discriminación pel perceptrón multicapa\nLas redes multicapa, basadas en múltiples capas de neuronas interconectadas, resuelven problemas no lineales con alta precisión.\n\n\n8.2 Redes neuronales convolucionales\nEspecializadas en procesar datos con estructura espacial, como imágenes. Extraen automáticamente características relevantes para tareas como clasificación de imágenes y visión por computadora.\nTeoria\nLaboratorio"
  }
]